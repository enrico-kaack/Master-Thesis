\section*{Zusammenfassung}

TODO auf deutsch
\section*{Abstract}    
The automatic detection of clean code violations enables developers to improve understandability and readability of source code more easily. We propose a taxonomy for clean code rule based on the assumed level of complexity for an automated checker. Furthermore, design and implement a platform to orchestrate automated rule checkers and find that it offers a simple way of extension compared to other platforms. Next, we train different machine learning models on the task to detect code patterns that violate clean code rules: A support vector classifier (1) that is not able to detect violations with acceptable performance. An LSTM-based neural network (2) that outperforms all other models, but is sensitive towards less training data. A random forest classifier (3) that can handle a moderate class imbalance in the training data, and a gradient boosting classifier (4) that is insensitive towards class imbalance. By introducing unseen variations, we further investigate if the models have learnt the structure of the problem. We find that the models do not perform well on unseen problem variations and only learnt the problem pattern. We suggest improvements by changing the encoding to emphasize code structure and by adding hand-labelled data with more variations.