\section*{Zusammenfassung}

Die automatische Erkennung von Verletzungen des Clean Codeserlaubt es Entwicklern, die Verständlichkeit und Lesbarkeit von Quellcodeeinfach zu verbessern. Wir führen eine Systematik ein, bei der wir die CleanCode Regeln basierend auf der Komplexität eines automatischenErkennungsverfahren einordnen. Darüber Hinaus entwerfen und implementieren wireine Plattform, die das orchestrieren verschiedener Erkennungsverfahren ermöglicht.Als nächstes Trainieren wir verschiedene Verfahren des maschinellen Lernens zurErkennung von Code Mustern, die die Clean Code Regeln verletzen: EinSupport-Vector-Klassifikator (1) kann die Code Muster nicht zufriedenstellenderkennen. Ein LSTM basierendes neuronales Netzwerk (2) übertrifft alle anderenModelle, ist aber empfindlich gegenüber weniger Trainingsdaten. EinRandom-Forest Klassifikator (3) kann moderate Ungleichheit in derKlassenhäufigkeit in den Trainingsdaten kompensieren und ein Gradient-BoostingKlassifikator ist nicht empfindlich gegenüber Ungleichheit in derKlassenhäufigkeit. Durch das Hinzufügen von unbekannten Problemvariationenuntersuchen wir, ob die Modelle die Struktur des zugrunde liegenden Problemsgelernt haben. Unsere Ergebnisse zeigen, dass die Modelle die unbekanntenVariationen schlecht erkennen können und nur das Muster statt der Struktur desProblems gelernt haben. Zur Verbesserung schlagen wir eine Veränderung der CodeKodierung für die Modelle vor, die größeren Fokus auf die Struktur des Codeslegen. Außerdem empfehlen wir das hinzufügen von weiteren Variationen durchhand-gesammelte Daten.

\newpage
\section*{Abstract}    
The automatic detection of clean code violations enables developers to improve understandability and readability of source code more easily. We propose a taxonomy for clean code rule based on the assumed level of complexity for an automated checker. Furthermore, design and implement a platform to orchestrate automated rule checkers and find that it offers a simple way of extension compared to other platforms. Next, we train different machine learning models on the task to detect code patterns that violate clean code rules: A support vector classifier (1) that is not able to detect violations with acceptable performance. An LSTM-based neural network (2) that outperforms all other models, but is sensitive towards less training data. A random forest classifier (3) that can handle a moderate class imbalance in the training data, and a gradient boosting classifier (4) that is insensitive towards class imbalance. By introducing unseen variations, we further investigate if the models have learnt the structure of the problem. We find that the models do not perform well on unseen problem variations and only learnt the problem pattern. We suggest improvements by changing the encoding to emphasize code structure and by adding hand-labelled data with more variations.