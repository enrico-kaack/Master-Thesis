This section is divided into the approach for the Clean Code Analysis Plattform and the Clean Code classification.

\chapter{Clean Code Analysis Plattform}
The goal of the design and implementation of the Clean Code Analysis Plattform (CCAP) is a tool for software developer to imnprove the code quality of existing and new code. The tool accepts an directory containing source code files as input and analyses the input for snippets of improveable code quality. If the analysis classifies a code snippet as problematic, it should help the developer to improve the snippet by providing information about the problem. Ultimately, this should train the developer to spot problematic code by its own and to write clean code by default, so the number of alerts should decrease. At the same time, the overall software quality of a project increases immediately at rewriting a marked snippet and in the long term at training the developer to write code with higher quality.

In order to use the tool effectively, the design and implementation should cover the following requirements:
\begin{description}
    \item[Useability]:  The CCAP should be an easy-to-use tool. Developers shall be able to install and run the tool. The extra effort of using this tool should be small and the developer should use the tool in his day-to-day workflow without additional friction. The developers can interpret the issue and localise the problematic code spot immediately.
    \item[Expandability]: The extension of the detected code problems should be easy. A clear defined interface for extensions is required so an extension developer would not need specific knowledge about the internal architecture of the tool. The expandability allow a desired workflow of a developer finding problematic code in a e.g. peer-review, formalizing it into an extension and sharing this extension with the team. With each iteration, the code quality of all team members would increase.
    \item[Integration]: The tool should be easy to integrate into different systems. This includes lokal workflows like git pre-commits or build systems and remote continous integration/delivery/deployment pipelines.
\end{description}
A more specific requirement is Python as an input language and the expansion langauge. After JavaScript, Python is the second most popular programming language 2019 according to the Github statistics (\url{https://octoverse.github.com/#top-languages}). Besides the general popularity, Python is heavily used in the scientific community for machine learning and in universities for teaching programming. These groups are part of the potential target audience and students in specific can benefit from automated reporting of low quality source code.

\section{Architecture}
The CCAP architecture is divided into a static part and two extension possibilities: An extension with analysis plugins adds more rules that are validated by the system. Adding an output plugin allows to specify the output format to fit custom workflow needs.
The static part consists of four components: A core component to act as a orchestration unit, a  component for handling the source code input and a component for handling the analysis plugins as well as output plugins.The design follows the requirements and goals for the plattform. 
(TODO: High level architecture schematic diagram)

The core component contains the main function and handles the argument validation and parsing. Furthermore, it orchestrate other components by initializing and executing those. This process is divided into the argument parsing, initialisation and execution phase:
In the argument parsing phase, the command line arguments are parsed and validated. It valdiates the existance of the required input directory argument and the optional plugin path configuration for the analysis plugins and the output plugins. Additionally, the logging level and the output format can be defined. The latter determines, which output plugin will be used, although the existance of the specified plugin is not validated in this phase. A parsing or validation error will cause a program termination without further processing.
The initialisation phase instantiate all components and the analysis plugin handler will scan the specified directories for plugins and keeps an index of all found plugins in memory. The output plugin manager scans for an output plugin, that satisfies the specified argument. If no plugin matches the output argument, the programm will terminate with a one exit code and a failure message to indicate the problem.
In the execution phase, the input handling component scans the input directory for files ending with \textit{.py} and parses the source code into an Abstract Syntax Tree (AST) per file. In the next step, the core passes the parsed data to the analysis plugin handler. The latter will execute all plugins on all files and collects the results. Afterwards, the core component calls the output plugin component to output the results. If no exceptions occur during the execution phase, the program will be terminated with a zero exit code indicating the succesfull run.

The input component scans the given input directory for all python source code files and parse the source code into an AST. 
For scanning the input directory, an algorithm will walk recursively over all folders and files. The detection of python source code files is based on the file ending \textit{.py}. The algorithm will return a list of file paths and the corresponding file content. 
Next, the AST parser is called and will add a parsed AST object to the list besides the file path and content. This list will be passed to the analysis plugins by the analysis plugin handler.  An alternative approach would be to not read and parse the code in the input component, but instead let the plugins read and parse the file content if needed. With many files to scan, the latter approach would have a lower memory footprint since the file content and the AST will not be held in memory. Consequently, every analysis plugin has to perform an expensive read operation from disk and the performance scales with the number of files and the number of analysis plugins. 
If the input component reads all files, the information are held in the main memory and the performance only scales with the number of files. Since the files are text-based files, the number of files needed to excess the main memory is expected to be high enough to fit most projects. (TODO sample calculation?).

All analysis plugins are managed by the analysis plugin handler. This component finds all plugins, executes the plugin and collects the reported results.
During the initialisation phase of the core component, the analysis plugin handler will scan the plugin directory for all python files. It imports all python files and scans those for classes, that inherit from an abstract \textit{AbstractAnalysisPlugin} class. The abstract class defines all methods that need to be implemented in the concrete plugin subclass. All found classes are instanziated by the analysis plugin handler.
During the core execution phase, the core receives a list with the file name, file content and the parsed ast. All plugins are called on a specific entry point method that is defined in \textit{AbstractAnalysisPlugin}. The plugin will return an instance of \textit{AnalysisReport} with the plugins metadata and a collection of problems. The report is collected for every plugin into an \textit{FullReport}. Additionally, the report contains information about the overal plugin execution time and run arguments. After all plugins have been executed for all files, the analysis plugin handler returns the \textit{FullReport} to the core component.

The last component in the execution chain is the output plugin handler that will pass the \textit{FullReport} to the specified output plugin. It implements the same algorithm as the analysis plugin handler to find all plugins that inherit from \textit{AbstractOutputPlugin}. Instead of keeping track of all plugins, only the plugin that correspond to the output format argument is instantiated. The output plugin has an entry point as defined in \textit{AbstractOutputPlugin}  that is called with all the collected results.

\subsection{Analysis Plugins}
Analysis plugins provide the easy extendability of the plattform by developers. All users of the tool can expand the set of problems it can detect by implementing a plugin python and placing it into the plugin directory of the tool. In order to be compatible with the core compoennts, a plugin has to be a class that inherits from the \textit{AbstractAnalysisPlugin} class. Firstly, the abstract class indroduces a class member variable \textit{metadata} of the class \textit{PluginMetaData}. A concrete plugin class sets this class member in the constructor to provide a plugin name, author and optional website. This meta data is used in the output to show more information about the plugin that reported a problem. Secondly, \textit{AbstractAnalysisPlugin} specifies a \textit{do\_analysis} method that serves as a entrypoint that the plattform will call. It accepts a \textit{ParsedSourceFile} object, that contains a file path, the file content and the corresponding AST. With this information, the plugin can implement any logic to detect problems. 
For example, the plugin can traverse the AST to look for specific node types, it can use a tokenizer on the files content and analyse the token stream or it can run sophisticated machine learning algorithms on the source code.
The plugin can even import third-party libraries, although they have to be installed by the user on the system. After detecting all problems, the plugin returns an \textit{AnalysisReport}. The report contains the plugins metadata and a list of found problems. These problems are instances  of a problem class inheriting from \textit{AbstractAnalysisProblem}. The abstract class expects a file path and line number as constructor arguments and requires the plugin developer to override the problem name and description. (TODO Refer to the sample in Return None plugin). The problem name and description will be shown in the final output and should follow the following guidelines:
\begin{itemize}
    \item Have a suitable name that allows experienced developer to quickly recognize the problem.
    \item Explain what code construct is problematic.
    \item Give reasons why this code is seen as problematic.
    \item Show guidance and examples on how to fix the problem and improve the code.
\end{itemize}
Although it is possible to use one plugin for multiple, different problem types, having one plugin for one problem type helps to reuse and share the plugin. Additionally, it can be disabled easily by removing the plugin from the plugin folder.

\subsubsection{Steps to create an analysis plugin}
In order to expand the CCAP with an analysis plugin, the following steps are required for a developer:
\begin{enumerate}
    \item Create a \textit{.py} file with a class inheriting from \textit{AbstractAnalysisPlugin}.
    \item Instanziate \textit{PluginMetaData} and assign it to the metadata member.
    \item Define a problem class inheriting from \textit{AbstractAnalysisProblem} and set the problem name and description following the guidelines above.
    \item Implement the \textit{do\_analysis} method with a \textit{ParsedSourceFile} parameter. Return an \textit{AnalysisReport} instance with all found problems.
    \item Place the \textit{.py} file into the analysis plugin directory of the tool.
\end{enumerate}
Whereas these are the minimum required steps to implement the plugin, the developer is free to add additional methods, classes or import libraries as necessary.
Furthermore, it is adviceable to implement several tests to ensure the plugins correctness. CCAP uses pytest \footnote{\url{https://docs.pytest.org/en/stable/}} for testing, implementing a test is as easy as writing a function with a \enquote{test\_} prefix and running the \textit{pytest} command. TODO: further test instruction after test implementation

\subsection{Output Plugins}
With output plugins, CCAP adds additional flexibility towards the output format. Depending on environmental requirements, the output can be addapted with custom logic by defining an output plugin. For instance, running the tool localy could write the results to the standard output in an human-readable way or create a formatted html file to be displayed in the browser. Running inside automated workflow, a machine-readable json output may be preferred.

Output plugins follow similiar concepts as analysis plugins. A plugin class inherits from \textit{AbstractOutputPlugin}. The abstract class introduces the metadata member of the \textit{PluginMetaData} class to encapsulate plugin name and author information. Additionally, a second member variable \textit{output\_format} has to be defined with a short abbreviation for the output format. This field is used by the the output plugin handler to select the output plugin in accordance to the run arguments. Consequently, it should be unique, otherwise, the first found plugin with a matching \textit{output\_format} field is selected by the output plugin handler. Therefore, using a custom prefix string is recommended.

As entrypoint method, the method \textit{handle\_report} has to be implemented. The method provides an instance of \textit{FullReport} as its argument. The  \textit{report} field holds a collection of  \textit{AnalysisReport} for every analysis plugin that has been executed. With metadata and problem information, the output plugin has access to all reuqired information to produce a desired output.

\subsection{Steps to create an output plugin}
To expand the output capabilities of the CCAP, the following steps are, similiar to analysis plugins, required:
\begin{enumerate}
    \item Create a \textit{.py} file with a class inheriting from \textit{AbstractOutputPlugin}.
    \item Instanziate \textit{PluginMetaData} and assign it to the metadata member.
    \item Set the \textit{output\_format} field with a unique abbreviation for this output format. Since it should be unique, a custom prefix prevents a name collision with preexisting plugins.
    \item Implement the \textit{handle\_report} method with a \textit{ParsedSourceFile} parameter. 
    \item Place the \textit{.py} file into the output plugin directory of the tool.
\end{enumerate}

Vorteil: läuft lokal, time to feedback ist geringer als bei cloud-basierten ansätzen wie codacy. (läuft bspw als git pre-commit hook oder direkt als VSCode plugin)