This section is divided into the approach for the Clean Code Analysis Plattform and the Clean Code classification.

\chapter{Clean Code Analysis Plattform}
The goal of the design and implementation of the Clean Code Analysis Plattform (CCAP) is a tool for software developer to imnprove the code quality of existing and new code. The tool accepts an directory containing source code files as input and analyses the input for snippets of improveable code quality. If the analysis classifies a code snippet as problematic, it should help the developer to improve the snippet by providing information about the problem. Ultimately, this should train the developer to spot problematic code by its own and to write clean code by default, so the number of alerts should decrease. At the same time, the overall software quality of a project increases immediately at rewriting a marked snippet and in the long term at training the developer to write code with higher quality.

In order to use the tool effectively, the design and implementation should cover the following requirements:
\begin{description}
    \item[Useability]:  The CCAP should be an easy-to-use tool. Developers shall be able to install and run the tool. The extra effort of using this tool should be small and the developer should use the tool in his day-to-day workflow without additional friction. The developers can interpret the issue and localise the problematic code spot immediately.
    \item[Expandability]: The extension of the detected code problems should be easy. A clear defined interface for extensions is required so an extension developer would not need specific knowledge about the internal architecture of the tool. The expandability allow a desired workflow of a developer finding problematic code in a e.g. peer-review, formalizing it into an extension and sharing this extension with the team. With each iteration, the code quality of all team members would increase.
    \item[Integration]: The tool should be easy to integrate into different systems. This includes lokal workflows like git pre-commits or build systems and remote continous integration/delivery/deployment pipelines.
\end{description}
A more specific requirement is Python as an input language and the expansion langauge. After JavaScript, Python is the second most popular programming language 2019 according to the Github statistics (\url{https://octoverse.github.com/#top-languages}). Besides the general popularity, Python is heavily used in the scientific community for machine learning and in universities for teaching programming. These groups are part of the potential target audience and students in specific can benefit from automated reporting of low quality source code.

\section{Architecture}
The CCAP architecture is divided into a static part and two extension possibilities: An extension with analysis plugins adds more rules that are validated by the system. Adding an output plugin allows to specify the output format to fit custom workflow needs.
The static part consists of four components: A core component to act as a orchestration unit, a  component for handling the source code input and a component for handling the analysis plugins as well as output plugins.The design follows the requirements and goals for the plattform. 
(TODO: High level architecture schematic diagram)

The core component contains the main function and handles the argument validation and parsing. Furthermore, it orchestrate other components by initializing and executing those. This process is divided into the argument parsing, initialisation and execution phase:
In the argument parsing phase, the command line arguments are parsed and validated. It valdiates the existance of the required input directory argument and the optional plugin path configuration for the analysis plugins and the output plugins. Additionally, the logging level and the output format can be defined. The latter determines, which output plugin will be used, although the existance of the specified plugin is not validated in this phase. A parsing or validation error will cause a program termination without further processing.
The initialisation phase instantiate all components and the analysis plugin handler will scan the specified directories for plugins and keeps an index of all found plugins in memory. The output plugin manager scans for an output plugin, that satisfies the specified argument. If no plugin matches the output argument, the programm will terminate with a one exit code and a failure message to indicate the problem.
In the execution phase, the input handling component scans the input directory for files ending with \textit{.py} and parses the source code into an Abstract Syntax Tree (AST) per file. In the next step, the core passes the parsed data to the analysis plugin handler. The latter will execute all plugins for all files and collects the results. Afterwards, the core component calls the output plugin component to output the results. If no exceptions occur during the execution phase, the program will be terminated with a zero exit code indicating the succesfull run.

The input component scans the given input directory for all Python source code files and parse the source code into an AST. 
For scanning the input directory, an algorithm will walk recursively over all folders and files. The detection of Python source code files is based on the file ending \textit{.py}. The algorithm will return a list of file paths and the corresponding file content. 
Next, the AST parser is called and will add a parsed AST object to the list besides the file path and content. This list will be passed to the analysis plugins by the analysis plugin handler.  An alternative approach would be to not read and parse the code in the input component, but instead let the plugins read and parse the file content if needed. With many files to scan, the latter approach would have a lower memory footprint since the file content and the AST will not be held in memory. Consequently, every analysis plugin has to perform an expensive read operation from disk and the performance scales with the number of files and the number of analysis plugins. 
If the input component reads all files, the information are held in the main memory and the performance only scales with the number of files. Since the files are text-based files, the number of files needed to excess the main memory is expected to be high enough to fit most projects. (TODO sample calculation?).

All analysis plugins are managed by the analysis plugin handler. This component finds all plugins, executes the plugin and collects the reported results.
During the initialisation phase of the core component, the analysis plugin handler will scan the plugin directory for all Python files. It imports all Python files and scans those for classes, that inherit from an abstract \textit{AbstractAnalysisPlugin} class. The abstract class defines all methods that need to be implemented in the concrete plugin subclass. All found classes are instanziated by the analysis plugin handler.
During the core execution phase, the core receives a list with the file name, file content and the parsed ast. All plugins are called on a specific entry point method that is defined in \textit{AbstractAnalysisPlugin} with the afforementioned list. The plugin will return an instance of \textit{AnalysisReport} with the plugins metadata and a collection of problems. The report is collected for every plugin into an \textit{FullReport}. Additionally, the report contains information about the overal plugin execution time and run arguments. After all plugins have been executed for all files, the analysis plugin handler returns the \textit{FullReport} to the core component.

The last component in the execution chain is the output plugin handler that will pass the \textit{FullReport} to the specified output plugin. It implements the same algorithm as the analysis plugin handler to find all plugins that inherit from \textit{AbstractOutputPlugin}. Instead of keeping track of all plugins, only the plugin that correspond to the output format argument is instantiated. The output plugin has an entry point as defined in \textit{AbstractOutputPlugin}  that is called with all the collected results.

\subsection{Analysis Plugins}
Analysis plugins provide the easy extendability of the plattform by developers. All users of the tool can expand the set of problems it can detect by implementing a plugin Python and placing it into the plugin directory of the tool. In order to be compatible with the core compoennts, a plugin has to be a class that inherits from the \textit{AbstractAnalysisPlugin} class. Firstly, the abstract class indroduces a class member variable \textit{metadata} of the class \textit{PluginMetaData}. A concrete plugin class sets this class member in the constructor to provide a plugin name, author and optional website. This meta data is used in the output to show more information about the plugin that reported a problem. Secondly, \textit{AbstractAnalysisPlugin} specifies a \textit{do\_analysis} method that serves as a entrypoint that the plattform will call. It accepts a list of \textit{ParsedSourceFile} objects, that contains the file path, the file content and the corresponding AST for all input files. With this information, the plugin can implement any logic to detect problems. 
For example, the plugin can traverse the AST to look for specific node types, it can use a tokenizer on the files content and analyse the token stream or it can run sophisticated machine learning algorithms on the source code.
The plugin can even import third-party libraries, although they have to be installed by the user on the system. After detecting all problems, the plugin returns an \textit{AnalysisReport}. The report contains the plugins metadata and a list of found problems. These problems are instances  of a problem class inheriting from \textit{AbstractAnalysisProblem}. The abstract class expects a file path and line number as constructor arguments and requires the plugin developer to override the problem name and description. (TODO Refer to the sample in Return None plugin). The problem name and description will be shown in the final output and should follow the following guidelines:
\begin{itemize}
    \item Have a suitable name that allows experienced developer to quickly recognize the problem.
    \item Explain what code construct is problematic.
    \item Give reasons why this code is seen as problematic.
    \item Show guidance and examples on how to fix the problem and improve the code.
\end{itemize}
Although it is possible to use one plugin for multiple, different problem types, having one plugin for one problem type helps to reuse and share the plugin. Additionally, it can be disabled easily by removing the plugin from the plugin folder.

\subsubsection{Steps to create an analysis plugin}
In order to expand the CCAP with an analysis plugin, the following steps are required for a developer:
\begin{enumerate}
    \item Create a \textit{.py} file with a class inheriting from \textit{AbstractAnalysisPlugin}.
    \item Instanziate \textit{PluginMetaData} and assign it to the metadata member.
    \item Define a problem class inheriting from \textit{AbstractAnalysisProblem} and set the problem name and description following the guidelines above.
    \item Implement the \textit{do\_analysis} method with a \textit{ParsedSourceFile} parameter. Return an \textit{AnalysisReport} instance with all found problems.
    \item Place the \textit{.py} file into the analysis plugin directory of the tool.
\end{enumerate}
Whereas these are the minimum required steps to implement the plugin, the developer is free to add additional methods, classes or import libraries as necessary.
Furthermore, it is adviceable to implement several tests to ensure the plugins correctness. CCAP uses pytest \footnote{\url{https://docs.pytest.org/en/stable/}} for testing, implementing a test is as easy as writing a function with a \enquote{test\_} prefix and running the \textit{pytest} command. TODO: further test instruction after test implementation

\subsubsection{Return None Plugin}
A rather simple analysis plugin demonstrate the capabilities of the CCAP: The Return None Plugin scans the source code for functions that return the None value. Returning None may result in runtime exceptions, if the function caller does not expect and handle a potential None return. Although the None can be returned directly or as a value of the returning variable, this plugin only focus on the explicit \textit{return None} statement to showcase the options for the developer to analyse the source code.

Detecting \textit{return None} is possible in multiple ways. The following shows the possibilities for developer to implement such a detector:
\begin{description}
    \item[Regular Expression]: In the \textit{do\_analysis} method, a developer has access to the source code as a string. Therefore, it is straitforward to use a regular expression to detect a \textit{return None} statement.
    \begin{lstlisting}[language=Python]
        import re
        matches = re.finditer(r"return None", source_file.content, re.MULTILINE | re.DOTALL)
    \end{lstlisting}
    Since the regex library only returns the start and end index of matches, these have to be converted to line numbers. Afterwards a \textit{ReturnNullProblem} instance can be created for every match and added to the \textit{AnalysisReport} for this plugin.

    This approach uses  regular expressions to match patterns in a string without utilizing the structureness of the code. It is a simple but powerful way and most developers are familiar with regular expression. On the flip-side, source code may have various ways syntactic ways to express a semantic. Consequently, the regular expressions have to be designed carefully to cover all variations. For instance, it is possible to encounter a doubled whitespace, that would break the afforementioned regular expression. Additionally, regular expressions do not operate on the structure of the code, therefore they can not detect high level patterns on the code structure.
    \item[Tokenization]: The process of dividing a character stream into a sequence of tokens is called tokenization (also know as lexical analysis). With the Python tokenizer, a token can contain multiple characters and has a token type like name, operator or intendation. A token sequence provides more information about the code structure that can be used to detect problematic patterns. 
    A token sequence for a \textit{return None} statemant would be the following: 
    \begin{lstlisting}
    [...(type: NAME, value: return), (type: NAME, value: None)...]
    \end{lstlisting}
    A simple algorithm would scan the sequence for two subsequent name tokens with the values "return" and "None". The abstraction level of a token sequence is higher than of a character sequence. Since the whitespace between "return" and "None" provide no semantic meaning, it is removed on this abstraction level. Whereas the regular expression based approach would have to deal with problems as the doubled whitespace, the token-based approach profits from the higher abstraction level and can access meaningful tokens directly.
    \item[Abstract Syntax Tree]: After tokenization, a parser takes the token stream and prases it into a hierachical datastructure like an Abstract Syntax Tree. With an AST the code is represented in an structural way and it is possible to traverse the tree following the structure of the code. The AST consists of nodes with different type and children, for example a node representing an if statement. It is possible to access the condition and the body of the if statement as descendant nodes in the AST. Analysing an AST allows an higher abstraction level then analysing the token string, since the  code strucutre is represented. Therefore, more abstract rules that analyse the code structure are possible.

    To detect a \textit{return None} statement in the abstract syntax tree, an algorithm would traverse all AST nodes looking for a return-typed node. If found, the value descendant contains the statement part after the return keyword. A \textit{None} value would be represented as a node of type constant or name constant (depending on the parser version). The value descendant of the constant node may then be checked for equality to \textit{None}. All AST nodes contain the corresponding line number, that are necessary for creating a problem message:
    \begin{lstlisting}[language=Python]
        for node in ast.walk(a.ast):
            if isinstance(node, ast.Return):
                return_value = node.value
                if isinstance(return_value, ast.Constant) or isinstance(return_value, ast.NameConstant):
                    if return_value.value is None:
                        problems.append(ReturnNullProblem(a.file_path, return_value.lineno))
    \end{lstlisting}

    Analysing the AST is best for problems in the code structure, since the AST represents the code structure in a well-defined, traversable datastructure. Although simpler patterns like the \textit{return None} could be detected using regular expressions, a detection on AST level allows to detect the semantic meaning instead of the syntactic representation of a problem. For more complex problems, it is inevitable to use the AST most of the time.
\end{description}

This implementation covers only the simple case, in which \textit{return None} is written explicitly. Of course, several modifications are possible, that would not be detected by this plugin and would required more sophisticated algorithms. Evaluating, if machine learning models trained on this simple rule can also detect such modifications is part of the second part in research question 3 (TODO: check, jump link).

Since detection alone is not a great help for the user, a usefull description of the problem is neccessary. 
As described in section (TODO: background, clean code, return none), there are some alternatives to returning none. If the none happens due to an error, it is better to raise an exception and provoke an explicit error handling, which prevent runtime errors. If the normal return type is a collection, an empty list is more appropriate, since the function caller will most likely write logic for an unknown amount of list items. But if the nornal return type is a single object, it is not obvious what to return. With PEP484 (TODO source), an optional type was introduced for type hints in Python 3. Using a static type checker for Python like mypy (TODO source) outputs a warning, if a developer forget to check an optional for not being none. 

\subsubsection{Condition with Comparison}
The second plugin detects a direct comparison in an conditional statement. For better readability and understandability, it is suggested to use a function call with an appropriate name that evaluates to true or false. This allows a natural reading of the conditional statement without deciphering the meaning of the boolean logic.

An if-statement consists of an condition and body part. If the condition evaluates to true, the body is executed. For the condition, any logical expression will be evaluated. A simple algorithm would take the AST, search for if nodes and check, if the condition part is a compare node. 
But negation and logic AND/OR would not been detected by the simple approach. Consequently, a recursive algorithm has to follow logic operators and checks the expressions for comparison nodes.

Following this considerations, an advanced algorithm would scan for if nodes in the ast. The conditional part as a boolean expression is evaluated in a recursive function that returns true if a direct comparison is done anywhere in boolean expression.
The conditional part can be a boolean operation (like AND/OR) or a unary operator (like NOT).  In these cases, the recursive function will be called again with the respective expressions. If the conditional part is neither a boolean operation nor a unary operator, a true is returned if the conditional part is not a method call. The last case would be the base case in the recursion. Listing \ref{lst:condition_coparison} shows the Python implementation of the recursion.

\begin{lstlisting}[language=Python, label=lst:condition_coparison, caption={Recursive function to analyse an if statement for direct comparisons. Since a condition should contain a method call, the function returns False if this is not the case.}]
def _check_if_direct_comparison(self, node):
    if isinstance(node, ast.BoolOp):
        violated = False
        #check all expressions of the boolean operator
        for value in node.values:
            if self._check_if_direct_comparison(value):
                violated = True
        return violated
    elif isinstance(node, ast.UnaryOp):
        return self._check_if_direct_comparison(node.operand)

    return not isinstance(node, ast.Call)
    \end{lstlisting}

\subsection{Output Plugins}
With output plugins, CCAP adds additional flexibility towards the output format. Depending on environmental requirements, the output can be addapted with custom logic by defining an output plugin. For instance, running the tool localy could write the results to the standard output in an human-readable way or create a formatted html file to be displayed in the browser. Running inside automated workflow, a machine-readable json output may be preferred.

Output plugins follow similiar concepts as analysis plugins. A plugin class inherits from \textit{AbstractOutputPlugin}. The abstract class introduces the metadata member of the \textit{PluginMetaData} class to encapsulate plugin name and author information. Additionally, a second member variable \textit{output\_format} has to be defined with a short abbreviation for the output format. This field is used by the the output plugin handler to select the output plugin in accordance to the run arguments. Consequently, it should be unique, otherwise, the first found plugin with a matching \textit{output\_format} field is selected by the output plugin handler. Therefore, using a custom prefix string is recommended.

As entrypoint method, the method \textit{handle\_report} has to be implemented. The method provides an instance of \textit{FullReport} as its argument. The  \textit{report} field holds a collection of  \textit{AnalysisReport} for every analysis plugin that has been executed. With metadata and problem information, the output plugin has access to all required information to produce a desired output.

\subsubsection{Steps to create an output plugin}
To expand the output capabilities of the CCAP, the following steps are, similiar to analysis plugins, required:
\begin{enumerate}
    \item Create a \textit{.py} file with a class inheriting from \textit{AbstractOutputPlugin}.
    \item Instanziate \textit{PluginMetaData} and assign it to the metadata member.
    \item Set the \textit{output\_format} field with a unique abbreviation for this output format. Since it should be unique, a custom prefix prevents a name collision with preexisting plugins.
    \item Implement the \textit{handle\_report} method with a \textit{ParsedSourceFile} parameter. 
    \item Place the \textit{.py} file into the output plugin directory of the tool.
\end{enumerate}

\subsubsection{Standard Output Plugin}
The Standard Output Plugin writes the formatted output to the stdout stream. It is enabled if no output plugin is explicitly specified. 

The output is divided into a general part and the problem list. The former contains the input path, all executed plugins, a total runtime and a summary field with the numbers of problems in total.The latter displays the list with problems, grouped by analysis plugin.  
For each problem, the problem name, file path, line number and description is printed. A colon sperates the file path and line number. Some terminals parse the path and line number, so a user can click the path and it opens the default editor with the cursor in the correct line. A sample output is shown in listing \ref{lst:stdout}

\begin{lstlisting}[label=lst:stdout]
Analysis Report on /Users/d064518/MA_CleanCodeAnalyser/test_programs.
Analyse Plugins: Condition Method Call Plugin, Simple Condition Method Call Plugin, Return None (Null) Plugin, Sample Plugin.
Total time: None
Summary: Found 16 problem(s)
    ----------------------------------------
PROBLEMS:
    PLUGIN NAME: Condition Method Call Plugin by Enrico Kaack <e.kaack@live.de>
        Found: Explicit comparison in condition in /Users/d064518/MA_CleanCodeAnalyser/test_programs/return_none.py:18
            Explicit comparisons in conditions should be replaced by method call for better readability
...
PLUGIN NAME: Return None (Null) Plugin by Enrico Kaack <e.kaack@live.de>
        Found: Returned None in /Users/d064518/MA_CleanCodeAnalyser/test_programs/return_none.py:4
            Returning None is dangerous, since the caller has to check for None. Otherwise, a runtime exception may occur.
\end{lstlisting} 

\subsubsection{HTML Output Plugin}
For larger projects, the Standard Output Plugin may not be sufficient to get an overview over the project or to generate a report. Therefore, a the HTML Output Plugin create a \textit{report.html} file, that can be opened in every browser and provides a cleaner overview. It has similiar general data at the top as the Standard Output Plugin, but it moves the problem description into an tooltip to offer a more compact overview. See figure \ref{fig:screen_html_output} for an example screenshot.

\begin{figure}
    \includegraphics[width=1\textwidth]{img/CCAP/screenshot_html_output.png}
    \label{fig:screen_html_output}
    \caption{Output of the HTML Output Plugin, dispalyed in a browser.}
\end{figure}

\section{Extensions}
While the core functionality exists, extensions for the CCAP would aim for improved useability for the user. One important improvement in useability would be an IDE integration for common IDEs like Visual Studio Code. A good starting point would be the Langauge Server Protocol\footnote{\url{https://microsoft.github.io/language-server-protocol/}}. This requires a language client as an Visual Studio Code extension and a protocol-compliant langauge server. The latter would wrap the the CCAP and modify it slightly to process a single, changed document and return the results. Since the CCAP architecture is modular, this is be possible without larger modifications. 

Annother feature would be a configuration to disable certain analysis plugins, that one does not want run on the project. With the current architecture, this would be possible by moving the analysis plugin file outside the plugin directory. A configuration with command-line parameters or with a per-project, hidden config file would have a better user experience. The latter could also be committed to the version control system so every team member has the same configuration.
Continuing on configuration possibilities, having configuration possibilities for plugins would increase flexibility for plugin developer. Introducing configurable warn levels like \enquote*{warning} or \enquote*{error} could help in continous integration pipelines to decide if a build succeed but the warnings are reported or if a build should fail because of error-level problems. Some rules could be seen as recommendations (warning level), whereas other rules would be inacceptable (error level).

Lastly, a feature to disable problem reporting for a specific code location could bring a boost in user acceptance. While some clean code rules are objective and can be measured precisely, some rules are more general recommendations that may not apply to every occurence of this situation. For instance, the clean code guidelines generally suggest not to have more than three function arguments, it may be necessary or even inevitable to have four arguments. The CCAP should report this as a problem but the user should be able to decide if its acceptable. In this case, the problem type on this specific location should not be reported in the future. In the current version, the user has no way to ignore a specific problem. Consequently, over time the number of problems the user has to ignore will accumulate until the user abandons the tool, since it does not provide an added value over the the frustration of manually ignoring problems.


Vorteil: läuft lokal, time to feedback ist geringer als bei cloud-basierten ansätzen wie codacy. (läuft bspw als git pre-commit hook oder direkt als VSCode plugin)