\section{Code Quality}\label{sec:code_quality}
The term code quality defines an evaluation of source code concerning the readability and understandability the source code. If source code is well-written and can be understand by developers easily, it is considered to be high quality.

One of the main reasons why companies and developers aim for high code quality is the maintainability of source code. With a better understandability of the source code, changing it becomes more efficient and less error prone.
Software maintanance consist mainly of those activities that involves chaning source code:
\begin{enumerate}
    \item Adding code for new feautres
    \item Implementing changing requirements by modifying code
    \item Fixing bugs to ensure correct functionality of the software.
\end{enumerate}
The importance of a good maintainability of source code is based on the assosiated cost.
For project planning, different sources suggest to plan 50 to 80\% of the total software development costs for maintainance. A recent study from 2018 between Stripe and Harris Poll found that developers spent 42\% of their time maintaining code. Increasing the effectiveness of maintaining code is therefore cost-relevant (SOURCE: \url{https://stripe.com/de/reports/developer-coefficient-2018}TODO).
The application of agile software development paradigms is another important contributor for code to be changeable. Agile development is a software development practice of building software in increments with a running software at the end of each increment.One of the advantages is the adaptability of this model to changing requirements, since with the end of each increment, a requirement change can be implemented. (TODO source Manifesto for agile software development).
The easiness of code changeability due to code quality becomes a business-critical requirement and can positively impact the maintainability in the follwoing ways\cite{baggen_standardized_2012}:
\begin{enumerate}
    \item Well-written code makes it easy to determine the location and the way source code has to be changed.
    \item A developer can implement changes more efficient in good code.
    \item Easy to understand code can prevent unexpected side-effects and bugs when applying a change.
    \item Changes can be validated easier. 
\end{enumerate}

Besides the maintainability, a higher code quality makes it easier for new developers to understand the code base and be productive, since it is easier to read and understand the code.

The understandability and readability of high-quality source code has implications going further than maintainability. The International Organization for Standardization provides the standard ISO/IEC 25000:2014 for \enquote{Systems and software Quality Requirements and Evaluation (SQuaRE)}\cite{iso_central_secretary_systems_2014}, that measures code quality by the contribution of the following characteristic:
\begin{enumerate}
    \item Reliability
    \item Performance efficiency
    \item Security
    \item Maintainability
\end{enumerate}
Reliability is the ability of a system to run without critical exception that would cause the system to crash, become unavailable or result in an inconsistent state. The most important factors for reliability are a correct multi-threading with access to shared ressources and correct memory allocation and deallocation. Performance efficiency describes a optimal use of the ressources like CPU-time, memory or network bandwith. The security aspect focus on code that does not open up as an attach vector. Especially common vulnerabilities like SQL injections shall be prevent.

To sum up, code quality is a concept to describe source code from the view of a developer ability to understand and change code. This view influence characteristics like maintainability or reliability that can be used as metrics to measure code quality. 

\section{Clean Code}\label{sec:clean_code}
In chapter \ref{sec:code_quality}, we defined the terminology of code quality. Yet the question remains, how to achieve a high code quality. One approach is the clean code concept, coined by the Robert C. Martin in its book \enquote*{Clean Code: A Handbook of Agile Software Craftsmanship}\cite{martin_clean_2009}. Basically, source code can be seen as clean code or as non-clean chaotic code. The author describes clean code principles in the form of practical rules. Those practical rules shall result in intuitive code that is easy to understand and read, i.e. the code quality increases.

Chaotic code on the other hand is harder to read and understand. The author argues, developers produce chaotic code in a conflict between deadline pressure and the necessary effort to make code more intuitive and follow clean code principles\cite{martin_clean_2009}. The former pressure is created by deadlines that focus on the visible output like the programs funcitonality whereas the latter is not directly visible in the final product. If the success criteria is solely based on the visible output, the code quality may suffer by fast-to-write chaotic code. This behaviour is shortsighten, since an accumulation of chaotic code reduces the productivity over time . A larger system with chaotic code will slow down later modifications or additions of code. With the costs assosiated with maintainability as described in chapter \ref{sec:code_quality}, reducing the maintainance effort is a logical step. With the clean code principles, developers can read and understand code in a more intuitive way and the described costs of chaotic code will decrease\cite{martin_clean_2009}.



The following sections explain the clean code guidelines following the book by Robert C. Martin \cite{martin_clean_2009}.TODO give a more detailed overview of the next chapters

\subsection{General Rules}
The following general rules are basic rules that act as a foundation for more specific rules.
Following these rules create essential building blocks for the understandability and readability of the code. 

The first rule is to follow standard conventions. Programming languages have conventions on formatting, naming, intendation levels and more. For the programming language Python, the PEP-8 style guide describes the conventions\footnote{\url{https://www.python.org/dev/peps/pep-0008/}} TODO to ref. If developers follow these conventions, the code becomes more familiar for other developers using the same conventions. Furthermore, it is common practice for big open-source projects to have their own contributing guidelines with coding conventions. For example, the Visual Studio Code GitHub repository contains a \enquote{How to contribute} documentation and a section on coding guidelines\footnote{\url{https://github.com/Microsoft/vscode/wiki/Coding-Guidelines}}.  Especially in such large open-source projects, many developers are working asynchronously on code. Therefore, having conventions and enforcing the compliance of the guidelines by rejecting change requests prevents the code from becoming a mosaic of different coding styles.

The second general rule is to keep things simple. This rule states, that aiming for a simple solution and encoding this solution in a simple code makes it faster to read and understand for developers.
A simpler software architecture enables a developer to modify code and grasp the impact of these modifications better, since a simple code does not hide dependent side-effects behind complex code. Unnecessary complexity on the other hand makes it harder to understand the code and dependent side-effects. This could increase the risk to introduce bugs into the software.

The third general rule states that everytime a developer touches the code, he should also improve the quality of the code or at least not worsen it. By doing a small fix and postponing the clean code principles, the code will become more chaotic until it is refactored. As a result, every change should keep at least keep the code level quality if not improving it.  

\subsection{Naming}\label{sec:naming}
The naming sections covers several rules for naming variables, functions, types and classes.
Good naming can make it easier to read and understand code. \enquote{Good} naming is an opinionated topic; For Robert C. Martin, the key factor of good naming is the descriptiveness of names\cite{martin_clean_2009}. Descriptive names provide the reader with enough information to understand what a variable stores or what a function computes. Abbreviations or symbolic names like \textit{a1, a2} are not descriptive and do not provide information about the meaning. Using symbolic names for mathematical expressions could be an exception to the rule, if the naming mirrors the expression.

To come up with a descriptive name, it is helpful to include a verb or verb expression for function names, because functions express an action. For class names, a noun emphasize the object character of class.

Another helpful propertie of a name is its pronouncability, since it is easier to read and to talk about the code with other developers. Consequently, it is useful to make longer but descriptive and pronouncable names, especially, since the autocomplete feature of IDEs will free the developer from typing the long name. Additionally, searching for long names works better than for short names, because long names are more likely to be unambigious compared to shorter names. An acceptable exception to this rule are short variables in a small scope (e.g. variable \textit{i} in a loop), but using \textit{i} in a large scope could be ambigious and troubleful for searching and undertanding.

An old naming convention is the Hungarian naming convention. In Hungarian naming, the type is encoded as a prefix of a variable name. TODO sample and source. Nowadays, the type encoding is redundant to the automatic type inference of IDEs. The automatic type inference also prevents confusion for the reader if the type in the notation and the actual type are inconsistent. The same logic applies to a prefix for member variables and methods, since a IDE can automatically highlight those different tokens.

\subsection{Functions}\label{sec:functions}
Functions should be small in length. Exceeding 20 lines should not be neccessary in most cases. If a function is small, it can be read more easily and without scrolling. Inside a function, if-, else- and while-statements should contain a function call for the condition and one function call for the body.  By following the naming schemas in section \ref{sec:naming}, the function calls document the meaning of the condition in an intuitive way without the need for additional comments. Consequently, function calls replaces nested structures in a more readable way.

A general guideline for functions is to fullfill one purpose. This is a rather unpractical view, since functions may have to call several functions subsequently to perform the required computation. Therefore, Robert C. Martin specifies that to one abstraction level per function. A low abstraction level handles data access and manipulation, e.g. string manipulation. On a middle abstraction layer, these low-level operations are orchestrated. On the next higher level, the mid-level functions are arranged etc. Switch-Statements violate the one purpose rule and is prone to be duplicated in several other code locations. Since they are a sometimes neccessary construct, placing switch-statements into an Abstract Factory and creating different Subclasses for the different behaviours, the switch-statement can be replaced by polymorphism. 

The number of function arguments should be three or lower. With many function arguments, it becomes harder to use the function. Additionally, testing becomes harder with more arguments; especially if all argument combinations should be tested. Functions with none or one argument simplifies testing alot. Since testing reduces the number of bugs, testable code is crucial. 
More arguments should be bundles in a config object or class, so many arguments are passed as one. Furthermore, using config objects enables grouping of arguments for the same context.
A special focus lies on "output arguments"; these get passed as reference and are mutated in the function. Output arguments are common in low-level languages like C, but they require additional attention by the reader. Functions are expected to use the input values and return the output as a return value. Consequently, if a developer fails to notice an output argument, he may does not expect it to be mutated by the function and could face a logical error. Using function arguments as immutable is the most intuitive way. Although it is sometime necessary to mutate the input arguments in a function for performance reasons (TODO source).

A particular bad practice in the function body are side-effects. Side-effects happens, if a function modifies a variable outside its scope without explicitly mentioning this in the name. This leads to dependencies between the functions that are not obvious. A developer who uses the function checks the signature (meaning the name, input arguments and return type) and expects the funciton to do what the name suggests. If the function has a side effect, it is an unintended behaviour and will lead to mistakes that are time-consuming to debug. Especially side-effects that initialize other objects result in a time-dependency that is hard to identify and could be overseen at all.

Following the one purpose per function guideline, a function should either perform an action or retrieve information. Especially actions that return a value are unintuitive, since it is not clear if the return value encodes the success state or indicate something else.
Furthermore, returning error codes violates this rule and errors should be indicated by raising an exception, if the language supports it. This allows a better seperation between aplication logic and error handling code. The code for catching and handling an exception may be seperated into an additional function, to provide a clean structure for higher level functions. In this case, error handling counts the one purpose of this function.
A general, important principle for software engineering applies directly to functions: Don't repeat yourself. Repeated Code is dangerous and chaotic, since it requires changes in multiple locations if it has to be modified. This makes duplicated code very prone to copy and paste errors and small mistakes that may not be obvious during development but will lead to a fatal crash at runtime. Many patterns and tools have been developed to mitigate duplicated code (TODO: some references to duplicated code detection etc).

\subsection{Comments}
Comments are part of every programming language and can play an important role in code quality. Good comments clarify the meaning of the code and help to understand the code. However, comments can become outdated and wrong or provide useless information. In a perfect world, the programming language and the programmer would be expressive enough that commenting is not required for clarity. Following the clean code guidelines for naming and functions makes many descriptive comments obsolete, since the function description is encoded in the function name.

Comments will not help to turn chaotic code into clean code. If a developer explains a line of code by a comment, it is often more helpful to call a function with a descriptive name to replace the comment. Since comments are not part of the programm logic, if the explaining comment is not updated with the code, the comment gives missinformation to the reader like described before.

In brief, before writing a comment, the developer should think about expressing the same in code. Robert C. Martin gives some exceptions for good comments \cite{martin_clean_2009}:

\begin{description}
    \item[Legal notes:] Legal notes like copyright or license information and author mentions may be necessary. Although they should be short in link to an external licensing document in full extent.
    \item[Explaining comments:] Some explaining comments can be helpful and are not easy to encode in normal code. For instance an explanation of special encodings or file formats are better to understand.
    \item[Intention:] Explaining an intention in a comment that is not obvious by the source code is also a valid use for a comment and can help to understand code that otherwise would not make any sense.
    \item[Warning for consequences:] Some parts of the code can have special consequences like not being thread safe or using many system ressources. A warning can help a developer from using a function and having trouble.
    \item[Emphasize:] A comment to emphasize a seemingly unimportant part of the code prevents breaking modifications of the code. 
\end{description}

\subsection{Data Structures and Objects}
Data Structures and Objects can be easily mixed together, but there purpose is converse. Objects hide data behind abstractions and has funcitons that work with these data. Conversely, data structures expose the data and do not provide functionality. Based on this definition, the Law of Demeter (TODO source) is defined: Objects should know as little as possible about the implementaion of objects it manipulates. As a result, code becomes decoupled, since objects do not depend on specific data strucutres of other objects.
In object-oriented programming, a method m of an object O may only call methods of the following components:
\begin{itemize}
    \item the object O itself
    \item the parameters of the method
    \item objects that are created within m's scope
    \item instance variables of O
\end{itemize}
The method should not call methods of objects, that are returned from the allowed objects. If a method calls specific functions of this objects, it assumes this behaviour. As a result, the two objects are closely coupled. 
Such bad code practice is called train wreck, since subsequent method calls look like multiple train carriages. A split in multiple variables and method calls improves the Train Wreck. Nevertheless, method calls on objects assume knowledge of the internal implemenation of the objects and would violate the Law of Demeter. However, if the methods are accessor functions, it would not violate the law, since the method calls are just an access to the data structure. Instead of calling a method of an object and calling a method on the returning object, the first called object could provide a function that takes care of calling the other object. This reduces the coupling between the three objects and would preserver the Law of Demeter in all objects.

Data structures are often used as Data Transfer Object (DTO) to communicate with other processes or services. These objects do not contain any functions, they only have accessible member varaiable. A sepcialisation of DTOs are Active Records, that contain additional methods for data storage, since they are used to represent a data source like a database. For both types, it is bad practice to insert business logic into this objects, since they should be treated as data structures. This creates a hybrid between an object and a data structure that should be avoided.

\subsection{Classes}
A Class should be small like functions. Instead of counting lines, the size of a class is the number of responsibilities. The name of the class should describe its responsibility, therefore, a single responsibility per class is intended. This is called Single-Responsibility-Principle and it restricts a class to one responsiility. A class should only have one reason for modification, consequently a class has a single responsibility (TODO source). The Single-Responsibility-Principle lead to a system of many small classes with one, clearly defined responsibility.

In addition, classes should have a high cohesion. Cohesion is high, if a method manipulates many instance variables. If all methods of a class manipulate all instance variables, the class is maximal cohesive. With a high cohesion, the methods and the class can be seen as a logical unit. A low cohesion indicates to split the class into multiple classes, since the methods are not a logical unit. As a result, classes gets smaller and will more likely comply to the Single-Responsibility-Principle. (TODO add metric for cohesion)

Depdendencies of classes are critical, since they may be modified. If the class depends on specific details of the implemenation, it has to be changed, if the dependency changes. Therefore, classes should depend on abstract classes that describe a concept, not the implementation details. Since the concept is unlikly to change if the implementation details change, the dependency is isolated. 

\subsection{Exception Handling}\label{sec:background:returning_none_and_error_handling}
Section \ref{sec:functions} briefly mentioned a clean code guideline for exeception handling. Exceptions should be handled by raising and catching exceptions instead of error codes or flags. Error code handling introduces code, that is not coherent with main logic. Error codes have to be checked imideatly and error handling has to be implemented directly. As a result, the two concerns main logic and error logic are mixed. By raising and catching errors, the error handling can be completly extracted into a sperate function and can be removed from the main logic. Furthermore, the try-catch block enforces a transactional behaviour. At the end of either the try or the catch block, the application should be in a consistent state. Error codes hide this explicit tranactional behaviour. Notwithstanding, Go as a more recent programming language\footnote{designed in 2007 as a response to problems with C++, Java and Python\cite{noauthor_go_nodate}} returns an explicit error types instead of raising an exception. This was designed to \enquote{encourage you to explicitly check for errors where thy occur}, instead of \enquote{throwing exceptions and sometimes catching them}\cite{gerrand_error_2011}.   
Exceptions should contain enough context to find the cause and the source of the error. Exception classes accomplish the grouping of cause source of the error. 

In case of error handling with a thrid-party libary, the library called should be wrapped and multiple, possible exception types should be grouped into a unified library exception type. The wrapping reduces the coupling to the third-party dependency. If the library changes (e.g. changing the signature and adding another exception type), only one location has to be adapted to comply to the changed library interface. Additionally, changing to another library is simple and tests can mock the third-party library. For exception handling, wrapping unifies the exception from the library that is clearly distinct from own exceptions.

Another form of exception handling is returning null. Instead of checking for an error code, the caller method checks for a null return value. As easy it is to miss an error code, as easy it is to miss a null checking. Consequently, the programm will terminate with an unhandled NullPointerException at runtime. Tony Hoare introduced the null reference in 1965 and later called it his \enquote{billion-dollar mistake}\cite{hoare_null_2009}, since it lead to many bugs and security vulnerabilities throughout the decades. Languages like Kotlin are designed with Null Safety enforced. Kotlin distinguishes between nullable and non-nullable references, with a compiler enforcing null checking\cite{noauthor_null_nodate}. If a language does not enforce null safety with a compiler, a special case like an empty collection or an optional type can be returned. Passing null to a function as a paramter is also dangerous, since a function would explicitly assert for non-null paramter values. Since this is not a clean way, null paramters should not be used.

\subsection{Code Smells}
Aforementioned guidelines can be rephrased as code smells. Code smells are a characteristic of code that possibly indicate a problem with the clean code rules and therefore maintainability of the system.

\subsection{Additional Guidelines}
Robert C. Martin describes more guidelines for system design, multi-threading, testing and third-party code\cite{martin_clean_2009}. These topics are not part of this chapter, since the focus of this work are the aforementioned guidelines and principles. 


\section{Quantitative Metrics for Code Quality}
Quantitative metrics  express code quality as a quantitative unit. 
This has multiple advantages:
\begin{itemize}
    \item A metric sums up the quality of a project in a single unit.
    \item A quantitative approach tracks the changes in code quality over time. Therefore, it is obvious if code quality improves or not. In case the quality  undercuts a threshold, special measures like mandatory refactoring can be undertaken.
    \item A developers performance can be evaluated  based on the code quality. Since the maintainability and reliability of the software depends on the code quality, this is a good incentive to enforce high quality work.
    \item A certain level of code quality can be required by a contract. As a result, a customer can expect less bugs and a smaller maintenance effort.
\end{itemize}
Metrics for code quality are a subset of more general software metrics. The following sections describe software metrics that express code quality.

\subsection{Cyclomatic Complextiy}\label{sec:cyclomatic_complexity}
Cyclomatic Complexity is a metric for the complexity of a section of code. It was introduced by Thomas McCabe, Sr.  Is is measured by counting the number of linearly independent execution paths\cite{mccabe_complexity_1976}.

To compute  the Cyclomatic Complexity, the control-flow graph is required. The control-flow graph represents possible execution paths. A node represents  a basic block, a code sequence without branching. A directed edge represent jumps in the control flow; e.g. an if-statement without an else-case has two edges. One edge to the basic block in the if-branch  and one edge to the basic block after the if-statement. Beside cyclomatic complexity, the control-flow graph is used for compiler optimisation techniques like dead code elimination and loop optimization (\url{https://ag-kastens.cs.uni-paderborn.de/lehre/material/compii/folien/c101-217a.pdf}).

The cyclomatic complexity on a control-flow graph is defined as:
\begin{displaymath}
    M = E - N + 2P
\end{displaymath}

E is the number of edges, N the numder of nodes and P the number of connected components.  A connected graph is a subgraph, in which all nodes are connected to each other by a path. P represents the number of subprograms (like multiple functions or classes will).
Following this definition, the cyclomatic complexity can be calculated.. See figure TODO for a visualisation.

MacCabe recommends to limit the cyclomatic complexity to 10. NIST confirms this recommendation in TODO. A lower cyclomatic complexity improves testability, since the complexity represents the number of execution paths that need to be tested. Therefore, $M$ is the upper bound for number of test cases for full branch coverage. 
Furthermore, studies suggest a positive correlation between cyclomatic complexity and defects in functions. TODO source
Safety standards like ISO 26262 (for electronics in automobiles) or IEC 62304 (for medical devices) mandates a low cyclomatic complexity \cite{isotc_22sc_32_iso_2018, isotisotc_210_iec_2006c_210}.

\subsection{Halstead Complexity Measures}
Maurice Halstead introduced the Halstead complexity measures in 1977\cite{halstead1977elements}. It is a collection of multiple countable measure and relations.

The Halstead  metrics operates on a sequence of tokens that are classified to be an operator or an operand.

The classification enables to count the following properties:
\begin{itemize}
    \item $\eta_1$ as the number of distinct operators 
    \item $\eta_2$ as the number of distinct operands
    \item $N_1$ is the sum of all operators
    \item $N_2$ is the sum of all operands  
\end{itemize}
The total program vocabulary can be computed as:
\begin{displaymath}
    \eta = \eta_1 + \eta_2
\end{displaymath}
The program length is not calculated by the lines of code, but as a sum of all operators and operands:
\begin{displaymath}
    N = N_1 + N_2
\end{displaymath}
The voume of a program in terms of program length and program vocabulary is defined as: 
\begin{displaymath}
    V = N * \log_2{\eta}
\end{displaymath}
An important metric is the difficulty of understanding a program, defined as:
\begin{displaymath}
    D = \frac{\eta_1}{2} * \frac{N_2}{\eta2}
\end{displaymath}
The major contribution to difficulty is the number of distinct operators $\eta_1$.
The combination of difficulty and volumes is the effort for udnerstanding or changing code:
\begin{displaymath}
    E = D * V
\end{displaymath}
The effort translates into real coding time following the relation
\begin{displaymath}
    T = \frac{E}{18}s.
\end{displaymath}

The number of  bugs correlates with the effor following
\begin{displaymath}
    B = \frac{E^{\frac{2}{3}}}{3000}
\end{displaymath}

(TODO general critique etc. on this metric since it seems to be arbitrary)

\subsection{Software Maintainability Index}
The software maintainability index was developed by Dan Coleman and Paul Oman in 1994. 16 HP engineers evaluated 16 software systems and scored it in a range from 0 to 100, with 100 representing best maintainability\cite{coleman_using_1994}. 
Following a regression analysis, they identified the following equation to match the maintainability of the evaluated systems:
\begin{displaymath}
MI = 171 - 5.2 *\ln{\overline{V}} - 0.23 * \overline{M} - 16.2 * \ln{\overline{LOC}} + 50 * \sin{\sqrt{2.4 * C}}
\end{displaymath}
where $\overline{V}$ is the average Halstead Volume, $\overline{M}$ the average cyclic complexity, $LOC$ the lines of code and $C$ as fraction of comments.

The software maintainability index was defined many years ago with a limited sample size of developers and projects. Additionally, programming languages have changed significantly over time. A study by Sj√∏berg et. al suggest no correlation between the software maintainability index and actual maintainability effort in a controlled environment\cite{sjoberg_questioning_nodate}. Although the study only analysis four software projects and lacks generalisation, they find a strong anti-correlation with different maintainability metrics. Only the code size seems to correlate with the actual maintainability. The foormer seems to be consistent with a systematic review on software maintainability predictions and metrics by Riaz et. al\cite{riaz_systematic_2009}.



Kritik an Clean Code

??Teaching clean code, the paper from one german university

\section{Tools for Code Quality Analysis}\label{sec:tool_comparison}
It is good practice to use static code analysis tools to improve code quality. A static code analysis examines a program by analysing an abstract syntax tree, the control or data flow, a pointer analysis or an abstract (approximated) execution. In comparison to dynamic analysis, the code is not executed. The result of a static analysis can be based on approximations, so there might be false positive results. An expert has to examine the result and correct results if necessary\cite{prahofer_static_2017}.

Besides the use of static code analysis for compiler and optimisations, it is also helpful for analysing the code quality. Different categories of code quality related principles can be analysed:
\begin{description}
    \item[Code Guidelines:] A static code analysis can ensure compliance to structural, naming and formatting code guidelines. 
    \item[Standard Compliance:] A requiremnt for a software may be compliance to an industry standard like IEC 61508 or ISO 26262 \cite{noauthor_iec_2010,isotc_22sc_32_iso_2018}. Static code analysis can ensure or at least assist with the compliance.  
    \item[Code Smells:] Some code smells follow a known pattern and a static code analysis can detect those smells. Since code smells can be an indication for chaotic code, it is best if these smells are detected and removed early on.
    \item[Bug Detection:] Although bug detection with static analysis can not detect all bugs, every detected bug before a software release is important. Examples for detected bugs are unhandled expressions or concurrency issues \cite{delaitre_evaluating_2015}.
    \item[Security Vulnerability Detection:] Static code analysis tools can detect some security vulnerabilities like SQL Injection Flaws, buffer overflows and missing input validation with high confidence. Since these are some common, easy to exploit vulnerabilites, a analysis for security vulnerabilities can increase the security of the overall software \cite{wichers_source_nodate}.  
\end{description}

The following sections provide an overview of a few static code analysis tools with focus on code quality and maintainability. I selected them based on popularity and if the projects are still maintained.

\subsection{PyLint}
PyLint is a code analysis tool for the Python programming language. It is open-source and licensed under the GNU General Public License v2.0 and available for all common plattforms. PyLint can be executed as a standalone program or can be integrated into common IDEs like Eclipse. A continous integration pipeline may include PyLint as well to ensure an analysis on every build.

With PyLint, the developer can make sure the code complies to the PEP8 (TODO) style guide for python coding. This includes name formatting, line length and more. It does not calculate a metric for the code but instead warns about violated principles. Additionally, PyLint can detect common errors like missing import statements that may cause the program to crash at start or later at runtime. To support refactoring, PyLint can detect duplicated code and will suggest to refactor the code.

PyLint can be configured to ignore some checks and to disable specific rules. To expand the rule set, a developer can write a "checker", an algorithm to check for a specific rule. The algorithm can analyse the raw file content, a token stream of the source code or an AST representation.The checker can raise a rule violation by provididng the location information and the problem type to the PyLint framework and it will be included in the PyLint analysis report.

\subsection{PMD Source Code Analyzer Project}
PMD is an "extensible cross-language static code analyzer" for Java, JavaScript and more. As an open-source project, it is licensed under a BSD-style license and is svailable for MacOS, Linux and Windows based systems. It integrates into build systems like Maven and Gradle as well as into common IDEs like Eclipse and Intelij. PMD can run as part of a continous integration pipeline and is included in the automated code review tool Codacy (see TODO).

Depending on the target language, PMD supports different rules. For Java, PMD has rule in the following categories:
\begin{description}
    \item[Best practices:] Best practices include rules like one decleration per line or using a logger instead of a \textit{System.out.print()} in production systems.  
    \item[Coding Style and Design] Several rules to improve readability of the code like naming conventions, ordering of declerations and design problems like a violation of the law of Demeter, Cyclomatic complexity calculation and detection of god classes.
    \item[Multihtreading: ]  Rules to mainly prevent the use of not thread safe objects or methods. Due to the nature of multi-threading and unpredictable scheduling of threads, problems like unpredictable values of variables or dead locks may occur. Since they may not occur in every execution, they are hard to spot and to debug. Consequently, warnings of using non thread safe code may save hours of debugging.
    \item[Performance: ] These rules flag known operations that have hidden performance implications. For example, string concatenation with the plus operator in Java causes the Java Virtual Machine to create an internal String buffer. This can slow down the program execution if numerous String concatenations are performed.
    \item[Security:] Security wise, PMD only checks for hard coded values for cryptographic operations like keys and initialization vectors.
    \item[Error Prone Code: ] PMD checks for several known code structures that will or may cause a bug at execution. Some rules are part of the clean code principles described in \ref{sec:clean_code} and some rules are specific to the target language.
\end{description}

A user can expand the PMD ruleset in two ways: A XPath expression can be spezified and will be validated against the AST by the PMD framework. For more control, it is possible to write a Java-based plugin that implements a custom AST traverser. The latter allows for more sofisticated rules and checks.

\subsection{Codacy}
Codacy is a software to "automate your code quality" (TODO source) for more than 30 programming languages. It is available as a cloud-based subscription service with a free tier for open-source projects and a self-host option for enterprise customers.  Codacy runs as a cloud software and a user can connect a GitHub, GitLab or Bitbucket repository that will be scanned automatically on every push or trigger a scan of local files with a command-line program. Additionally, a badge can be added to the readme page to show off the analysis results.

Codacy can be seen as a plattform that runs multiple different "analysis engines". These analysis engines combine multiple tools depending on the langauge. For Python, Codacy uses  PyLint, Bandit (a scanner for security issues) and a metric plugin.
Due to the licensing of those analysis engines, the engines are open-sourced, wheras the Codacy platform is propriatary.

Depending on the language, Codacy supports scanning for common security and maintainability issues. The later is faced with scanning for Code Standardization, Test Coverage and Code Smells to reduce technical debt.

Codacy allow customization by disableing certain rules and changing rule paramters like the pattern to fit the rules to the projects preference. 

\subsection{Sonarqube}
Sonarqube is a analysis tool to maintain high code quality and security. It supports 15 programming languages in the open-source version licensed under the GNU Lesser General Public License, Version 3.0. In the Developer Edition, Sonarqube supports 22 languages and 27 languages in the enterprise edition. Sonarqube is a self-hostable server application with an SonarScanner client module that can be integrated in build pipelines like Gradle and Maven as well as a command-line tool for other build pipelines.The SonarScanner reports the result to the server, that serves a website to review the results. With the Developer Edition, branch and pull request analysis are possible and a pull request will be annotated with the analysis results.

For Python, Sonarqube offers more than 170 rules. These rules are in the following categories:
\begin{description}
    \item[Code Smells:] Code Smells like duplicated String literals are flagged with four different levels of severity (from higher to lower severity): Blocker, Critical, Major, Minor. Explanations and examples are accessible for the developer to understand and fix the issue. The analysis is more in-depth than AST-based solutions, since it additionally uses control and data-flow analysis.
    \item[Bug:] Multiple rules cover bugs that would definetively result in a runtime exception and program termination. As an example, calling a function with the wrong number of arguments will be flagged with the highest severity, since it will raise a TypeError during runtime. Although programmers may notice issues like this during coding and testing, the issue can remain hidden if it only triggers in a special execution path of the system.
    \item[Security Hotspot: ] The Security Hotspot analysis unfolds pieces of code that may be a real vulnerability and requires a human review. The scanning has a hotspot detection for seven out of the OWASP Top Ten Web Application security risks (TODO src). A flagged hotspot contain a detailed explanation of the reason for being flagged and a guide on how to review this hotspot. The recommendation to double check a piece of code can help to prevent a security vulnerabiliy from being deployed to production.
    \item[Security Vulnerabilities: ]  A security vulnerabilites analysis reveal code that is at risk of being exploited and has to be fixed immediately. Especially misconfiguration of cryptographic libraries can be revealed easily and future damage prevented.   
\end{description}

Additionally, Sonarqube offers several metrics like test coverage and an custom derivate of Cyclomatic Complexity named Cognitive Complexity\cite{campbell2018cognitive}. The authors sees several shortcommings in the original Cyclomatic Complexity model:
\begin{itemize}
    \item Parts of code with the same Cyclomatic Complexity do not neccessarily represent equal difficulty for maintenance.
    \item  Cyclomatic Complexity does not include modern language features like try/catch and lambdas. Therefore, the score can not be accurate.
    \item Cyclomatic Complexity lacks useability on a class or module level. Since the minimum complexity of a method is one and a high aggregated Cyclomatic Complexity for a class is measured for small classes with complex methods or large classes with low complexity per method (e.g. a domain class with just getter/setter).
\end{itemize}
Cognitive Complexity adresses these points, especially the incorporation of modern langauge features and meaningfullness on class and module level.

The calculation is based on three rules\cite{campbell2018cognitive}:
\begin{description}
    \item[Ignore Shorthand Structures]: Method calls condense multiple statements into one easy to understand statement. Therefore, method calls as shorthand are ignored for the score. Similiarly, shorthand strcutures like the null-coalescing operator reduces the Cognitive Complexity compraed to an extensive null check and is therefore ignored for score calculation.
    \item[Break in the linear control flow]: A break in the expected linear control flow by loop structures and conditionals adds to Cognitive Complexity as to Cyclomatic Complexity. Additionally, Catches, Switches, sequences of logical operators, recursion and jumps also adds to Cognitive Complexity, since these strcutures break the linear control flow.
    \item[Nested flow-breaking strcutures]: Flow-breaking structures that are nested additionally increase the Cognitive Complexity, since it is harder to understand than non-nested structures.  
\end{description}
The method-level Cognitive Complexity score represents a relative complexity difference between methods that would have the same Cyclomatic Complexity. Furthermore, the aggregated Cognitive Complexity on a class or module level now differentiate between classes with many, simple methods and classes with a few, but complex methods. As a result, domain classes (containing mainly getters/setters) have a lower score compared to classes with complex code behaviour\cite{campbell2018cognitive}.

Developers can extend Sonarqube similiarly to PMD. They can write a Java plugin that can acces the AST but also a semantic model of the code. Additional, extensions can be created that provide funcitonality to other extensions. The Java plugin is compiled into a .jar file and placed into the plugin dir of the Sonarqube installation. For simpler rules, XPath exressions are possible through the web interface and allow a quick extension. For instance, if developers observe bad code style during a pull request review, they can quickly write a rule to enforce this rule in all subsequent pull requests automatically.

Integration in continous integration, manual review

Tools review for all tools that check different stuff

Single Responsible principle, Open-Closed principle

