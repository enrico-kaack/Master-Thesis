\section{Summary}
Software maintenance is a considerable cost factor for software development. With high code quality, the maintenance efforts can be optimised. With the clean code principles, developers have a ruleset to write understandable and readable code that will positively impact code quality. The focus of this thesis lies on the automated checking of such rules.

We introduced a taxonomy of complexity level for the clean code rules, depending on the level of complexity for an automated checker. On the highest level, we grouped clean code rules that would potentially require machine-learning approaches to learn the detection of a rule violation. 

The contribution of this work is twofold: 
First, we designed an implemented the clean code analysis platform (CCAP). It is a tool that checks for clean code violations. One of the essential design aspects is the easiness to extend the tool to cover additional clean code rules. In comparison with preexisting tools, we see a high value for users and extension developers, that write automated checker for new rules. On the downside, it lacks the integration into different workflows. The CCAP is an addition to Sonarqube. The strength in extensibility supplement the rather complicated extensibility of Sonarqube. Furthermore, CCAP is a useful tool to teach about clean code principles.

Second, we compared different machine learning models on detecting violating code patterns. A support vector classifier was not able to complete the task with acceptable performance. The LSTM-based neural network outperforms all other models on the task, but it is sensitive towards less training data due to many trainable parameters.
A random forest classifier fulfiled our acceptance criteria. It can handle moderate class imbalance and profit from oversampling the training samples. The gradient boosting classifier is insensitive towards class imbalance which is a useful property for the task. When classifying for a single clean code violation, most lines of code do not contain the violating pattern, so the training dataset is imbalanced. The gradient boosting classifier also fulfils our acceptance criteria and performs better than the random forest on two out of three problematic code patterns.

Furthermore, we tried to simulate a situation in which the dataset may not cover the full variety of possible problem patterns. We tested the trained models on unseen problem variations. No model performed on an acceptable level. We assume, our models did not learn the structure of the problematic pattern but just the possible variations. 
We can think of two main reasons: First, the variation in the training set is too limited for the model to extract the general structure. We suggest to explicitly introduce variety by hand-generating different patterns to fine-tune the model and to use in the test set for hyperparameter tuning.
Second, our encoding approach does not utilise the structure of the source code. Since our models did not extract the structure from our encoding, encoding the structure as input features would ease the learning of the problem structure.  

\section{Future Work}
TODO
CCAP language server protocol IDE integration, different warning levels, disabling plugins, ignore a specific rule violation line