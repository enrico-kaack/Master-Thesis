\section{Research Questions}

\begin{description}
    \item[RQ1] What is the utility of the CCAP besides existing tools? 
    \item[RQ2] How do different models compare on the task of detecting non-clean code?
    \item[RQ3] Do machine-learning based models cover a larger variety of cases than rule-based checker? 
\end{description}

\subsection{RQ1: What Is the Utility of the CCAP Besides Existing Tools?}
\paragraph{Motivation}
Several tools are established that aim to improve code quality and detect unclean code. Every new tool with similar claims has to prove its usefulness beside the preexisting tools. We compare the CCAP with existing tools based on the design goals expandability, useability and integration. Additionally, we will mention and compare useful features unique to existing tools.
\paragraph{Approach}
To evaluate, if the CCAP is a useful addition, we compare different features to see, if it can supplement or replace existing tools. As described in section \ref{sec:tool_comparison}, the compared tools are Sonarcube, PMD Source Code Analyser Project, Codacy and PyLint.

\paragraph{Results}
Table TODO shows a summary of the features.

\subparagraph{Finding 1: Expandability}
For expandability of analysis plugins, CCAP is comparable to PyLint, that also offer plugins to analyse the raw string, the token stream or the AST. PMD allows plugins to analyse the AST or define XPath rules. Since XPath rules can be used in the CCAP as well (using a third-party library in the plugin), PMD offers fewer expansion possibilities. Sonarqube, on the other hand, provides the most possibilities for extension. Not only can developers analyse the AST, specify XPath rules, but they have access to an additional semantic model of the code that provides direct access to structures such as methods, parameters, return values. This semantic model simplifies analysis in comparison to the AST analysis.
Additionally, Sonarqube allows plugins to expose an API for other plugins to use. This different type of plugin further increases the possibilities for rule checking plugins. The least expandability offers Codacy, that allow customising or disabling existing rules, but does not offer to add rules to the existing ruleset.

Regarding output plugins, no compared tool offers the output customisation of CCAP. PMD and PyLint have different predefined output formats like JSON, HTML, CSV or text. Although PyLint allows customising the message format using a formatting string as a command-line argument, they do not offer extensions for the output. Sonarqube displays the analysis reports in its WebUI. The scanner tool, SonarScanner, sends the reports to the server component, that renders the results in the browser. Codacy's local scanner produces text output; the cloud version also has a WebUI.

\subparagraph{Finding 2: Integration shortcomings}
The CCAP has significant shortcomings in its integrations into IDEs, build processes and CI pipelines. All contestant tools provide plugins for build systems like Gradle or Maven. Except for Codacy, all tools have IDE integrations into the most common editors. Sonarqube and Codacy have integrations into source control systems. Same applies to PMD and PyLint since they are included in Codacy and therefore have the same integrations.

\subparagraph{Finding 3: Useability}
For a plugin developer, the useability of the plugin system is simple for CCAP and PyLint. Both tools have the same plugin mechanism and a simple plugin interface. PMD has a similar, straightforward plugin interface, but its Java plugin has to be bundled into a jar file before adding to the classpath. The latter applies to Sonarqube as well; additionally, the powerful and rich plugin API makes it harder to use.

For the user, installing CCAP is like installing every other python package. Running from the command-line is fast and can be automated, e.g. using git pre-commit hooks. The same workflow would be achieved with PyLint. Sonarqube is more complex to install due to its multiple components, but after the setup, the WebUI and integrations into most developer workflow have the best useability. Codacy as a cloud service only requires permission to access the source code repository to start scanning the code.

\subparagraph{Finding 4: Multi-Language scanning}
Sonarqube, PMD and Codacy offer multi-language scanning. This multi-language ability is advantageous for code repositories with multiple languages and for reusing the same tools and infrastructure for multiple projects with different programming languages. Supporting multi-language scanning is not possible with CCAP and would require a redefinition of the architecture. Although multi-language support was not a design goal, it would be a significant advantage for adoption.

\subparagraph{Finding 5: Maturity and Community-Support}
The maturity and community-support of tools impact the integration of the tools into the developer workflow. Some integrations are community-made and shared, so everybody has an advantage. The community support would also be necessary for CCAP to write and share additional analysis plugins and to integrate into different workflows.

\paragraph{Summary}
In summary, CCAP offers a good, but not best in class expandability with analysis plugins. It has a high useability for plugin developer and users. The lack of integrations for different workflows reflects the lack of community support and maturity. 
We see the CCAP as an addition to the powerful Sonarqube. The simple, yet robust expandability of the CCAP could supplement the shortcomings of Sonarqube in its powerful, but complex expandability. For instance, a code reviewer could code problematic code into a simple python plugin, distributes it to the team and would hopefully not reencounter the same problem.
Additionally, the CCAP could be used to teach about Clean Code and to enforce specific coding rules to students, since the local setup is straightforward. Clean Code rules could be turned into analysis plugins by students or teachers without having to understand a complicated interface.

\subsection{RQ2: How Do Different Models Compare on the Task of Detecting Non-Clean Code?}\label{rq:2}
\paragraph{Motivation}
As we have shown with the CCAP and the analysis plugins, it is possible to write an algorithm that can detect specific code patterns like non-clean code patterns. If we can write a well-tested detection algorithm, we do not need error-prone machine learning models to detect code patterns. Nonetheless, if we can not design an algorithm that detects the desired code pattern reliably, we may perform better using a machine learning model. Additionally, if we encode a subjective pattern, it may not be possible to design an algorithm to detect such a pattern objectively. As a solution, we could label code parts as our pattern and use machine learning approaches to extract rules to detect such a pattern implicitly.

For this research question, we will evaluate different machine learning models on different code patterns and compare the results.

\paragraph{Approach}\label{par:approach}
For answering the research question, we will use the approach detailed in chapter \ref{chap:clean_code_classification}.  The dataset, consisting of 18 projects from GitHub, is prepared as described in chapter \ref{chap:clean_code_classification_dataset}. The data is split into 90\% train and 10\% test data. We use the training dataset to train Random Forest, Gradient Boosting Classifier, Support Vector Machines and an LSTM-based neural network. Due to the class imbalance, we will additionally train with an oversampled dataset (oversampling rate of 0.5) and an undersampled dataset (undersampling rate of 0.5). Since our test set contains real-world code samples, we expect it to represent a real-world label distribution, and we, therefore, accept the potential effects of the data imbalance. Nevertheless, we will try to train with an over- and undersampled dataset to observe the effect on the performance.

Our evaluation metrics are recall, precision and the combined f1 score. Due to the data imbalance, accuracy is not a meaningful metric. A high recall means a high detection rate of non-clean code. The costs of a false negative prediction are potential costs in unclean code (maintainability, understandability). With high precision, a detected non-clean code sample is likely to be a non-clean code sample and the system reports less false positives. The immediate cost of false-positive predictions (resulting in a lower precision) is the extra developer time needed.
Additionally, the cry wolf effect comes into play~\cite{breznitz_cry_1984}. In our case, a developer who has seen a false alarm will take subsequent alarms less serious. Additionally, a study has shown that a higher false alarm rate in the advisory warning system in cars results in a lower, subjective evaluation of the system~\cite{naujoks_cooperative_2016}. A low subjective evaluation of developers would lead to a decrease in adoption rate, which consequently leads to more unfixed problematic code. Since both recall and precision are essential to the success of our models, we decided to use the equally weighted f1 score as a single-value evaluation metric.
Additionally, to compensate for the cascading effect of a low precision due to the cry wolf effect, we require a precision of 0.8 as a satisficing metric. Accordingly, we accept all models that return two false positive and eight true positive samples. On the other hand, we define the recall as our optimising metric.

We evaluate all models for the three different problem types RETURN\_NONE (RN), CONDITION\_COMPARISON\_SIMPLE (CCS) and CONDITION\_COMPARISON (CC). We expect the CC type to be the most difficult to learn since it is the most complex rule. CCS and RN should be easier to learn due to less possible variations in the code structure.

\paragraph{Results}
We list all hyperparameter configurations and the corresponding training, test and holdout performance for all problem types in the appendix in \Crefrange{tab:all_results_random_forest}{tab:all_results_svm}. We report a detailed analysis in a separate finding for each model. Additionally, we describe our findings when comparing the models for the given classification tasks.



\subparagraph{Finding 1: Support Vector Classifier} 
\textit{The support vector classifier in our configurations is not able to detect any problem type.}
Due to the quadratic time complexity during training described in \Cref{sec:svm_quadratic_complexity}, we were only able to test a few configurations. We tested configurations with a general subsampling and undersampling to reduce the input size and to achieve a manageable training time. Therefore, we do not have data for non-resampled datasets or oversampled datasets to draw reasonable conclusions.

We can only observe some effects:
First, the class balancing increases the recall for all problem types up to 0.79 for RN, 0.5 for CCS and 0.55 for CC. Second, undersampling with a ratio of 0.1 result in a f1 performance of zero for the tested RN and CCS problem types. Third, the performance for the CC type is double the performance for the CCS type (0.11 vs 0.06) and the performance for the CCS type is ten times the performance of the RN type. 
The zero performance with an 0.1 undersampling ratio may be an indicator of a strong sensitivity for class imbalance. But we have no supporting data for this hypothesis. Jiang Tian et. al describes a poor performance on imbalanced datasets with conventional support vecotr machines~\cite{tian_imbalanced_2011}.

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
    In conclusion, the support vector classifier does not fulfil our acceptance criteria of precision higher than 0.8. Especially the sensitivity for class imbalance makes the use for our problem domain with severely imbalanced datasets pointless.  
    Furthermore, train time limits our experiments. It is important to note that we did no further investigation in improving the train time or performance. Maybe we could have increased classification and runtime performance by choosing a different kernel function such as a linear kernel function or by following strategies adressing this problem as proposes by Tian et al.~\cite{tian_imbalanced_2011}. However, we decided to invest our efforts into the other, more promising models. 
    \end{minipage}}
\end{center}


\subparagraph{Finding 2: Random Forest}
\textit{Random forest classifier performs well and fulfils our satisficing condition on precision.} It achieves f1 scores of over 0.8 for the RN type and over 0.9 for the CCS and CC type.
The best performing random forest classifier uses a 0.5 oversampling ration, 100 trees, type encoding and no additional class weighting.


\textit{Our observation indicates that the random forest classifier profit from more data, even with data duplication.} We base this thesis on our observations with over- and undersampling the training data. 
All oversampled configurations perform better on the test set than their not resampled counterparts. This performance improvement is especially large for the RN type, with an f1 score increase of 0.03-0.05. For the CC and CCS problem type, the performance growth is smaller, with only around 0.01 in f1 score. We explain the difference between the problem types in the difference in class frequency. For the RN problem type, only 0.21\% of the samples are labelled positive, whereas 2.66\% of the samples for CCS and 4.87\% of the samples for CC are labelled positive. Oversampling the data with a ratio of 0.5 results in more duplicated samples from the minority class for the RN problem type than for CCS and CC. \Cref{tab:resampling_size_performance_rf} contains detailed measures of the train set size after oversampling and the performance metrics.

\textit{For undersampling, we observe a decrease in performance for all problem types.} For the RN problem type, the f1 performance drops up to 0.51, whereas the performance decrease for CCS and CC is lower with up to 0.12 and 0.07, respectively (undersampling ratio of 0.5). We explain this difference again with the difference in class frequency and the resulting change in training size. With an undersampling ratio of just 0.1, fewer samples from the majority class get deleted and the performance for the RN type only deteriorates by 0.08.
The different performance decrease between the CCS and CC type can also result from the difference in class frequency. The undersampling with an ratio of 0.5 reduces the training size for CCS to 394,704 whereas for CC just to 722,970. With an undersampling ratio of just 0.1, the effect is smaller and only visible for the RN type (see \Cref{tab:resampling_size_performance_rf}).


\begin{table}[]
\tabcolsep=0.11cm
\begin{tabularx}{\textwidth}{lXXX|XXX}
\toprule
                    & \multicolumn{3}{c}{Test F1 Score} & \multicolumn{3}{c}{Dataset}   \\ \midrule
                    & RN        & CCS        & CC     & RN            & CCS         & CC          \\ \midrule
No resampling     &  0.7599   &  0.9208    &  0.9211  &  4,950,329    & 4,950,329   & 4,950,329   \\
0.5 oversampling  &  0.8379   &  0.9329    &  0.929   &  7,409,925    & 7,228,141   & 7,064,008   \\
1.0 oversampling  &  0.8358   &  0.9287    &  0.9273  &  9,879,900    & 9,637,522   & 9,418,678   \\ \midrule
0.1 undersampling &  0.6857   &  0.9268    &  0.929   &  114,169      & 1,447,248   & 2,650,890   \\
0.5 undersampling &  0.2583   &  0.806     &  0.8632  &  31,137       & 394,704     & 722,970     \\ \bottomrule
\end{tabularx}
\caption{Dataset sizes for different resampling strategies with f1 performance on the test set. The f1 performance for the speciifc resampling is the random forest classifier with 100 trees, no class weighting and type encoding.}
\label{tab:resampling_size_performance_rf}
\end{table}

\textit{Our experiments show that the random forest classifier is not very sensitive to class imbalance.} Only an extreme class imbalance as for the RN type with a class distribution 0.21\% vs 99.79\% limits the classifier. We conclude this thesis from two main observations. 
First, all performance metrics for the CCS and CC problem type are comparable for all configuration without over- or undersampling. If the model would be sensitive to class imbalance, the higher class imbalance of the CCS type (97.34\% vs 2.66\%) should reflect in a worse performance than the CC type. We do not observe such performance differences. By contrast, the class frequency of positive samples for the RN type is by a factor twelve smaller than for the CCS type. The resulting severe class imbalance seems to affect the performance negatively. 
Second, the class weighting has no positive influence on the performance of the classifier. With balanced class weighting, the weighting for each class is adjusted inversely proportional to the class frequency. If the random forest classifier were sensitive to class imbalance, we would expect a performance improvement. Instead, we see a slight deterioration in f1 performance up to 0.04.


\textit{Type encoding increases the performance of the random forest classifier slightly for the CCS and CC type of up to 0.02.} For the RN problem type, we observe a similar improvement for oversampling with type encoding. For undersampling with type encoding, we observe a significant decrease of \~0.09 in f1 performance. 
We expected type encoding to have a more significant impact. The types should work as an additional feature the classifier can utilize. With the index-based encoding of the token values, 100,001 possible values can be encoded. The number of different token types is significantly smaller with only 61 different types (see the complete list of possible token types in \Cref{fig:tokenizer_types}). Especially for function and method names, it would allow the model to treat two different function calls as one. 
However, the improvement is lower than expected. At least for the CCC type, the \texttt{NAME} type of a method call should be easier to learn on than the changing value of the method name.  

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
    Overall, the random forest classifier handles class imbalance fairly well and can profit from more data, even if it contains duplicates from oversampling. Undersampling decrease the performance of the classifier. Type encoding has slight positive impact on the performance.
    \end{minipage}}
\end{center}

\subparagraph{Finding 3: Gradient Boosting Classifier}\label{finding:rq2_gbc}
\textit{The gradient boosting classifier performs well on all problem types and fulfils our satisficing metric.} We achieve an f1 score of over 0.9 for the RN and CC type and 0.84 for the CCS type. This best performing configuration is trained with 300 boosting stages, a learning rate of 0.2 and type encoding.


\textit{The gradient boosting classifier is sensitive to less training samples or samples with less variety.} Two main observations allow this conclusion. First, we observe a decrease in performance for under- and oversampling (see  \Cref{tab:resampling_size_performance_gbc}). For undersampling the RN type with a ratio of 0.5, the f1 score drops from 0.8306 by roughly 45\% and by approximate 41\% for oversampling with a ratio of 0.5. For the CCS type, the f1 score drops from 0.773 by approximately 9\% for over- and undersampling. The smallest drop is for the CC type from 0.8567 by nearly 7\% for over- and undersampling. With undersampling, we reduce the number of examples and with oversampling, we increase the minority class without increasing the variety of samples. Since the performance characteristics for over- and undersampling are comparable, we conclude that more samples with the same variety further decrease the model's performance. The decrease in f1 score is caused by a drop in precision, i.e. an increase of false positives. With less data, the classifier tends to be biased towards the positive class.
Second, the larger performance drop for the RN problem type and the smaller decrease for the CC type correlates with the number of training samples. A larger decrease in training data results in a larger drop in performance. 

\begin{table}[]
    \tabcolsep=0.11cm
    \begin{tabularx}{\textwidth}{lXXX|XXX}
    \toprule
                        & \multicolumn{3}{c}{Test F1 Score} & \multicolumn{3}{c}{Dataset}   \\ \midrule
                        & RN        & CCS        & CC     & RN            & CCS         & CC          \\ \midrule
    No resampling     &  0.8306   &  0.773    &  0.8567  &  4,950,329    & 4,950,329   & 4,950,329   \\
    0.5 oversampling  &  0.4847   &  0.6948   &  0.7933  &  7,409,925    & 7,228,141   & 7,064,008   \\
    0.5 undersampling &  0.4545   &  0.6836   &  0.7925  &  31,137       & 394,704     & 722,970     \\ \bottomrule
    \end{tabularx}
    \caption{Dataset sizes for different resampling strategies with f1 performance on the test set. The f1 performance for the speciifc resampling is the gradient boosting classifier with 200 stages, a learning rate of 0.2 and type encoding.}
    \label{tab:resampling_size_performance_gbc}
    \end{table}

\textit{The random forest classifier is insensitive towards class imbalance.} One supporting point is the aforementioned performance decrease with over- or undersampling. Additionally, the performance of the RN problem type is comparable to the CC problem type despite their difference in class imbalance (0.21\% class frequency for RN vs 4.87\% for CC). 

\textit{The random forest classifier has problems learning the CCS type.} Contradictory to our assumptions, the CC problem type is easier to learn than the CCS type. The precision is lower by up to 0.05, whereas the recall is lower by up to 0.11 (without resampling and $\geq 200$ boosting stages). We assume, the gradient boosting classifier has trouble learning the CCS subset and instead learns the more general superset of CC problems. To recap, the CCS problem type only labels a direct comparison in the if-condition as problem wheras the CC type consider nested boolean expressions. 
With the CCS subset, we restrict the classifier to only detect a direct comparison. We analysed the classification of the test samples, especially the false-positive ones. If the classifier learn the structure of the CC superset, the classifier would label samples as positive that are only positive for the CC type. We randomly sampled 20 false-positive samples and found 11 samples like in \Cref{lst:gbc_false_positive}. Those 11 samples contain the a direct comparison nested inside boolean operators. Such nesting is only labeled as a CC problem, not as CCS problem. We think our restriction in problem types collide with the classifier's ability to detect the more gneral pattern of a comparison somewhere inside the if-condition.


\textit{Subsampling the train data for the individual base learner during training has no impact.} The performance for all subsampling ratios are identical, so the individual base learners are trained with the same data as before.


\begin{lstlisting}[float=h, language=Python, label=lst:gbc_false_positive, caption={False-positive samples for the best performing random forest classifier with 300 boosting stages, a learning rate of 0.2, type encoding and without resampling for the CCS type. We removed spaces between tokens if necessary, but leave the intendation as in the sample.}]
    if input.dim() == 4 and mode == 'bicubic' : 
    assert align_corners is not
DEDENT,NAME,NAME,OP,NAME,OP,OP,OP,NUMBER,
NAME,NAME,OP,STRING,OP,NEWLINE,INDENT,NAME,
NAME,NAME,NAME
########################

input_data = [ indices , values , default ] 
if <UNKNOWN> and n > 1 : 
       
NEWLINE,NAME,OP,OP,NAME,OP,NAME,OP,NAME,OP,
NEWLINE,NAME,NAME,NAME,NAME,OP,NUMBER,OP,
NEWLINE,INDENT
########################
: 
<UNKNOWN> 
if target is none or source is none : 
    return false 

OP,NEWLINE,INDENT,STRING,NEWLINE,NAME,NAME,
NAME,NAME,NAME,NAME,NAME,NAME,OP,NEWLINE,
INDENT,NAME,NAME,NEWLINE,DEDENT
\end{lstlisting}

\textit{The gradient boosting classifier is robust to overfitting the training data.} We observe a comparable train and test performance for all non-resampled configurations. Only with under- or oversampling, the classifier seems to overfit the train data, since the test performance is worse the train performance. 
Another indicator for the robustness is the performance increase when using more boosting stages. Increasing the boosting stages increase the recall and precision for all problem type. Our best model with 300 boosting stages and a 0.2 learning rate can probably be more fine-tuned by increasing the boosting stages and adapting the learning rate. Again, being restraint by the increase in training time, we did not fine-tune the classifier further.

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
        In conclusion, the gradient boosting classifier is robust to overfitting and somewhat improves its performance with more boosting steps. We can also observe robustness for class imbalance and on the other hand sensitivity towards less training data. Subsampling has no effect and we observe an unexpected high difficulty of learning the CCS type for this classifier.
    \end{minipage}}
\end{center}


\subparagraph{Finding 4: LSTM-Based Neural Network}
\textit{Our implementation of an LSTM-based neural network performs well and fulfils our satisficing condition.} We achieve an f1 score of over 0.97 for all problem types without resampling, 10 LSTM cells, an embedding size of 32 and training with 2 epochs and 256 batch size. 


Since we have a remarkably high performance, we do not vary all parameters. Instead, we use a 0.5 under- and oversampling as for other models, vary the batch size and number of epochs and try reducing the size of the embedding layer since this layer contains the majority of the trainable parameter (see listing \ref{lst:lstm}).


\textit{Oversampling does not have a measurable impact on our LSTM-based neural network.} The f1 performance with a 0.5 oversampling is not significantly different from the performance without oversampling. We observe overfitting on the train data with a near-perfect recall and precision. The test performance remains higher than 0.97 f1 score. Although the model overfits to the train data, it still generalises well enough with comparable performance to the models trained on the original class distribution.


\textit{The LSTM-based neural network is sensitive to fewer train data.} We draw this conclusion based on the performance deterioration for undersampled train sets (see \Cref{tab:resampling_size_performance_lstm}). For the RN type, undersampling results in a significant drop in performance from 0.987 by roughly 36\%. For the CCS and CC type, the performance decrease is smaller at less than 3\%. The train set of the RN type is significantly smaller after undersampling than for the CCS and CC type, due to the lower class frequency of the minority class of only 0.21\% before undersampling. Therefore, more samples from the majority class are deleted during undersampling and the variety decreases. In combination with many trainable parameters in our model, the model fails to fit the training data correctly. 
Responsible for the performance drop is a decrease in precision, whereas the recall remains high. In other words, the false-positive rate increases by factor 100 to 0.3\%. By removing negative samples and variety in training, we think it becomes more challenging for the model to detect negative samples correctly. On the other hand, the high recall indicates a good detection capability for the positive class. Since we did not decrease the number of positive samples, the model can learn on the full variety of those models. 

\begin{table}[]
    \tabcolsep=0.11cm
    \begin{tabularx}{\textwidth}{lXXX|XXX}
    \toprule
                        & \multicolumn{3}{c}{Test F1 Score} & \multicolumn{3}{c}{Dataset}   \\ \midrule
                        & RN        & CCS        & CC     & RN            & CCS         & CC          \\ \midrule
    No resampling     &  0.987   &  0.9808    &  0.9782  &  4,950,329    & 4,950,329   & 4,950,329   \\
    0.5 oversampling  &  0.9859   &  0.9781   &  0.9745  &  7,409,925    & 7,228,141   & 7,064,008   \\
    0.5 undersampling &  0.6294   &  0.9564   &  0.9561  &  31,137       & 394,704     & 722,970     \\ \bottomrule
    \end{tabularx}
    \caption{Dataset sizes for different resampling strategies with f1 performance on the test set. The f1 performance for the speciifc resampling is the LSTM with an embedding size of 32, 10 lstm cells, a training on 3 epochs with a batch size of 256.}
    \label{tab:resampling_size_performance_lstm}
\end{table}


\textit{Our model configuration has too many trainable parameters for the problem complexity.} We base this assumption on two observations. First, we halved the size of the embedding layer to 16 and consequently reducing the 3,201,795 trainable parameters of the model to roughly half the size. If the model needed all parameters to fit the training data accurately, we would expect a performance drop. Instead, we do not observe any significant change in performance. Due to this observation, we assume a further reduction of the embedding size may be possible without performance loss. However, since this configuration trains in a reasonable time of under an hour without a GPU and has a test performance, we do not investigate this too powerful model further.
Second, a variation of the epochs and batch size for the training yields no significant difference. We think, the problem detection is too simple for the model and it quickly reaches its optimum. Further training does not increase the model's performance further. 

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
        To sum up, we achieve near-perfect performance with our neural network model. We acknowledge, that our model has too many trainable parameters that we do not need for the same performance as shown with the reduction of the embedding layer size. The model does not benefit from resampling. Oversampling has no impact on the test performance and undersampling impairs the training of the model's weight since it reduces the overall number of samples.
    \end{minipage}}
\end{center}

\subparagraph{Finding 6}
We found a weaknes in the way we labeled samples with the rule checkers. We decided to label a sample as rule violation, if all tokens of the problematic line are present. With the sliding window approach, it is possible to have all tokens for the detection in the sample, but missing the first or last newline or intendation tokens. The sample would therefore be labeled as clean code, although the problematic pattern is there. In the analysis of the false-positives for the gradient boosting classifier, we found samples for both cases in the false-positives as seen in \Cref{lst:weaknes_end_start_sliding_window}. In the first sample, not all preceding \texttt{DEDENT} types are within the window, wheras the trailing \texttt{NEWLINE} token is missing in the second sample.

To improve this weaknes, we would strip out all \texttt{DEDENT} and possibly \texttt{NEWLINE} tokens from the data before applying the sliding window. Due to time constrains, we are not able to retrain all models with the improved approach. However, the problematic line will occur in at least another sample, if the line is shorter than 16 tokens. It will most likely not impact the variety of problem patterns, but it will slightly reduce our performance score due to false-positives.


\begin{lstlisting}[float,floatplacement=H, language=Python, label=lst:weaknes_end_start_sliding_window, caption={False-positives of the best gradient boosting classifier with 300 boosting stages, a learning rate of 0.2, type encoding and without resampling for the CCS type. In the first sample, not all preceding \texttt{DEDENT} tokens are in this window. For the second example, the trailing \texttt{NEWLINE} token is missing in the window. Without this encoding issue, both samples would have been detected correctly.}]
if 'add_host' in <UNKNOWN> : 
<UNKNOWN> 
                            <UNKNOWN> = <UNKNOWN> . get ( 'add_host' ,
DEDENT,DEDENT,DEDENT,NAME,STRING,NAME,
NAME,OP,NEWLINE,COMMENT,NL,INDENT,NAME,
OP,NAME,OP,NAME,OP,STRING,OP
########################
if vhost.ssl: 
points += 3 

if points > <UNKNOWN> :
NL,DEDENT,NAME,NAME,OP,NAME,OP,NEWLINE,
INDENT,NAME,OP,NUMBER,NEWLINE,NL,DEDENT,
NAME,NAME,OP,NAME,OP
\end{lstlisting}


\paragraph{Summary}
\input{tables/rq2_best_per_model.tex}
 Table \ref{tab:rq2_best_classifier} shows the performance score of the best performing model of each classifier for all problem types.
 The SVM performs poorly and does not meet our mandatory satisficing condition of precision higher than 0.8. Therefore, we will not evaluate it further. On the other hand, the LSTM-based neural network model outperforms all other models on this classification task for all problem types.

 Both random forest and gradient boosting classifier, fulfil the satisficing condition for all problem types. Interestingly, they supplement each other: For the RN problem type, the gradient boosting classifier performs better with an f1 score of 0.9328, whereas the random forest classifier surpasses the gradient boosting classifier for the CCS and CC problem type with 0.9352 and 0.9351 f1. As discussed, we explain the former result with the better insensitivity to a class imbalance of the gradient boosting classifier. In contrast, the latter seems to be harder to learn for the gradient boosting classifier. Especially the difficulties for the CCS type are unexpected and do not affect the random forest classifier or neural network.

\subsection{RQ3: Do Machine-Learning Based Models Cover a Larger Variety of Cases Than Rule-Based Checker? }\label{rq:3}

\paragraph{Motivation}
For the previous research question, we trained models on a dataset. We created the dataset by downloading open-source repositories and applying our analysis plugins from \Cref{sec:analysis_plugins} to label samples as clean or non-clean code. With this automated approach, it is simple to generate a dataset and to scale the dataset to any size needed for training. Such algorithms (checkers) that detect non-clean code are the prerequisite for the dataset and successful training of machine learning models. This research question wants to put the machine learning approach into a scenario, in which this prerequisite is not met and we lack a checker to generate a labelled dataset. 
This scenario transfers to clean code rules with a high level of complexity (as defined in ~\Cref{sec:cc_complexity_levels}). For these rules, we may not be able to analyse the source code or another representation with a checker to generate a representative dataset. We could (1) hand-label samples and train a machine learning model on those samples. Alternatively, we may (2) only find a deterministic checker that detects a subset of the rule violations. We hope that model trained on either of those datasets generalise well to all patterns of the rule violation. Both scenarios boil down to a lack of samples for all or many structural variations of the rule violation and a discrepancy of the test set and the real world with more variety. We will evaluate both scenarios for our models from the previous experiment in this research question.


\paragraph{Approach}
To answer this research question, we use all models for all three problem types from the previous research question: 
We use the models trained on the RN problem type to simulate scenarios (1). First, we manipulate the holdout dataset as described in \ref{sec:approach_code_manipulation}. The code still contains returns of \texttt{None}, although it is not explicitly written as two consecutive code tokens. We want to explore if a model generalised well enough to detect unseen variations of the problem type. With the manipulation, we ensure that all samples contain the unseen variations and our performance metrics only refer to those samples. 

Second, we simulate scenario (2) of having trained on a subset of all variations with the CCS models. The CCS model was trained on a subset of the CC problem patterns. We manipulate all occurrences of the CC problem type to contain only patterns that are not in the subset for the CCS training. It is important to note that the class frequency of positive samples changes from the trained 2.51\% to the 5.1\% positive samples for the manipulated CC type in the holdout set.

Last, we evaluate the models trained on the CC type with our manipulated code to provide a baseline for the CCS trained models. We expect the CC model to show similar performance than in the previous experiments since it was trained on the problem structure of the manipulated CC dataset.

We expect the best performing CC model from the previous experiments to outperform all CCS models and to show similar performance like in the previous research question. 

\paragraph{Results}
We evaluate the models' performance for both scenarios: The RN problem type simulates scenario (1) whereas the CCS problem type represents scenario (2). We list all evaluation results in the appendix in \Crefrange{tab:rq3_random_forest}{tab:rq3_lstm}.

\subparagraph{Finding 1}\label{finding:return_none_manipulated_bad}
\textit{The detection of the manipulated RN problem type works poorly with all models.} The random forest classifier has an f1 score of less than 0.1, the gradient boosting classifier of 0.24 and the LSTM-based neural network of 0.39. The precision of all models is below 0.26 and would therefore not comply with the satisficing condition from the previous experiment of 0.8. Our simulation for scenario (1) fails.

It is obvious that our models do not generalise on the problem structure.  Since all models perform significantly worse than in the previous experiment, it is feasible to search for the reason in the problem type.  We think the model did not learn the problem structure during training. Our reasoning goes as follows.
The problem type appears in the training set only in one variation: A \texttt{return} keyword is followed by a subsequent \texttt{None} value. The surrounding tokens varied, but only the subsequent occurrence of these two tokens had to be learnt to perform to detect the problem type. For the previous experiment, this behaviour resulted in the best performance. To perform well on this experiment, the model should have learned a more sophisticated approach of a \texttt{None} value after the \texttt{return} keyword before the end of the line. Since this was not part of the training data, it is obvious that the model learnt the simple subsequent detection of this pattern, since it yields the best performance in the previous experiment.

\subparagraph{Finding 2}\label{finding:ccs_more_variety}
\textit{The classification of the modified CC samples with the CCS models yields better results than for the RN type.}
For the random forest classifier, the performance on the modified CC dataset is at 0.72 f1 score. The best gradient boosting classifier in this experiment achieves 0.83 and the best LSTM-based neural network reaches 0.66 f1 score. 
Compared to the performance for the RN types, the performance values are at least double as high, so the model has learnt the pattern from the subset better. We attribute this to the wider variety in problem pattern in the source code: Since the CCS checker looks for comparison in the condition, it will label different comparison operators such \texttt{==}, \texttt{<} or \texttt{>} as rule violations, whereas the only variation for the RN type is \texttt{return None} as subsequent tokens.

\subparagraph{Finding 3}\label{finding:better_vs_worse}
\textit{The more fine-tuning during training, the less the model generalise for the modified problems.}

All models on the modified problem types behave contrary to the unmodified problem types in the previous experiments. The hyperparameter combinations with a worse performance in the previous experiment score better in this evaluation. By contrast, the best model configurations from the previous experiment are among the worst-performing models for this evaluation. In \Cref{fig:rq3_performance_comparison}, we plotted the f1 performance of the baseline, manipulated RN and CCS. For each classifier, we sorted the models descending on the baseline f1 score. The baseline performance is the performance of the CC model from the previous experiment on the manipualted CC data. It correlates with the performance of the CC model on the original holdout data in the previous experiment. The overall picture shows, that the models with the best baseline performance have a low performance on the manipualted datasets. With decreasing baseline performance, the performance of the manipualted CCS increases for the random forest and gradient boosting classifier. The manipualted RN type follows this pattern for the random forest classifier and the LSTM. 


\begin{figure}[h]
    \begin{center}
        \input{diagrams/rq3_performance_comparison.pgf}
    \end{center}
    \caption[F1 performance of the models, sorted by the baseline f1 score.]{F1 performance of the models, sorted by the baseline f1 score. The baseline performance correlates with the performance of the CC type in the previous experiment. With a worse performance in the previous experiment, the performance for the manipualted types increases.}
    \label{fig:rq3_performance_comparison}
\end{figure}

We observe this behaviour on different hyperparameters we tuned for better performance in the previous experiment. It applies to both problem types.
For the random forest classifier, undersampling the training set lead inevitably to worse performance in the previous experiment than without undersampling. In this experiment, the undersampled random forest classifier leads to the overall best scores. For gradient boosting classifier, the models without resampling scored best in the previous experiment and are performing worst in this experiment. Additionally, increases in boosting stages that lead to an improvement in the previous experiment now lead to a deterioration in performance. 
The LSTM-based neural network results draw a similar picture. The undersampling harmed the performance, whereas, in this experiment, a 0.5 undersampling achieves the only f1 score over zero for the RN type and the best for the manipulated CC type.

We would explain this observation separate for the RN and CCS type:
For the RN type, the models probably only learnt the direct token sequence (as described in \hyperref[finding:return_none_manipulated_bad]{Finding 1}). The better it learnt this sequence, the better its performance on the previous experiment. For this experiment, the models that only learnt to look for a \texttt{return} and a \texttt{None} somewhere in the token stream achieve a higher result. In contrast, it performed poorly on the previous experiment. Consequently, we assume the poor performing models from the previous experiment followed this pattern. (TODO can i proof this point with FP from the evaluation? maybe find samples in which there are just these two tokens)

The CCS problem type, simulating scenario (2), offered more variety to learn from, as discussed in \Cref{finding:ccs_more_variety}. If the model learnt from this variety a general problem structure, fine-tuning the hyperparameters for the previous experiment would restrict this generality to only CCS patterns. We assumed in \Cref{finding:rq2_gbc} that the gradient boosting classifier get restricted by the CCS type and is more likely to learn the CC pattern. In this experiment, the best gradient boosting classifier configuration surpasses all other models. Additionally, it surpasses the baseline performance measure of the model trained on the unmodified CC type that we assumed to perform better. This solidifies our assumptions about the learning of the CCS type. We think this explanation applies to other models (TODO false positives from previous experiment anschauen und sehen ob da CC type die indikatoren waren).

The explanations differ for both scenarios, but their common ground is an insufficient amount of variety. While the RN type has only one variety in the problem pattern, the CCS rules restricted the variety to the subset.

\paragraph{Improvements}
As identified in the findings (\ref{finding:return_none_manipulated_bad} and \ref{finding:better_vs_worse}), we see the root cause for the bad performance in the learning of the specific code pattern and not the general structure of a problem type. We identified the lack of variety as the main reason, manifesting as the restriction of the learning on the CCS type. For the RN type, the general structure on a token level would be something like a \texttt{None} value somewhere after a \texttt{return} but before a line break. A slightly more advanced structure could be extracted from the CCS type: A comparison inside a condition (starting with an \texttt{if} statement and closing with the colon) should be marked. With the chosen approach, the model did not train on the general problem structure of the clean code violation but explicit code patterns. 

We see several ways to improve:
First, we did not provide the model with the full variety of problem patterns. Therefore, we suggest hand-label or hand-generated patterns with explicit variations.
In the motivation for this research question, we state that it may not be practically possible to collect many samples with the full variety of problem patterns for an adequate training corpus. For the improvement, we need a base collection of samples. This base collection can be obtained by an algorithm that can only detect a subset of the problem patterns (like in our approach with the CCS type) or a similar enough code pattern (like we tried with the RN type). On top of that, we explicitly create samples with more variety by either hand-labelling existing code or writing new code. With the hand-crafted samples, we fine-tune the model to increase the variety of code patterns the model can generalise on. Simultaneously, we loosen the indirect restriction of the dataset covering only a subset of the data. By combining a scaleable, automatic generated dataset with hand-labelled samples, we may be able to improve the models' performance on the samples that we could not label automatically. Additionally, we should use some hand-labelled samples for more variety in the test corpus to tune the hyperparameters to more possible problem variations in real datasets.

Second, we may compensate for less variety in training samples with a different encoding. Using the encoding to focus on the code structure may require fewer variations to extract the problematic code structure. We use an indexed-based encoding of the source codes token stream. With this approach, we do not exploit the code structure in our encoding. Other machine learning tasks on code profit from other encoding types that exploit more structure from the code. Alon et al. introduced code2vec to represent code as paths in the AST~\cite{alon_code2vec_2018}, capturing the structure of the code. Similiarly, Zhang et al. split the AST into smaller statement trees and apply a recurrent neural network approach to generate a code representation~\cite{zhang_novel_2019}.
Since we identified the lack of learning the problem structure from the samples, an encoding approaches that explicitly take advantage of the structure of code may improve the performance of this experiment. It may be even possible to train the model on a subset of possible problem variations like we tried with the manipulated CCS type without the restricting effect, since the problem structure may be inferred from the subset alone.
Additionally, this may be combined with the fine-tuning on hand-labelled samples to increase the performance to a practical level. The satisficing condition of precision higher than 0.8 we defined in the previous experiment would also apply to this experiment. A lower precision would lead to the cry wolf effect and the practical use would therefore be small.

\paragraph{Summary}
In summary, the detection of the modified RN type performs poorly with all models and configurations. We see the reason in the low variety of different problem patterns. The experiment with the RN type should simulate the scenario in which we could only obtain a dataset with hand-labelled samples that do not cover all possible patterns. Based on the analysis of the results, we admit that a hand-labelled sample set will probably contain a greater variety of samples. With more variety, the result would most likely be more comparable to the performance of the CCS type with more variation. If we define the subset of the CCS type as a hand-labelled set, we can interpret the results for this problem type in the context of scenario (1). 

The performance on the modified CC type is more promising than for the RN type. We explain this by more variety in the training samples, although the models seem to be restricted to this subset (especially the gradient boosting classifier). If we interpret the performance with the CCS type model on the modified CC type data as scenario (1), the variations of our hand-labelled samples will restrict the models' performance. Furthermore, it would lead to a misleading parameter tuning, since our test set would not contain all real-world variations.

For improvement, we suggest to hand-craft additional samples with a different variation of the problem pattern. This can be used for increasing the variety of the training set with fine-tuning or to better approximate the test set with the real world to tune the models' parameter towards real-life datasets. Additionally, encoding the data by utilising the source code structure, less variety may be necessary to learn the variations. If this is the case, it would have positive practical implications, since it would require less manual code labelling or creating.



