\section{Research Questions}

\begin{description}
    \item[RQ1] What is the utility of the CCAP besides existing tools? 
    \item[RQ2] How do different models compare on the task of detecting non-clean code?
    \item[RQ3] Do machine-learning based models cover a larger variety of cases than rule-based checker? 
\end{description}

\subsection{RQ1: Can the Clean Code Analysis Platform be a useful addition to developers workflow besides existing tools?}
\paragraph{Motivation}
Several tools are established that aim to improve code quality and detect unclean code. Every new tool with similar claims has to prove its usefulness beside the preexisting tools. We compare the CCAP with existing tools based on the design goals expandability, useability and integration. Additionally, we will mention and compare useful features unique to existing tools.
\paragraph{Approach}
To evaluate, if the CCAP is a useful addition, we compare different features to see, if it can supplement or replace existing tools. As described in section \ref{sec:tool_comparison}, the compared tools are Sonarcube, PMD Source Code Analyser Project, Codacy and PyLint.

\paragraph{Results}
Table TODO shows a summary of the features.

\subparagraph{Finding 1: Expandability}
For expandability of analysis plugins, CCAP is comparable to PyLint, that also offer plugins to analyse the raw string, the token stream or the AST. PMD allows plugins to analyse the AST or define XPath rules. Since XPath rules can be used in the CCAP as well (using a third-party library in the plugin), PMD offers fewer expansion possibilities. Sonarqube, on the other hand, provides the most possibilities for extension. Not only can developers analyse the AST, specify XPath rules, but they have access to an additional semantic model of the code that provides direct access to structures such as methods, parameters, return values. This semantic model simplifies analysis in comparison to the AST analysis.
Additionally, Sonarqube allows plugins to expose an API for other plugins to use. This different type of plugin further increases the possibilities for rule checking plugins. The least expandability offers Codacy, that allow customising or disabling existing rules, but does not offer to add rules to the existing ruleset.

Regarding output plugins, no compared tool offers the output customisation of CCAP. PMD and PyLint have different predefined output formats like JSON, HTML, CSV or text. Although PyLint allows customising the message format using a formatting string as a command-line argument, they do not offer extensions for the output. Sonarqube displays the analysis reports in its WebUI. The scanner tool, SonarScanner, sends the reports to the server component, that renders the results in the browser. Codacy's local scanner produces text output; the cloud version also has a WebUI.

\subparagraph{Finding 2: Integration shortcomings}
The CCAP has significant shortcomings in its integrations into IDEs, build processes and CI pipelines. All contestant tools provide plugins for build systems like Gradle or Maven. Except for Codacy, all tools have IDE integrations into the most common editors. Sonarqube and Codacy have integrations into source control systems. Same applies to PMD and PyLint since they are included in Codacy and therefore have the same integrations.

\subparagraph{Finding 3: Useability}
For a plugin developer, the useability of the plugin system is simple for CCAP and PyLint. Both tools have the same plugin mechanism and a simple plugin interface. PMD has a similar, straightforward plugin interface, but its Java plugin has to be bundled into a jar file before adding to the classpath. The latter applies to Sonarqube as well; additionally, the powerful and rich plugin API makes it harder to use.

For the user, installing CCAP is like installing every other python package. Running from the command-line is fast and can be automated, e.g. using git pre-commit hooks. The same workflow would be achieved with PyLint. Sonarqube is more complex to install due to its multiple components, but after the setup, the WebUI and integrations into most developer workflow have the best useability. Codacy as a cloud service only requires permission to access the source code repository to start scanning the code.

\subparagraph{Finding 4: Multi-Language scanning}
Sonarqube, PMD and Codacy offer multi-language scanning. This multi-language ability is advantageous for code repositories with multiple languages and for reusing the same tools and infrastructure for multiple projects with different programming languages. Supporting multi-language scanning is not possible with CCAP and would require a redefinition of the architecture. Although multi-language support was not a design goal, it would be a significant advantage for adoption.

\subparagraph{Finding 5: Maturity and Community-Support}
The maturity and community-support of tools impact the integration of the tools into the developer workflow. Some integrations are community-made and shared, so everybody has an advantage. The community support would also be necessary for CCAP to write and share additional analysis plugins and to integrate into different workflows.

\paragraph{Summary}
In summary, CCAP offers a good, but not best in class expandability with analysis plugins. It has a high useability for plugin developer and users. The lack of integrations for different workflows reflects the lack of community support and maturity. 
We see the CCAP as an addition to the powerful Sonarqube. The simple, yet robust expandability of the CCAP could supplement the shortcomings of Sonarqube in its powerful, but complex expandability. For instance, a code reviewer could code problematic code into a simple python plugin, distributes it to the team and would hopefully not reencounter the same problem.
Additionally, the CCAP could be used to teach about Clean Code and to enforce specific coding rules to students, since the local setup is straightforward. Clean Code rules could be turned into analysis plugins by students or teachers without having to understand a complicated interface.

\subsection{RQ2: How do different models compare on the task of detecting non-clean code?}
\paragraph{Motivation}
As we have shown with the CCAP and the analysis plugins, it is possible to write an algorithm that can detect certain code patterns like non-clean code patterns. If we are able to write a well-tested detection algorithm, we do not need error-prone machine learning models to detect code patterns. Nonetheless, if we can not design an algorithm that detects the desired code pattern reliably, we may perform better using a machine learning model. Additionally, if we encode a subjective pattern, it may not be possible to design an algorithm to objectively detect such a pattern. As a solution, we could label code parts as our pattern and use machine learning approaches to implicitly extract rules to detect such a pattern.

For this research question, we will evaluate different machine learning models on different code patterns and compare the results.

\paragraph{Approach}
For answering the research question, we will use the approach detailed in chapter \ref{chap:clean_code_classification}.  The dataset, consisting of 18 projects from GitHub, is prepared as described in chapter \ref{chap:clean_code_classification_dataset}. The data is split into 90\% train and 10\% test data. We use the training dataset to train Random Forest, Gradient Boosting Classifier, Support Vector Machines and an LSTM-based neural network. Due to the class imbalance, we will additionally train with an oversampled dataset (oversampling rate of 0.5) and an undersampled dataset (undersampling rate of 0.5). Since our test set contains real-world code samples, we expect it to represent a real-world label distribution, and we, therefore, accept the potential effects of the data imbalance. Nevertheless, we will try to train with an over- and undersampled dataset to observe the effect on the performance.

Our evaluation metrics are recall, precision and the combined f1 score. Due to the data imbalance, accuracy is not a meaningful metric. A high recall means a high detection rate of non-clean code. The costs of a false negative prediction are potential costs in unclean code (maintainability, understandability). With high precision, a detected non-clean code sample is likely to be a non-clean code sample and the system reports less false positives. The immediate cost of false-positive predictions (resulting in a lower precision) is the extra developer time needed. Additionally, the cry wolf effect comes into play~\cite{breznitz_cry_1984}. In our case, a developer who has seen a false alarm will take subsequent alarms less serious. Additionally, a study has shown that a higher false alarm rate in advisory warning system in cars results in a lower, subjective evaluation of the system~\cite{naujoks_cooperative_2016}. A low subjective evaluation of developers would lead to a decrease in adoption rate, which consequently leads to more unfixed problematic code. Since both recall and precision are important to the success of our models, we decided to use the equally weighted f1 score as a single-value evaluation metric.
Additionally, to compensate for the cascading effect of a low precision due to the cry wolf effect, we require a precision of 0.8 as a satisficing metric. This means we accept all models that return two false positive and eight true positive samples. On the other hand, we define the recall as our optimising metric.

We evaluate all models for the three different problem types RETURN\_NONE (RN), CONDITION\_COMPARISON\_SIMPLE (CCS) and CONDITION\_COMPARISON (CC). We expect the CC type to be the most difficult to learn since it is the most complex rule. CCS and RN should be easier to learn due to less possible  variations in the code structure.

\paragraph{Results}
We list all hyperparameter configurations and the corresponding training, test and holdout performance for all problem types in the appendix in \Crefrange{tab:all_results_random_forest}{tab:all_results_svm}. We report the detailed analysis in a seperate finding for each model. Additionally, we describe our findings when comparing the models for the given classification task.s



\subparagraph{Finding 1: Support Vector Classifier}
The support vector classifier in our configurations are not able to detect any problem type. We see as the main reason the sensitivity towards class imbalance. The  f1 score of the CCS type is roughly ten times higher than for RN. Additionally, the f1 score for CC is roughly double as high as the score for CCS. Although we measured this difference after training on undersampled data, the factors correlate with the number of samples in the original dataset. The number of samples for CCS is rougly ten times higher than for RN and CC has double the amount of samples compared to CCS. TODO explanation??
Annother supporting observation is the positive impact of class balancing. Class balancing increases the recall for all problem types up to 0.79 for RN, 0.5 for CCS and 0.55 for CC. The increase in recall implies a reduction of false-negatives. Such a reduction is possible, if the classfier reduces its bias towards the negative class. 

Due to the quadratic time complexity during training described in \Cref{sec:svm_quadratic_complexity}, we were only able to test a few configurations. We tested configurations with a general subsampling and undersampling to reduce the input size and to achieve a manageable training time. Therefore, we do not have data for non-resampled datasets or oversampled datasets to further investigate the imbalance impact.

In conclusion, the support vector classifier does not fullfill our acceptance criteria of a precision higher than 0.8. Especially the sensitivity for class imbalance makes the use for our problem domain with heavility imbalanced datasets pointless.  
Furthermore, the train time limits our experiments. It is important to note that we did no further investigation in improving the train time or the perfromance. Maybe we could have increased classification and runtime performance with choosing a different kernel function such as a linear kernel function, but we decided to invest our efforts into the other, more promising models. 

\subparagraph{Finding 2: Random Forest}
Random forest classifier perform well and fullfil our satisficing condition on precision. It achieves f1 scores of over 0.8 for the RN type and over 0.9 for the CCS and CC type.
The best performing random forest classifier uses a 0.5 oversampling ration, 100 trees, type encoding and no additional class weighting.


Our observation indicate that the random forest classifier profit from more data, even with data duplication. We base this thesis on our obeservations with over- and undersampling the training data. All oversampled configurations perform better on the test set than their not resampled counterparts. This performance improvement is especially large for the RN type with an f1 score increase of 0.03-0.05. For the CC and CCS problem type, the performance growth is smaller with only around 0.01 in f1 score. We explain the difference between the problem types in the difference in class frequency. For the RN problem type, only 0.21\% of the samples are labeled positiv, whereas 2.66\% of the samples for CCS and 4.87\% of the samples for CC are labeled positive. Oversampling the data with a ratio of 0.5 result in more duplicated samples from the minory class for the RN problem type than for CCS and CC. Table TODO contains detailed measures of the train set size after oversampling.
For undersampling, we observe a decrease in performance for all problem types. For the RN problem type, the f1 performance drops up to 0.51, whereas the performance decrease for CCS and CC is lower with up to 0.12 and 0.07 respectively (undersampling ratio of 0.5). We explain this difference again with the difference in class frequency and the resulting change in training size. With an undersampling ratio of just 0.1, less samples from the majority class gets deleted and the performance for the RN type only deteriotes by 0.08.
The different performance decrease between the CCS and CC type can also result from the difference in class frequency. The undersampling reduces the training size for CC to TODO from table wheras for CC just to TODO. With an undersampling ratio of just 0.1, the effect is smaller and does not reflect in the performance statistics for CCS and CC.



Our experiments show that the random forest classifier is not very sensitive to class imbalance. Only an extreme class imbalance as for  the RN type with a class distribution 0.21\% vs 99.79\% limits the insensitivity of the classifier. We conclude this thesis from two main observations. 
First, all performance metrics for the CCS and CC problem type are compareable for all configuration without over- or undersampling. If the model would be sensitive to class imbalance, the higher class inbalance of the CCS type (97.34\% vs 2.66\%) should reflect in a worse performance than the CC type. We do not observe such performance differences. By contrast, the class frequency of positive samples for the RN type is by a factor twelve smaller than for the CCS type. The resulting extreme class imbalance seem to affect the performance in a negative way. 
Second, class weighting has no positive influence on the classifiers performance. With balanced class weighting, the weighting for each class is adjusted inversly proportional to the class frequency. If the random forest classifier would be sensitive to class imbalance, we would expect a performance improvement. Instead, we see a slight deteriation in f1 performance up to 0.04.


Type encoding increases the performance of the random forest classifier slightly for the CCS and CC type. We observe a slight increase of up to 0.02. For the RN problem type, we observe a similiar improvement with oversampling. For undersmapling, we observe a significant decrease of \~0.09 in f1 performance. 
We expected a positive impact as seen for the CCS and CC type, since the type encoding reduces the variety of possible input values to the model. For the index-based encoding of the token values, there are 100,001 possible values, whereas the number of different token types is significantly smaller with only 61 different types (TODO see attachment list of types of tokenizer). For instance, a string token would always have the same token type no matter the strings content. On the other side, the token value would change depending on the string value. We think, this would simplify the problems for the model. TODO we evaluated the feature importance for the random forest in figure TODO).
The unexpected results from the RN type may be caused by the class imbalance, since the overall variety of possible inputs increases with 40 instead of 20 input values. The number of positive samples that provide the pattern encoded in the token types may be too small to learn it. Supportive for this reasoning are the performance improvements for oversampling, with which the model would have more samples to learn this pattern.


Overall, it seems like a random forest classifier can handle moderate class imbalance fairly well. If the classes are severely imbalanced as for the RN problem type, the classifiers performance suffers.
We base this conclusion on several observations: First, we see better performance for the CCS and CC problem type overall, both of which have a more balanced distribution than the RN problem type. It is important to note that the different problem types also have different difficulty levels for the model. But we assume the RN type is easier to detect for a model since the model only has to learn two subsequent tokens anywhere in the sequence. Therefore, we attribute this effect to the class distribution, but we remark that this may be tested in an additional, isolated experiment. 
Second, without over- or undersampling, we observe similar performance for CCS and CC, despite their different frequency in the train set (0.21\% of samples contain the RN type, 2.66\% CCS and 4.87\% CC). 
And finally, we would expect a bigger impact of class balancing, if the random forest classifier would be sensitive to class imbalance.
Additionally, encoding the token types results in a slightly better performance than only encoding token values. To further quantify the impact, a more specific experiment setup would be necessary.

TODO: conlcusion back from git history

\subparagraph{Finding 3: Gradient Boosting Classifier}
The gradient boosting classifier performs well on all problem types and fulfils our satisficing  metric. We achieve an f1 score of over 0.9 for the RN and CC type and 0.84 for the CCS type. This best performing configuration is trained with 300 boosting stages, a learning rate of 0.2 and type encoding.

%old overall
The overall best performing gradient boosting works with 300 boosting steps and a learning rate of 0.2. For the RN problem type, it delivers an f1 score of 0.9196, a recall score of 0.8666 and a precision of 0.9796. Comparable performance is measured for the CC type: 0.9008 f1, 0.8673 recall and 0.937 precision. The performance decreases for the CCS type to 0.8419 f1, 0.7902 recall and 0.9009 precision for the same, best-performing configuration. A decrease in performance for the CCS problem type applies to all configuration in comparison to the CC type. Nevertheless, the gradient boosting classifier fulfils our satisficing criteria of a precision score higher than 0.8.

The gradient boosting classifier is sensitive to less training samples or samples with less variety. Two main observations allows this conclusion. First, we observe a decrease in performance for under- and oversampling. For undersampling the RN type with a ratio of 0.5, the f1 score drops from 0.8306 by roughly 45\% and by approximate 41\% for oversampling with a ratio of 0.5. For the CCS type, the f1 score drops from 0.773 by approximately 9\% for over- and undersampling. The least drop is for the CC type from 0.8567 by nearly 7\% for over- and undersampling. With undersampling we reduce the numer of examples and with oversampling, we increase the minority class without increasing the variety of samples. Since the performance characteristics for over- and undersampling are compareable, we conclude that more samples with the same variety further decrease the models performance. The decrease in f1 score is caused by a drop in precision, i.e. an increase of false positives. With less data, the classifier tends to be biased towards the positive class.
Second, the larger performance drop for the RN problem type and the smallest decrease for the CC type correlates with the number of training samples. A larger decrease in training data results in a larger drop in performance. See table TODO for change in sample size for undersampling. 

The random forest classifier is insensitive towards class imbalance. One supporting point is the aforementioned performance decrease with over- or undersampling. Additionally, the performance of the RN problem type is compareable to CC problem type despite their difference in class imbalance (0.21\% class frequency for RN vs 4.87\% for CC). 

The random forest classifier has problems learning the CCS type. Contradictory to our assumptions, the CCS problem type is easier to learn than the CC type. The precision is lower by up to 0.05, whereas the recall is lower by up to 0.11 (without resampling and $\geq 200$ boosting stages). TODO find explanation^^

Subsampling the train data for the individual base learner during training has no impact. The performance for all subsampling ratios are identical, so the individual base learners are trained with the same data as before.

The gradient boosting classifier is robust to overfitting the training data. We observe a compareable train and test performance for all non-resampled configurations. Only with under- or oversampling, the classifier seems to overfit the train data, since the test performance is worse the train performance. 
Another indicator for the robustness is the performance increase when using more boosting stages. Increasing the boosting stages increase the recall and precision for all problem type. Our best model with 300 boosting stages and a 0.2 learning rate can probably be more finetuned by increasing the boosting stages and adapting the learning rate. Again, being restraint by the increase in training time, we did not fine-tuned the classifier further.

In conclusion, the gradient boosting classifier is robust to overfitting and rather improves its performance with more boosting steps. We can also observe robustness for class imbalance and on the other hand sensitivity towards less training data. Subsampling has no effect and we observe an unexpected high difficulty of learning the CCS type for this classifier.


\subparagraph{Finding 4: LSTM}
Our implmentation of a LSTM-based neural network performs well and fulfils our satisficing condition. We achieve an f1 score of over 0.97 for all problem types without resampling, 10 LSTM cells, an embedding size of 32 and a training with 2 epochs and 256 batch size. 


Since we have a remarkably high performance, we do not vary all parameters. Instead, we use a 0.5 under- and oversampling as for other models, vary the batch size and number of epochs and try reducing the size of the embedding layer since this layer contains the majority of the trainable parameter (see listing \ref{lst:lstm}).


Oversampling does not have a measureable impact on our LSTM-bsaed neural network. The f1 performance with a 0.5 oversampling is not significantly different from the performance without oversampling. We observe an overfitting on the train data with a near perfect recall and precision. The test performance remains higher than 0.97 f1 score. Although the model overfits to the train data, it still generalises well enough with comparable performance to the models trained on the original class distribution.


The LSTM-based neural network is sensitive to less train data. We draw this conclusion based on the performance deteriation for undersampled train sets. For the RN type, undersampling results in a major drop in performance from 0.987 by roughly 36\%. For the CCS and CC type, the performance decrease is smaller at less than 3\%. The train set of the RN type is significantly smaller after undersampling than for the CCS and CC type, due to the lower class frequency of the minority class of only 0.21\% before undersampling. Therefore, more samples from the majority class are deleted during undersampling and the variety decreases. In combination with many trainable parameters in our model, the model fails to correctly fit the training data. 
Responsible for the performance drop is a decrease of precision, wheras the recall remains high. In other words, the false positive rate increases by factor 100 to 0.3\%. By removing negative samples and variety in the training, we think it becomes harder for the model to detect negative samples correctly. On the other hand, the high recall indicate a good detection capability for the positive class. Since we did not decrease the number of positive samples, the model can learn on the full variety of those models. TODO: precision description is bad


Our model configuration has too many trainable parameters for the problem complexity. We base this assumption on two observations. First, we halved the size of the embedding layer to 16 and consequently reducing the 3,201,795 trainable paramters of the model to roughly half the size. If the model would need all parameters to accurately fit the training data, we would expect a performance drop. Instead, we do not observe any significant change in performance. Due to this observation, we assume a further reduction of the embedding size may be possible without performance loss. However, since this configuration trains in a reasonable time of under an hour without a GPU and has a test performance, we do not investigate this too powerful model further.
Second, a variation of the epochs and batch size for the training yields no significant difference. We think, the problem detection is too simple for the model and it quickly reaches its optimum. Further training therefore does not increase the models performance further. 


To sum up, we achieve near-perfect performance with our neural network model. We acknowledge, that our model is has too many trainable parameters that we do not need for the same performance as shown with the reduction of the embedding layer size. The model does not benefit from resampling. Oversampling has no impact on the test performance and undersampling impairs the training of the model's weight since it reduces the overall number of samples.

\paragraph{Summary}
\input{tables/rq2_best_per_model.tex}
 Table \ref{tab:rq2_best_classifier} shows the performance score of the best performing model of each classifier for all problem types.
 The SVM performs poorly and does not meet our mandatory satisficing condition of precision higher than 0.8. Therefore, we will not evaluate it further. On the other hand, the LSTM-based neural network model outperforms all other models and we expect it to perform well in the next research question recognising modifications of the code.

 Both, random forest and gradient boosting classifier, fulfil the satisficing condition for all problem types. Interestingly, they supplement each other: For the RN problem type, the gradient boosting classifier performs better with an f1 score of 0.9328, whereas the random forest classifier surpasses the gradient boosting classifier for the CCS and CC problem type with 0.9352 and 0.9351 f1. As discussed, we explain the former result with the better insensitivity to a class imbalance of the gradient boosting classifier, whereas the latter seems to be harder to learn for the gradient boosting classifier. Especially the difficulties for the CCS type are unexpected and do not affect the random forest classifier or neural network.
 
 Note: in a real setup with hand-labelled examples, we expect more variety in the samples so the classifier may even perform better on the next research question 


 \subsection{RQ3: Can the models detect modified non-clean code that can not be detected by the original rule checker? }
See tables in attachment.

\paragraph{Motivation}
For the previous research question, we trained models on a dataset. We created the dataset by downloading open-source repositories and applying our analysis plugins from \ref{sec:analysis_plugins} to label samples as clean or non-clean code. With this automatic approach, it is simle to generate a dataset und to scale the dataset to any size needed for training. These algorithm that detect non-clean code are the prerequesite for the dataset and a successfull training of machine learning models. This research question wants to put the machine learning approach into a szenario, in which the prerequesite is not met and there no dataset can be generated. This szenario applies to clean code guidelines that can not be checked by an algorithm that analyse the source code or any other representation in a deterministic way. Alternativeley, a deterministic algorithm may only detect a subset of clean-code vilations. Both szenarios boil down to not having samples for all or many structural variations of the rule violation. Therefore, we evaluate, how our previously trained models perform on slightly modified code that still violates the same clean code rule but could not be detected by the original analysis plugins.


\paragraph{Approach}
To answer this research question, we use all models for all three problem types from the previous research question: For the RN problem type, we manipulate the holdout dataset as described \ref{sec:approach_code_manipulation}. The code still contain a logical \textit{return None}, although it is not explicitly written as two consecutive code tokens. This should explore, how well the different classifier extracted the structural information and can detect this noisy data. 

With the CCS models, we simulate the scenario of having trained on a subset of all variations and testing on the full set of problem variations. We do so by manipulating all rules found by the algorithm for the CC problem type to only contain samples that are not part of the CCS subset. The CCS algorithm could therefore not detect a clean code violation.

Last, we evaluate the models trained on the CC type with our manipulated code to provide a baseline for the CCS trained models. We expect the best performing CC model from the previous experiments should outperform all CCS models and should show similar performance like in the previous research question. We then investigate the the difference between the baseline and the model trained on CCS.

\paragraph{Results}
For the results, we evaluate the results of the three models: Random Forest Classifier, Gradient Boosting Classifier and LSTM-based neural network. Per model, we analyse the performance for the RN manipulation, the CCS manipulation, teh comparison baseline and compare the results with the results from the previous research question.


\subparagraph{Finding 1: Random Forest Classifier}
The random forest classifier trained on the RN problem type achieves less than 0.06 f1 score. The classifier can not handle this kind of manipulation.

The best CCS models achieves 0.7158 f1 with 0.758 recall and 0.678 precision. The basline model trained on the CC data reaches a 0.8186 f1 performance with a 0.9782 recall and 0.7038 precision. Compared to the baseline model, the CCS model has a 0.1 lower f1 score. The CCS model is trained with a 0.5 undersampling rate, 100 trees in the forest, class weighting and type encoding. The general better performance of models trained with undersampling is contradictory to the evaluation of the model in the previous experiment, in which oversampling performed better than undersampling. This reflects the baseline performance, for which the models with oversampled training and type encoding perform better than the undersampled counterparts. 

The precision of models trained on undersampled datasets is lower than on the models trained on oversampled or not resampled training data. On the other hand, the recal drops to zero (for oversampled or not resampled training data), which means the false negative rate increases. The model classifies most samples as negative and only a few as positive (TODO get numbers). With undersampled training, the model does not seem to be biased towards the negative classification as found with non-resampled or oversampled training data. Since the undersampling of the majority class (negative class, meaning clean code) reduces the number of unique negative samples in the training corpus, this decrease in bias seems reasonable. On the other hand, it is important to note that we test the model on the manipulated CC dataset that contains 5.1\% positive samples in comparison to the 2.51\% positive samples in the CCS train set without resampling. This may also influence the bias towards the negative samples.

\subparagraph{Finding 2: Gradient Boosting Classifier}
For the manipulated RN type, a gradient boosting classifier with a 0.5 oversampling ratio achieves a 0.24 f1 score with 0.7676 recall and 0.1422 precision. The second best configuration is the equal configuration with a 0.5 undersampling ratio with similar performance scores. The same configuration trained on non-resampeld training data has a near zero f1 and recall performance and a higher 0.349 precision. Decreasing the number of boosting stages with 0.5 over- or undersampling decreases the precision significantly while increasing the recall.
With over- or undersampling, the model seems be biased towards classifying samples positive, indicated by the low precision. Without resampling, the classifier is biased towards the negative class and will wrongly assume positives to be negatives. Since the class distribution is compareable to the class distribution of the training dataset, the classifier is biased towards the negative class.

This behaviour is contrary to the overservation during the train and testing for the previous research question:  We received a significant better performance without resampling the training data and we concluded, that the gradient boosting classifier is not so sensitive towards class imbalance. 
The results for the CCS trained model draw a similiar picture: With an oversampling rate of 0.5, the configuration achieves the best f1 score of 0.83 with 0.978 recall and 0.7209 precision. We observe rather similiar performance with an f1 score of 0.8291, a recall of 0.9773 and a precision of 0.7199. The performance of non-resampled configuration drops below 0.53 f1 score. Especially the best performing classifier for the direct classification now has the lowest performance for this task. To sum up, the bad performing model configurations for direct classification perform better in this classification task. Since this applies to both problem types, we interprete these results as follows: With better fitting of the original classification problem, the gradient boosting classifier learnt to detect the sequence of subsequent token types as seen in the RN and CCS training data. After code manipulation, the structure of the code and therefore the sequence of the tokens is changed. The better the model trained to detect the subsequent tokens with more boosting stages or more unique data, the better the performance for the previous experiment. Consequently, the model did not learn the general structure like a direct comparison following within the next few tokens of the \enquote{if} keyword, but it overlearnt the subsequent structure of the tokens. On the other side, this is another indicator the model did not train on the surrounding context but instead on the tokens of the problem.

\subparagraph{Finding 3: LSTM-based Neural Network}
For the manipulated RN type, only the model configuration with a 0.5 undersampling, 3 epochs and 256 batch size is able to have a performance better than zero. With an f1 of 0.3895, a recall of 0.847 and a precision of 0.2529, the performance is too low to be any usefull. With the higher recall and lower precision, the models false positive rate is higher.

The same model configuration performs best for the manipulated CC problem type: With a f1 score of 0.6868, a recall of 0.5009 and a precision of 0.9534, the model has a higher false negative rate whereas the positive classification is correct for the most time. There is a major gap to the baseline performance of 0.9502 f1. 
The second best performance reaches a configuration without resampling but with an embedding size of only 16. The best performing model configurations from the previous experiment score worst in this experiment.

\subparagraph{Finding 4: Manipulated RETURN\_NONE Classification}\label{finding:return_none_manipulated_bad}
All models perform unexpected bad on detecting the manipulated RN type. The best random forest configuration for this experiment achieves a 0.0539 f1 score with 0.3022 recall and 0.0296 precision. The gradient boosting classifier improved the f1 score to 0.24, the recall to 0.7676 and the precision to 0.1422. Another increase in performance is scored by the best performing LSTM-based neural network with a f1 score of 0.3895, a recall of 0.847 and a precision of 0.2529. 

All models have in common, that the best configuration for this experiment performs far worse than the best configuration for the previous experiment. Additionally, we observe a low precision for all models on the manipulated RN type, meaning a lot of false positive classifications.
Wee see the following as possible reasons for this performance characteristics:
The problem type appears in the training set only in one variation: A \enquote*{return} keyword is followed by a subsequent \enquote*{None} value. Only the subsequent occurance of these two tokens needs to be learnt to perform well on the evaluation from the previous experiment. To perform well on this experiment, the model should have learned a more sophisticated approach of a \enquote*{None} value after the \enquote*{return} keyword before the end of the line. Since this was not part of the training data, it is obvious the model learnt the simple subsequent detection of this pattern, since it yields the best performance. 


\subparagraph{Finding 5: The Better the Model Performed on RQ2, the Worse in RQ3} \label{finding:better_vs_worse}
Another common observtion accross all models and problem types: Models with lower performance on the previous experiment performed better than the models with good performance on the previous research question. 
This effect is obvious with the better performane of the models trained on undersampled data, that performed worst on the previous experiment. For gradient boosting classifier, the models without resampling scored best in the previous experiment and are performing worst in this experiment. Additionally, increases in boosting stages that lead to an improvement in previous experiment now lead to a deteriation in performance. Same applies to the LSTM-based neural network results: The undersampling had a negative impact on performance whereas in this experiment, a 0.5 undersampling achieves the only f1 score over zero for the RN type and the best for the manipulated CC type.
All of these observations combined lead to the conclusion, that the models did not learn the rule in a broader way, but instead only learnt to detect this specific sequence of tokens as described in \ref{finding:return_none_manipulated_bad} for the RN type. The same argument applies to the manipulated CCS samples.

\paragraph{Improvements}
As identified in the findings (\ref{finding:return_none_manipulated_bad} and \ref{finding:better_vs_worse}), we see the root cause for the bad performance in this research questions in the learning of the specific code pattern and not the generel structure of a problem type. For the RN type, the general structure would be something like a \enquote*{None} value somewhere after a \enquote*{return} but before before a line break. A slightly more advanced structure could be extracted from the CCS type: A comparison inside a condition (starting with an \enquote*{if} statement and closing with the colon) should be marked. With the choosen approach, the model did not train on the general problem structure of the clean code violation but on explicit code patterns. The fitting on explicit code patterns can be broken down into two parts:
First, we did not provide the model with the full variety of problem pattern. In the motivation for this research question, we state that it may not be possible to collect many samples with the full variety of problem patterns for an adequate training corpus. A possible approach would be to generate a dataset similar to ours. Therefore, we need an algorithm that can detect a subset of the code pattern (like in our approach with the CCS type) or a similar enough code pattern (like we tried with the RN type). On top of that, we gather hand-labeled samples for more variety. With the hand-labeled samples we fine-tune the model to increase the variety of code patterns the model can generalize on. By combining a scaleable, automatic generated dataset with hand-labeled sampels, we may be able to improve the models performance on the samples that we could not label automatically.

The second cause for the explicit fitting of our models may be our approach in encoding. We use an indexed-based encoding of the source codes token stream. With this approach, we do not exploit code structure in our encoding. Other machine learning tasks on code profit from other encoding types that exploit more strcutre from the code: TODO list some that exploit more structure of the code. Since we identified the lack of learning the problem structure from the samples, the encoding approaches that explicitly take advantage of the structurness of code may improve the this experiment. It may be even possible to train the model on a subset of possible problem variations like we tried with the manipulated CCS type. Additionally, this may be combined with the finetuning on hand-labeled samples to increase the performance to a practical level. The satisficing condition of a precision higher than 0.8 we defined in the previous experiment would also apply to this experiment. A lower precision would lead to the cry wolf effect and the pracitcal use would therefore be small.
The use of a different encoding schema, models and fine tuning can also be adapted to one specific problem type. Depending on the number of training samples, the availability of an algorithm to detect a subsample of the problematic patterns and the structure of the problematic pattern can result in a specific approach that is not universal to other problem types.

\paragraph{Summary}




