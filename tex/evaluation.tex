\section{Research Questions}

\begin{description}
    \item[RQ1] Can the Clean Code Analysis Platform be a useful addition to developers workflow besides existing tools? 
    \item[RQ2] What models perform well on detecting non-clean code?
    \item[RQ3] Can the models detect modified non-clean code that can not be detected by the original rule checker? 
\end{description}

\subsection{RQ1: Can the Clean Code Analysis Platform be a useful addition to developers workflow besides existing tools?}
\paragraph{Motivation}
Several tools are established that aim to improve code quality and detect unclean code. Every new tool with similar claims has to prove its usefulness beside the preexisting tools. We compare the CCAP with existing tools based on the design goals expandability, useability and integration. Additionally, we will mention and compare useful features unique to existing tools.
\paragraph{Approach}
To evaluate, if the CCAP is a useful addition, we compare different features to see, if it can supplement or replace existing tools. As described in section \ref{sec:tool_comparison}, the compared tools are Sonarcube, PMD Source Code Analyser Project, Codacy and PyLint.

\paragraph{Results}
Table TODO shows a summary of the features.

\subparagraph{Finding 1: Expandability}
For expandability of analysis plugins, CCAP is comparable to PyLint, that also offer plugins to analyse the raw string, the token stream or the AST. PMD allows plugins to analyse the AST or define XPath rules. Since XPath rules can be used in the CCAP as well (using a third-party library in the plugin), PMD offers fewer expansion possibilities. Sonarqube, on the other hand, provides the most possibilities for extension. Not only can developers analyse the AST, specify XPath rules, but they have access to an additional semantic model of the code that provides direct access to structures such as methods, parameters, return values. This semantic model simplifies analysis in comparison to the AST analysis.
Additionally, Sonarqube allows plugins to expose an API for other plugins to use. This different type of plugin further increases the possibilities for rule checking plugins. The least expandability offers Codacy, that allow customising or disabling existing rules, but does not offer to add rules to the existing ruleset.

Regarding output plugins, no compared tool offers the output customisation of CCAP. PMD and PyLint have different predefined output formats like JSON, HTML, CSV or text. Although PyLint allows customising the message format using a formatting string as a command-line argument, they do not offer extensions for the output. Sonarqube displays the analysis reports in its WebUI. The scanner tool, SonarScanner, sends the reports to the server component, that renders the results in the browser. Codacy's local scanner produces text output; the cloud version also has a WebUI.

\subparagraph{Finding 2: Integration shortcomings}
The CCAP has significant shortcomings in its integrations into IDEs, build processes and CI pipelines. All contestant tools provide plugins for build systems like Gradle or Maven. Except for Codacy, all tools have IDE integrations into the most common editors. Sonarqube and Codacy have integrations into source control systems. Same applies to PMD and PyLint since they are included in Codacy and therefore have the same integrations.

\subparagraph{Finding 3: Useability}
For a plugin developer, the useability of the plugin system is simple for CCAP and PyLint. Both tools have the same plugin mechanism and a simple plugin interface. PMD has a similar, straightforward plugin interface, but its Java plugin has to be bundled into a jar file before adding to the classpath. The latter applies to Sonarqube as well; additionally, the powerful and rich plugin API makes it harder to use.

For the user, installing CCAP is like installing every other python package. Running from the command-line is fast and can be automated, e.g. using git pre-commit hooks. The same workflow would be achieved with PyLint. Sonarqube is more complex to install due to its multiple components, but after the setup, the WebUI and integrations into most developer workflow have the best useability. Codacy as a cloud service only requires permission to access the source code repository to start scanning the code.

\subparagraph{Finding 4: Multi-Language scanning}
Sonarqube, PMD and Codacy offer multi-language scanning. This multi-language ability is advantageous for code repositories with multiple languages and for reusing the same tools and infrastructure for multiple projects with different programming languages. Supporting multi-language scanning is not possible with CCAP and would require a redefinition of the architecture. Although multi-language support was not a design goal, it would be a significant advantage for adoption.

\subparagraph{Finding 5: Maturity and Community-Support}
The maturity and community-support of tools impact the integration of the tools into the developer workflow. Some integrations are community-made and shared, so everybody has an advantage. The community support would also be necessary for CCAP to write and share additional analysis plugins and to integrate into different workflows.

\paragraph{Summary}
In summary, CCAP offers a good, but not best in class expandability with analysis plugins. It has a high useability for plugin developer and users. The lack of integrations for different workflows reflects the lack of community support and maturity. 
We see the CCAP as an addition to the powerful Sonarqube. The simple, yet robust expandability of the CCAP could supplement the shortcomings of Sonarqube in its powerful, but complex expandability. For instance, a code reviewer could code problematic code into a simple python plugin, distributes it to the team and would hopefully not reencounter the same problem.
Additionally, the CCAP could be used to teach about Clean Code and to enforce specific coding rules to students, since the local setup is straightforward. Clean Code rules could be turned into analysis plugins by students or teachers without having to understand a complicated interface.

\subsection{RQ2: What models perform well on detecting non-clean code?}
\paragraph{Motivation}
As we have shown with the CCAP and the analysis plugins, it is possible to write an algorithm that can detect certain code patterns like non-clean code patterns. If we are able to write a well-tested detection algorithm, we do not need error-prone machine learning models to detect code patterns. Nonetheless, if we can not design an algorithm that detects the desired code pattern reliably, we may perform better using a machine learning model. Additionally, if we encode a subjective pattern, it may not be possible to design an algorithm to objectively detect such a pattern. As a solution, we could label code parts as our pattern and use machine learning approaches to implicitly extract rules to detect such a pattern.

For this research question, we will evaluate different machine learning models on different code patterns and compare the results.

\paragraph{Approach}
For answering the research question, we will use the approach detailed in chapter \ref{chap:clean_code_classification}.  The dataset, consisting of 18 projects from GitHub, is prepared as described in chapter \ref{chap:clean_code_classification_dataset}. The data is split into 90\% train and 10\% test data. We use the training dataset to train Random Forest, Gradient Boosting Classifier, Support Vector Machines and an LSTM-based neural network. Due to the class imbalance, we will additionally train with an oversampled dataset (oversampling rate of 0.5) and an undersampled dataset (undersampling rate of 0.5). Since our test set contains real-world code samples, we expect it to represent a real-world label distribution, and we, therefore, accept the potential effects of the data imbalance. Nevertheless, we will try to train with an over- and undersampled dataset to observe the effect on the performance.

Our evaluation metrics are recall, precision and the combined f1 score. Due to the data imbalance, accuracy is not a meaningful metric. A high recall means a high detection rate of non-clean code. The costs of a false negative prediction are potential costs in unclean code (maintainability, understandability). With high precision, a detected non-clean code sample is likely to be a non-clean code sample and the system reports less false positives. The immediate cost of false-positive predictions (resulting in a lower precision) is the extra developer time needed. Additionally, the cry wolf effect comes into play\cite{breznitz_cry_1984}. In our case, a developer who has seen a false alarm will take subsequent alarms less serious. Additionally, a study has shown that a higher false alarm rate in advisory warning system in cars results in a lower, subjective evaluation of the system\cite{naujoks_cooperative_2016}. A low subjective evaluation of developers would lead to a decrease in adoption rate, which consequently leads to more unfixed problematic code. Since both recall and precision are important to the success of our models, we decided to use the equally weighted f1 score as a single-value evaluation metric.
Additionally, to compensate for the cascading effect of a low precision due to the cry wolf effect, we require a precision of 0.8 as a satisficing metric. This means we accept all models that return two false positive and eight true positive samples. On the other hand, we define the recall as our optimising metric.

We evaluate all models for the three different problem types RETURN\_NONE(RN), CONDITION\_COMPARISON\_SIMPLE(CCS) and CONDITION\_COMPARISON(CC). We expect the CC type to be the most difficult to learn since it is the most complex rule. CCS and RN should be easier to learn due to less possible  variations in the code structure.

\paragraph{Results}
We list all hyperparameter configurations and the corresponding training, test and holdout performance for all problem types in the appendix (Table \ref{tab:all_results_random_forest}-\ref{tab:all_results_svm}). We report the detailed analysis in a seperate finding for each model. Additionally, we describe our findings when comparing the models for the given classification task.



\subparagraph{Finding 1: poor SVM performance}
SVM performed poorly. Based on the f1 score, the best configuration has an f1 of 0.0066, a recall of 0.0134 and a precision of 0.0044. In other words, only 1.34\% of positive samples are recognised as positive. The model seems to fail to separate the two classes. A reason may be the class imbalance since only 0.21\% of all samples are non-clean code (positive) with the RETURN\_NONE type. In this configuration, we used undersampling with a ratio of 0.5, since oversampling was not feasible due to the increase in train time (see section \ref{sec:svm_quadratic_complexity}). After undersampling, 1/3 of all samples are therefore positive and 2/3 are negative. Since the time to train is still too long, we only took half of the undersampled dataset. 

We additionally used the SVM in a configuration with an automatic class weighting that is inverse-proportional to the class frequency. The recall score increases by 57-fold to 0.7660, whereas the precision decrease by half to 0.0026. We conclude that the SVM is very sensitive to class imbalance, which is an improper characteristic of our imbalanced classification problem. Additionally, a precision of 0.0026 is not useable for any application. We can not boost the precision with more data to help the model to distinguish true positives and false positives, since the train time will increase quadratically. We a baseline that low we do not expect much improvement with further investigation and therefore conclude that there is no valid reason to further invest in the SVM model.


\subparagraph{Finding 2: Random Forest}
Generally, the random forest classifier performs with an f1 score over 0.8 for all problem types with an oversampling ratio of 0.5, 100 trees, no additional class weighting and type encoding. For RN, this configuration has a 0.8379 f1, 0.7221 recall and 0.9978 precision on the test dataset. For CCS and CC, the f1 with 0.9329 and 0.929 are even better. The recall for those problem types increases to 0.8898 and 0.8888 while the precision remains high with 0.9803 and 0.9731, respectively. The random forest classifier therefore fulfils our satisficing condition of a precision score higher than 0.8.

Generally, we observe a performance increase by applying oversampling, especially for the RN problem type. From an f1 score of 0.7699 for 100 trees, no additional class balance and type encoding to 0.8379 with an oversampling ratio of 0.5. Since only 0.21\% of RN train samples are positive, oversampling those to half of the number of majority samples will lead to a bigger increase in training samples than oversampling less imbalanced problem types such as CCS and CC. This explains the better performance of the latter problem types since the oversampled train dataset is larger than for RN. Furthermore, we observe a negligible difference of 0.02 in f1 score for oversampling to a ration of 1.0 (same amount of samples for both labels).

Another common pattern is a worse performance on undersampled train sets. We observe a significant drop in performance for the RN problem type if we use a 0.5 undersampling ratio. Similarly, the performance for CCS and CC drops, although the decrease is smaller. Furthermore, we observe a moderate performance drop with 0.1 undersampling. From this pattern and the aforementioned observations for oversampling, we infer that a random forest classifier can profit from more data, even if they contain duplicated samples for the minority class due to oversampling. This can also explain the larger drop for undersampling for the RN problem type since the non-clean code class contains fewer samples for RN than for CCS and CC (2,078 for RN with 0.5 undersampling, 263,136 for CCS and 481,980 for CC). With fewer samples, the undersampling process reduces the overall amount of samples by deleting samples from the majority class. With an undersampling ratio of 0.1, fewer examples are deleted and the effect is less impactful but still observable. This effect is also visible in the smaller difference in performance between the CCs and CC problem type with undersampling. For CCS, the random forest classifier performs slightly worse than for CC, since only 131,568 samples have a CCS problem type whereas 240,990 samples contain the CC problem type.

The impact of class weighting in comparison to equivalent configurations seems to be negligible. 
To evaluate the impact of class weighting, we compare the variation in this parameter with otherwise identical configurations.
For an oversampling ratio of 0.5 with type encoding, we see 0.8379, 0.9329 and 0.929 in test f1 performance without class weighting and 0.8352(-0.0027), 0.9287(-0.0042) and 0.9267(-0.0023) in test f1 score performance. The difference is not significant.
For undersampling with a ratio of 0.5 and type encoding, the f1 score changes from 0.2583, 0.806 and 0.8632 test f1 score without class weighting to 0.2709(+0.0126), 0.8111(+0.0051) and 0.8671(+0.0039) test f1 score with class weighting. Again, the difference is not significant.
Without resampling and without type encoding, the f1 test performance decreases from 0.7814, 0.9057 and 0.9116 without class weighting to 0.7475(-0.0339), 0.8894(-0.0163) and 0.8913(-0.0203) with class weighting.
Last, without resampling but with type encoding, the f1 test score decreases from 0.7699, 0.9208 and 0.9211 without class weighting to 0.7506(-0.0193), 0.9064(-0.0144) and 0.9071(-0.014) with class weighting.
All changes due to class weighting are not significant, so class weighting does not have any positive influence on the performance of the random forest classifier.

Enabling type encoding improves our f1 performance for all configurations overall problem types except for the RN type without over- or undersampling. We already mentioned the extreme class imbalance for the RN problem type and therefore accept this type as an outlier to the general observation. The improvement in f1 scores is due to the improvement in recall of around 0.02 for all problem types excluding the aforementioned exception. This data implies that the additional type information is useful to more reliably determine our kinds of problematic code. We guess this boils down to the smaller variance in token types. For example, a string token is always threaded as the same token no matter the content. Since the content could change and may not be included in the dictionary, the variance in the type encoding is lower and therefore, easier to learn. TODO print feature importance!!!

Overall, it seems like a random forest classifier can handle class imbalance fairly well, as long as the distribution is not too unbalanced as for RN problem type. The performance deteriorates if the imbalance exceeds a critical threshold (for RN); otherwise, it has a minor impact.
We base this conclusion on several observations: First, we see better performance for the CCS and CC problem type overall, both of which have a more balanced distribution than the RN problem type although. It is important to note that the different problem types also have different difficulty levels for the model. But we assume the RN type is easier to detect for a model since the model only has to learn two subsequent tokens anywhere in the sequence. Therefore, we attribute this effect to the class distribution, but we remark that this may be tested in an additional, isolated experiment. Second, without over- or undersampling, we observe similar performance for CCS and CC, despite their different frequency in the train set (0.21\% of samples contain the RN type, 2.66\% CCS and 4.87\% CC). And finally, we would expect a bigger impact of class balancing, if the random forest classifier would be sensitive to class imbalance.
Additionally, encoding the token types results in a better performance than only encoding token values.

overfitting for random forest

\subparagraph{Finding 3: Gradient Boosting Classifier}
The overall best performing gradient boosting works with 300 boosting steps and a learning rate of 0.2. For the RN problem type, it delivers an f1 score of 0.9196, a recall score of 0.8666 and a precision of 0.9796. Comparable performance is measured for the CC type: 0.9008 f1, 0.8673 recall and 0.937 precision. The performance decreases for the CCS type to 0.8419 f1, 0.7902 recall and 0.9009 precision for the same, best-performing configuration. A decrease in performance for the CCS problem type applies to all configuration in comparison to the CC type. Nevertheless, the gradient boosting classifier fulfils our satisficing criteria of a precision score higher than 0.8.

Over- and undersampling does not have a positive effect: The f1 score of the 200 stages, 0.2 learning rate and 1.0 subsampling drops by 0.3459 for a 0.5 oversampling ratio for the RN problem type. With an undersampling ratio of 0.5, the f1 score decreases by 0.3761 to 0.4545 on the test set. The performance deterioration for the CCS and CC type are lower but still significant: For CCS, the f1 score drops by 0.0782 to 0.6948 with oversampling and by 0.0494 to 0.6836 with undersampling. Using over- or undersampling as rebalancing strategies does not improve the performance of the classifier. The best classifier configuration performs better on the RN and CC problem type compared to the CCS problem type (0.9196 and 0.9008 vs 0.84119 f1 on test dataset). This allows two implications: Since the RN type is strongly underrepresented in the train set (class frequency of 0.21\%) compared to the CCS class (2.26\% class frequency), the gradient boosting classifier does not suffer from strong class imbalances. On the other hand, the CCS problem type seems harder to learn, contradictory to our assumptions about the difficulty levels of the different problem types.

Furthermore, we experimented with giving only a subsample of the train data to the individual base learners for fitting (subsampling ratio of 0.7 and 0.4). Since we see the exact same performance metrics, this hyperparameter has no effect on the model's performance.

By contrast, increasing the number of boosting stages improves every precision metric for all problem types. From a f1 score of 0.6177, 0.5974 and 0.7314 for RN, CCS and CC at 100 boosting stages, we observe a improvement to 0.8306, 0.774 and 0.8567 at 200 boosting stages and 0.9196, 0.8419 and 0.9008 at 300 boosting stages. Additionally, the train performance for these configurations is similar to the test and holdout performance. Consequently, the classifier does not overfit on the train data and we could further increase the boosting stages to further improve the performance. Since additional boosting stages increase the train time of the model, we did not investigate further into this direction, but we expect by increasing the boosting stages and tuning the learn rate correspondingly, one could maximise the performance. 

In conclusion, the gradient boosting classifier is robust to overfitting and rather improves its performance with more boosting steps. We can also observe robustness for class imbalance and we therefore do not profit from our over- and undersampling approach. Subsampling has no effect and we observe an unexpected high difficulty of learning the CCS type for this classifier.




TODO: write in approach what we vary
- classify the problems into different difficulties

\subparagraph{Finding 4: LSTM}
The best performing LSTM model is trained with a 32 embedding size, a 256 batch and training with two or three epochs. For the RN problem type, the test f1 score for a two epoch training is insignificantly higher (0.9894 instead of 0.987), whereas for CCS and CC the three epoch training yield a slightly better performance of 0.9808 (instead of 0.9782) and 0.9782 (instead of 0.9774). Since the performance difference is marginal, we see the three epoch training as the best configuration for our LSTM model. Apparently, the LSTM-based neural network fulfils our satisficing condition with a precision higher than 0.8.

Since we have a remarkably high performance, we do not vary all parameters. Instead, we use a 0.5 under- and oversampling as for other models, vary the batch size and number of epochs and try reducing the size of the embedding layer since this layer contains the majority of the trainable parameter (see listing \ref{lst:lstm}).

Reducing the size of the embedding layer by half to 16, we do not observe a significant drop in performance compared to a bigger embedding size. With a drop of less than 0.01, using an embedding layer in the size of 32 is unnecessarily large. Even a size of 16 may be still too large and could be reduced further. Since we want to cover multiple classifiers and the LSTM-based neural network performs well on a reasonable training time of less than one hour without a GPU, we do not investigate the too powerful LSTM further and reduce the number of trainable parameters in the model. 

Using oversampling with a ratio of 0.5, we see a perfect recall score of 1.0 (rounded to the fourth decimal) and a near-perfect precision for all problem types. Since we duplicate the samples of the minority class up to the specified ratio, the model can overfit those samples more easily. Although the model overfits the train data, it still generalises well enough for comparable performance to the models trained on the original class distribution.

When we apply undersampling, we observe a drop in precision for all problem types, but a major drop in precision for the RN type (0.9952 to 0.8734). This behaviour is comparable to the performance drop of the random forest classifier with undersampling. We conclude, similar to the random forest classifier that the LSTM-based neural network reacts to less training data, especially for the RN problem type. The train set for the latter is significantly smaller than for the over problem types since its class frequency is only 0.21\% and therefore more samples from the majority class have to be deleted. In combination with too many trainable weights of our too powerful model, the model can not adjust its weights correctly. Since we reduce the variety of negative samples, it becomes harder to spot these. Consequently, the false positive rate increases and therefore, the precision decreases. The recall remains high since the model has enough samples to correctly label positive samples and the false negatives do not increase.

To sum up, we achieve near-perfect performance with our neural network model. We acknowledge, that our model is too big with too many trainable parameters we do not need for the same performance as shown with the reduction of the embedding layer size. The model does not benefit from oversampling and undersampling impairs the training of the model's weight since it reduces the overall number of samples.

\subparagraph{Finding 5: Comparison}
\input{tables/rq2_best_per_model.tex}
 Table \ref{tab:rq2_best_classifier} shows the performance score of the best performing model of each classifier for all problem types.
 The SVM performs poorly and does not meet our mandatory satisficing condition of precision higher than 0.8. Therefore, we will not evaluate it further. On the other hand, the LSTM-based neural network model outperforms all other models and we expect it to perform well in the next research question recognising modifications of the code.

 Both random forest and gradient boosting classifier fulfil the satisficing condition for all problem types. Interestingly, they supplement each other: For the RN problem type, the gradient boosting classifier performs better with an f1 score of 0.9328, whereas the random forest classifier surpasses the gradient boosting classifier for the CCS and CC problem type with 0.9352 and 0.9351 f1. As discussed, we explain the former result with the better insensitivity to a class imbalance of the gradient boosting classifier, whereas the latter seems to be harder to learn for the gradient boosting classifier. Especially the seemingly difficulties for the CCS type are unexpected.
 
 Note: in a real setup with hand-labelled examples, we expect more variety in the samples so the classifier may even perform better on the next research question 


 \subsection{RQ3: Can the models detect modified non-clean code that can not be detected by the original rule checker? }
See tables in attachment.

\paragraph{Motivation}
For the previous research question, we trained models on a dataset. We created the dataset by downloading open-source repositories and applying our analysis plugins from \ref{sec:analysis_plugins} to label samples as clean or non-clean code. With this automatic approach, it is simle to generate a dataset und to scale the dataset to any size needed for training. These algorithm that detect non-clean code are the prerequesite for the dataset and a successfull training of machine learning models. This research question wants to put the machine learning approach into a szenario, in which the prerequesite is not met and there no dataset can be generated. This szenario applies to clean code guidelines that can not be checked by an algorithm that analyse the source code or any other representation in a deterministic way. Alternativeley, a deterministic algorithm may only detect a subset of clean-code vilations. Both szenarios boil down to not having samples for all or many structural variations of the rule violation. Therefore, we evaluate, how our previously trained models perform on slightly modified code that still violates the same clean code rule but could not be detected by the original analysis plugins.


\paragraph{Approach}
To answer this research question, we use all models for all three problem types from the previous research question: For the RN problem type, we manipulate the holdout dataset as described \ref{sec:approach_code_manipulation}. The code still contain a logical \textit{return None}, although it is not explicitly written as two consecutive code tokens. This should explore, how well the different classifier extracted the structural information and can detect this noisy data. 

With the CCS models, we simulate the scenario of having trained on a subset of all variations and testing on the full set of problem variations. We do so by manipulating all rules found by the algorithm for the CC problem type to only contain samples that are not part of the CCS subset. The CCS algorithm could therefore not detect a clean code violation.

Last, we evaluate the models trained on the CC type with our manipulated code to provide a baseline for the CCS trained models. We expect the best performing CC model from the previous experiments should outperform all CCS models and should show similar performance like in the previous research question. We then investigate the the difference between the baseline and the model trained on CCS.

\paragraph{Results}
For the results, we evaluate the results of the three models: Random Forest Classifier, Gradient Boosting Classifier and LSTM-based neural network. Per model, we analyse the performance for the RN manipulation, the CCS manipulation, teh comparison baseline and compare the results with the results from the previous research question.


\subparagraph{Finding 1: Random Forest Classifier}
The random forest classifier trained on the RN problem type achieves less than 0.06 f1 score. The classifier can not handle this kind of manipulation.

The best CCS models achieves 0.7158 f1 with 0.758 recall and 0.678 precision. The basline model trained on the CC data reaches a 0.8186 f1 performance with a 0.9782 recall and 0.7038 precision. Compared to the baseline model, the CCS model has a 0.1 lower f1 score. The CCS model is trained with a 0.5 undersampling rate, 100 trees in the forest, class weighting and type encoding. The general better performance of models trained with undersampling is contradictory to the evaluation of the model in the previous experiment, in which oversampling performed better than undersampling. This reflects the baseline performance, for which the models with oversampled training and type encoding perform better than the undersampled counterparts. 

The precision of models trained on undersampled datasets is lower than on the models trained on oversampled or not resampled training data. On the other hand, the recal drops to zero (for oversampled or not resampled training data), which means the false negative rate increases. The model classifies most samples as negative and only a few as positive (TODO get numbers). With undersampled training, the model does not seem to be biased towards the negative classification as found with non-resampled or oversampled training data. Since the undersampling of the majority class (negative class, meaning clean code) reduces the number of unique negative samples in the training corpus, this decrease in bias seems reasonable. On the other hand, it is important to note that we test the model on the manipulated CC dataset that contains 5.1\% positive samples in comparison to the 2.51\% positive samples in the CCS train set without resampling. This may also influence the bias towards the negative samples.

\subparagraph{Finding 2: Gradient Boosting Classifier}
For the manipulated RN type, a gradient boosting classifier with a 0.5 oversampling ratio achieves a 0.24 f1 score with 0.7676 recall and 0.1422 precision. The second best configuration is the equal configuration with a 0.5 undersampling ratio with similar performance scores. The same configuration trained on non-resampeld training data has a near zero f1 and recall performance and a higher 0.349 precision. Decreasing the number of boosting stages with 0.5 over- or undersampling decreases the precision significantly while increasing the recall.
With over- or undersampling, the model seems be biased towards classifying samples positive, indicated by the low precision. Without resampling, the classifier is biased towards the negative class and will wrongly assume positives to be negatives. Since the class distribution is compareable to the class distribution of the training dataset, the classifier is biased towards the negative class.

This behaviour is contrary to the overservation during the train and testing for the previous research question:  We received a significant better performance without resmapling the training data and we concluded, that the gradient boosting classifier is not so sensitive towards class imbalance. Taking this into account, the those models can not detect the manipulated RN problems but instead just guesses the result. Since the recall drops to zero without a rebalanced training in this experiment, this should be valid for these model configurations. Although the models trained on resampled models perform better, they were significantly worse in detecting the original problem type in the previous experiment. If those model configurations would only guess for the manipulated code, we would still see a slight tendency towards negative classification (resulting in low recall). This is not the case. Another explanation may lay in the smaller dataset for undersampled training due to the low frequency of positive samples. With less training data, the model was not able to generalize those samples very well. Consequently it may only learned the occurence of a \enquote{return} token to be enough. This would explain this slight performance advantage. But overall, the gradient boosting classifier is not useable for the manipulation on the return None type as done in this experiment.
TODO: fuze with next few lines into one analysis since it applies both

The results for the CCS trained model draw a similiar picture: With an oversampling rate of 0.5, the configuration achieves the best f1 score of 083 with 0.978 recall and 0.7209 precision. We observe rather similiar performance with an f1 score of 0.8291, a recall of 0.9773 and a precision of 0.7199. The performance of non-resampled configuration drops below 0.53 f1 score. Especially the best performing classifier for the direct classification now has the lowest performance for this task. Since this applies to both problem types, we interprete these results as follows: With better fitting of the original classification problem, the gradient boosting classifier learnt to detect the sequence of subsequent token types as seen in the RN and CCS training data. After code manipulation, the structure of the code and therefore the sequence of the tokens is changed. The better the model trained to detect the subsequent tokens with more boosting stages or more unique data, the better the performance for the previous experiment. Consequently, the model did not learn the general structure like a direct comparison following within the next few tokens of the \enquote{if} keyword, but it overlearnt the subsequent strucutre of the tokens. On the other side, this is another indicator the model did not train on the surrounding context but instead on the tokens of the problem.


LSTM:
- RN: work better with 0.38, still not good
- CC: 0.65, worse than gbc and worse than random forest. although it performed increadibly well on the direct training

CCS was a subsample of CC, --> check what CCS classifier were good in comparison to now good

--> reformulate findings to do seperate findings for worse performance on return none, the better on original the worse on this


Diskussion:
why is RN so bad, why CC moderately possible?
How could this be further improved? Maybe use finetuning training with just a few samples (like transfer learning assume the model has generalized basic structure and has to be finetuned for this types --> would be good since you only need to handsample a few samples to have a good working model for this types of problems) --> general mark importance of ML if there are no rules available therefore it is not posisble to generate a dataset like we did --> an approach with just a few samples would be great

simple encoding does not explore the code structure, another encoding necessary