In this chapter, we will evalute the approach described in \Cref{chap:approach}. First, we will define the research questions. Afterwards, we will describe the motivation, details of the approach, and the results per research question. We explain the results we observe and recommend possible improvements.

\section{Research Questions}
The main objectives of this work are twofold: First, we design and implement the CCAP as a platform for automated code checking. We compare our platform with preexisting tools and validate, if the CCAP can supplement or replace an existing tool. 
Second, we use machine learning models to detect violations of a clean code rule. We train models to detect a specific problem pattern and evaluate the generalisation capabilities on unseen pattern variations. By testing on unseen pattern variations, we want to simulate a real-world scenario for checking for rules with high level of complexity. For rules with an high level of complexity, we can not generate a labelled dataset automatically. Instead we have to hand-collect samples, that may be limited in variety.

We formulate this objectives into the following research questions that will lead this evaluation:
\begin{description}
    \setlength{\itemsep}{1pt}
    \item[RQ1]What is the utility of the CCAP besides existing tools? 
    \item[RQ2]How do different models compare on the task of detecting non-clean code?
    \item[RQ3]Do machine learning-based models cover a larger variety of cases than rule-based checker? 
\end{description}

\subsection{RQ1: What Is the Utility of the CCAP Besides Existing Tools?}\label{rq:1}
\paragraph{Motivation}
Several tools are established that aim to improve code quality and detect unclean code (see \Cref{sec:tool_comparison}). Every new tool with similar claims has to prove its usefulness besides the preexisting tools. For this reason, we compare the CCAP with preexisting tools to evaluate its utility.

\paragraph{Approach}
To answer this research question, we compare the CCAP with existing tools based on the design goals extensibility, useability and integration. Additionally, we will mention and compare useful features unique to existing tools. Finally, we asses if it can supplement or replace existing tools. As described in \Cref{sec:tool_comparison}, the compared tools are Sonarcube, PMD Source Code Analyser Project, Codacy and PyLint.

\paragraph{Results}
TODO vergleichstabelle

\subparagraph{Finding 1: Extensibility}
The extensibility of CCAP with analysis plugins is comparable to PyLint, that also offer plugins to analyse the raw string, the token stream or the AST. PMD allows plugins to analyse the AST or define XPath rules. Since XPath rules can be used in the CCAP as well (using a third-party library in the plugin), PMD offers fewer expansion possibilities. Sonarqube, on the other hand, provides the most possibilities for extension. Not only can developers analyse the AST or specify XPath rules, but they have access to an additional semantic model of the code that provides direct access to structures such as methods, parameters, and return values. This semantic model simplifies an analysis in comparison to the AST analysis.
Additionally, Sonarqube allows plugins to expose an API for other plugins to use. With plugins that expose funcitonality to other plugins, the possibilites for rule checking plugins increases further and functionality can be reused. The least expandability offers Codacy, that allows customising or disabling existing rules. However, it does not offer to add rules to the existing ruleset.

Regarding output plugins, no compared tool offers the output customisation of CCAP. PMD and PyLint have different predefined output formats like JSON, HTML, CSV, or text. Although PyLint allows customising the message format using a formatting string as a command-line argument, they do not offer extensions for the output. Sonarqube displays the analysis reports in its WebUI. The scanner tool, SonarScanner, sends the reports to the server component, that renders the results in the browser. Codacy's local scanner produces text output; the cloud version also has a WebUI. 
With the extension possibility of CCAP, we offer the developer a simple mechanism to adapt the output to its own needs. While this feature is unique, the result could also be achieved by parsing the JSON ouput of e.g. PyLint in a subsequent step.

\subparagraph{Finding 2: Integration}
The CCAP has significant shortcomings in its integrations into IDEs, build processes and CI pipelines. All contestant tools provide plugins for build systems like Gradle or Maven. Except for Codacy, all tools have IDE integrations into the most common editors. Sonarqube and Codacy have integrations into source control systems. Same applies to PMD and PyLint when bundled with Codacy.

\subparagraph{Finding 3: Useability}
ISO 9241 defines useability is defined as the\enquote{extent to which a system, product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use}~\cite{isotc_159sc_4_iso_2018}. For the tools, we identify two main user groups: the plugin developers want to encode a rule to be checked automatically and the developers want to analyse their code. We compare the useability for the tools with the focus on the efficiency, measured as required time to achieve a useful output.

For a plugin developer, the useability of the plugin system is simple for CCAP and PyLint. Both tools have the same plugin mechanism and a simple plugin interface. A simple interface reduces the required time to understand and use the interface.
PMD has a similar, straightforward plugin interface, but its Java plugin requires an extra bundling step before adding to the classpath. The latter applies to Sonarqube as well; additionally, the powerful and rich plugin API of Sonarqube is contrary to the simple plugin interface of CCAP and PyLint. While this opens more possiblities to the developer, it requires more time to understand the possibilities. Especially, if a developer want to quickly encode a problematic pattern that does not fit an XPath rule, the more complicated plugin interface of Sonarquabe can decrease the efficiency. 

For the developer, installing CCAP is like installing every other python package. Running from the command-line is fast and can be automated, e.g. using git pre-commit hooks. The same workflow could be achieved with PyLint and PMD. Sonarqube is more complex to install due to its multiple components, but after the setup, the WebUI and integrations into most developers workflows have a compareable or better useability than CCAP or PyLint. Codacy as a cloud service has the simplest setup by granting access to the source code repository to start scanning the code. The setup of the local scanner requires a configuration with a project token to retrieve information from the cloud service.

\subparagraph{Finding 4: Multi-Language Scanning}
Sonarqube, PMD and Codacy offer multi-language scanning. This multi-language ability is advantageous for code repositories with multiple languages and for reusing the same tools and infrastructure for multiple projects with different programming languages. 

Supporting multi-language scanning is not possible with CCAP and would require a modification of the architecture: The source handler would have to scan the files with respect to their programming language and transform those into a language-independent representation. Other parts of the architecture could remain similar. Although multi-language support was not a design goal, it would be a significant advantage for adoption.

\subparagraph{Finding 5: Maturity and Community Support}
The maturity and community-support of tools impact the integration of the tools into the developer workflow. Some integrations are community-made and shared, so everybody has an advantage. 
CCAP has no community support, since it has not been released. The community support would be necessary for CCAP to write and share additional analysis plugins and to integrate into different workflows. 

\paragraph{Summary}
In summary, CCAP offers a good, but not best in class extensibility with analysis plugins. It has a high useability for plugin developer and users. The lack of integrations for different workflows reflects the lack of community support and maturity. 
We see the CCAP as an addition to the powerful Sonarqube. The simple, yet robust expandability of the CCAP could supplement the shortcomings of Sonarqube in its powerful, but complex expandability. For instance, a code reviewer might identify an unclean code snippet, writes the corresponding new plugin to identify this problem in the future, distributes it to the team, and would hopefully not reencounter the same problem again. With Sonarqube, the effort to understand the plugin interface may surpass the time the reviewer is willing to invest.
Additionally, the CCAP could be used to teach about Clean Code and to enforce specific coding rules for students' exercises, since the local setup is straightforward. Clean Code rules could be turned into analysis plugins by students or teachers without having to understand a complicated interface or requiring a server setup.

With an increase in adoption and further development, the drawbacks of integration and community support would diminish. It has the potential to become an equal contestant to PyLint.

\subsection{RQ2: How Do Different Models Compare on the Task of Detecting Non-Clean Code?}\label{rq:2}
\paragraph{Motivation}
As we have shown with the CCAP and the analysis plugins, it is possible to write an algorithm that can detect specific code patterns like non-clean code patterns. If we can write a well-tested detection algorithm, we do not need error-prone machine learning models to detect code patterns. Nonetheless, if we can not design an algorithm that detects the desired code pattern reliably, we may perform better using a machine learning model. Additionally, if we encode a subjective pattern, it may not be possible to design an algorithm to detect such a pattern objectively. As a solution, we could label code parts as our pattern and use machine learning approaches to extract rules to detect such a pattern implicitly.


\paragraph{Approach}\label{par:approach}
For answering the research question, we will use the approach detailed in \Cref{chap:clean_code_classification}.  The dataset, consisting of 18 projects from GitHub, is prepared as described in chapter \Cref{chap:clean_code_classification_dataset}. First, 20\% of all files are moved to a seperate holdout dataset. We split the remaining files into 90\% train and 10\% validation data. We use the training dataset to train Random Forest Classifier, Gradient Boosting Classifier, Support Vector Machines and an LSTM-based neural network. Due to the class imbalance, we will additionally train with an oversampled dataset (oversampling rate of 0.5) and an undersampled dataset (undersampling rate of 0.5). Since our datasets contain real-world code samples, we expect them to represent a real-world label distribution, and we, therefore, accept the potential effects of the data imbalance. 

Our evaluation metrics are recall, precision and the combined f1 score. As described in \Cref{sec:inbalanced_dataset}, accuracy is not a meaningful metric due to the data imbalance. We choose the f1 score as combined metric, since we see recall and precision as equally important. This opinion is ensured by the f1 score that is the harmonic mean of recall and precision.
Our reasoning to weight recall and precision equally is as follows:
A high recall means a high detection rate of non-clean code. The costs of a false negative predictions are potential costs in unclean code (maintainability, understandability). With high precision, a predicted non-clean code sample is likely to be a non-clean code sample and the system reports less false positives. The immediate cost of false positive predictions (resulting in a lower precision) is the additional time of developers to identify it as an false alarm.
Additionally, the Cry Wolf Effect comes into play~\cite{breznitz_cry_1984}. In our case, a developer who has seen a false alarm will take subsequent alarms less serious. A study has shown that a higher false alarm rate in the advisory warning system in cars results in a lower, subjective evaluation of the system~\cite{naujoks_cooperative_2016}. A low subjective evaluation of developers would lead to a decrease in adoption rate, which consequently leads to more unfixed problematic code. Since both recall and precision are essential to the success of our models, we decided to use the equally weighted f1 score as a single-value evaluation metric.
Additionally, to compensate for the cascading effect of a low precision due to the cry wolf effect, we require a precision of 0.8 as a satisficing metric. Accordingly, we accept all models that return two false positive and eight true positive samples. On the other hand, we define the recall as our optimising metric. 

We evaluate all models for the three different problem types \texttt{RETURN\_NONE} (RN), \texttt{CONDITION\_COMPARISON\_SIMPLE} (CCS) and \texttt{CONDITION\_COMPARISON} (CC). We expect the CC type to be the most difficult to learn since it is the most complex rule. CCS and RN should be easier to learn due to less possible variations in the code structure.

\paragraph{Results}
We list all hyperparameter configurations and the corresponding training, validation and holdout performance for all problem types in the appendix in \Crefrange{tab:all_results_random_forest}{tab:all_results_svm}. We report a detailed analysis in a separate finding for each model. Additionally, we describe our findings when comparing the models for the given classification tasks.



\subparagraph{Finding 1: Support Vector Classifier} 
\textit{The support vector classifier in our configurations is not able to detect any problem type.}

Due to the quadratic time complexity during training described in \Cref{sec:svm_quadratic_complexity}, we were only able to test a few configurations. Since the scope of paper is the comparison between different models, we did not have time to optimise the support vector classifier. We only tested configurations with a general subsampling and undersampling to reduce the input size and to achieve a manageable training time. Therefore, we do not have data for non-resampled datasets or oversampled datasets to draw reasonable conclusions. 

We can only observe some effects:
First, the class balancing increases the recall by up to 0.79 for RN, 0.5 for CCS and 0.55 for CC. Second, undersampling with a ratio of 0.1 results in a f1 performance of zero for the tested RN and CCS problem types. Third, the f1 performance for the CC type is almost double the performance for the CCS type, i e. 0.11 vs 0.06. The f1 performance for the CCS type is ten times the performance of the RN type, i.e. 0.062 vs 0.0066. 
The zero performance with an 0.1 undersampling ratio may be an indicator of a strong sensitivity for class imbalance. Although we did not test this hypothesis any further, due to the time constrains detailed above, this is in line with the work of Tian et. al~\cite{tian_imbalanced_2011}. They observe a poor performance of support vector classifier on imbalanced datasets and and evalaute further mitigation strategies. 

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
    In conclusion, the support vector classifier does not fulfil our acceptance criteria of precision higher than 0.8. Especially the proneness to class imbalance decreases the useability for our problem domain with severely imbalanced datasets.  
    Furthermore, train time limits our experiments. It is important to note that we did no further investigation in improving the train time or performance. It could be possible to increase classification and runtime performance by choosing a different kernel function such as a linear kernel function or by following strategies adressing this problem as proposes by Tian et al.~\cite{tian_imbalanced_2011}. We unsuccessfully try to combat class imbalance by our undersampling approach. Oversampling would increase the training time and is not feasible. We see further improvement as out of scope for this thesis, since we want to compare different machine learning models and not optimising every single classifier. 
    \end{minipage}}
\end{center}


\subparagraph{Finding 2: Random Forest}
\textit{Random forest classifier performs well and fulfils our satisficing condition on precision.} It achieves f1 scores of over 0.8 for the RN type and over 0.9 for the CCS and CC type.
The best performing random forest classifier uses a 0.5 oversampling ration, 100 trees, type encoding and no additional class weighting.


\textit{Our observation indicates that the random forest classifier profits from more data, even with data duplication.} We base this thesis on our observations with over- and undersampling the training data. 
All oversampled configurations perform better on the validation set than their not resampled counterparts. This performance improvement is especially large for the RN type, with an f1 score increase of 0.03-0.05. For the CC and CCS problem type, the performance growth is smaller, with only around 0.01 in f1 score. We explain the difference between the problem types in the difference in class frequency as we described in \Cref{sec:inbalanced_dataset}. For the RN problem type, only 0.21\% of the samples are labelled positive, whereas 2.66\% of the samples for CCS and 4.87\% of the samples for CC are labelled positive. Oversampling the data with a ratio of 0.5 results in more duplicated samples from the minority class for the RN problem type than for CCS and CC. \Cref{tab:resampling_size_performance_rf} contains detailed measures of the train set size after oversampling and the performance metrics.

\textit{For undersampling, we observe a decrease in performance for all problem types.} For the RN problem type, the f1 performance drops by up to 0.51, whereas the performance decrease for CCS and CC is lower with up to 0.12 and 0.07 for an undersampling ratio of 0.5. We explain this difference again with the difference in class frequency and the resulting change in training size. With an undersampling ratio of just 0.1, fewer samples from the majority class get deleted and the performance for RN only deteriorates by 0.08.
The different performance decrease between the CCS and CC type can also result from the difference in class frequency. The undersampling with an ratio of 0.5 reduces the training size for CCS to 394,704 whereas for CC just to 722,970. With an undersampling ratio of just 0.1, the effect is smaller and only visible for the RN type (see \Cref{tab:resampling_size_performance_rf}).


\begin{table}[]
\tabcolsep=0.11cm
\begin{tabularx}{\textwidth}{lXXX|rrr}
\toprule
                    & \multicolumn{3}{c}{Test F1 Score} & \multicolumn{3}{c}{Dataset}   \\ \midrule
                    & RN        & CCS        & CC     & RN            & CCS         & CC          \\ \midrule
No resampling     &  0.7599   &  0.9208    &  0.9211  &  4,950,329    & 4,950,329   & 4,950,329   \\
0.5 oversampling  &  0.8379   &  0.9329    &  0.929   &  7,409,925    & 7,228,141   & 7,064,008   \\
1.0 oversampling  &  0.8358   &  0.9287    &  0.9273  &  9,879,900    & 9,637,522   & 9,418,678   \\ \midrule
0.1 undersampling &  0.6857   &  0.9268    &  0.929   &  114,169      & 1,447,248   & 2,650,890   \\
0.5 undersampling &  0.2583   &  0.806     &  0.8632  &  31,137       & 394,704     & 722,970     \\ \bottomrule
\end{tabularx}
\caption{Dataset sizes for different resampling strategies with f1 performance on the validation set. The f1 performance for the speciifc resampling is the random forest classifier with 100 trees, no class weighting and type encoding.}
\label{tab:resampling_size_performance_rf}
\end{table}

\textit{Our experiments show that the random forest classifier is not very prone to class imbalance.} Only an extreme class imbalance as for the RN type with a class distribution 0.21\% vs 99.79\% limits the classifier (TODO comemntar: evtl durch aufbau von random forest erklären). We conclude this insight from two main observations. 
First, all performance metrics for the CCS and CC problem type are comparable for all configurations without over- or undersampling. If the model would be sensitive to class imbalance, the higher class imbalance of the CCS type (97.34\% vs 2.66\%) should reflect in a worse performance than the CC type. We do not observe such performance differences. By contrast, the class frequency of positive samples for the RN type is by a factor twelve smaller than for the CCS type. The resulting severe class imbalance seems to affect the performance negatively. 
Second, the class weighting has no positive influence on the performance of the classifier. With balanced class weighting, the weighting for each class is adjusted inversely proportional to the class frequency. If the random forest classifier were sensitive to class imbalance, we would expect a performance improvement. Instead, we see a slight deterioration in f1 performance up to 0.04.


\textit{Type encoding increases the performance of the random forest classifier slightly for the CCS and CC type of up to 0.02.} For the RN problem type, we observe a similar improvement for oversampling with type encoding. For undersampling with type encoding, we observe a larger decrease of around 0.09 in f1 performance (TODO: precision und recall anschauen was wird schwächer und warum). 
We expected type encoding to have a more significant impact. The types should work as an additional feature the classifier can utilize. With the index-based encoding of the token values, 100,001 possible values can be encoded. The number of different token types is significantly smaller with only 61 different types (see the complete list of possible token types in \Cref{fig:tokenizer_types}). Especially for function and method names, it would allow the model to treat two different function calls as one. 
However, the improvement is lower than expected. At least for the CCS type, the \texttt{NAME} type of a method call should be easier to learn on than the changing value of the method name.  

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
    Overall, the random forest classifier handles class imbalance fairly well and can profit from more data, even if it contains duplicates from oversampling. Undersampling decrease the performance of the classifier. Type encoding has slight positive impact on the performance.
    \end{minipage}}
\end{center}

\subparagraph{Finding 3: Gradient Boosting Classifier}\label{finding:rq2_gbc}
\textit{The gradient boosting classifier performs well on all problem types and fulfils our satisficing metric.} We achieve an f1 score of over 0.9 for the RN and CC type and 0.84 for the CCS type. This best performing configuration is trained with 300 boosting stages, a learning rate of 0.2 and type encoding.


\textit{The gradient boosting classifier is sensitive to less training samples or samples with less variety.} Two main observations allow this conclusion. First, we observe a decrease in performance for under- and oversampling (see  \Cref{tab:resampling_size_performance_gbc}). For undersampling the RN type with a ratio of 0.5, the f1 score drops from 0.8306 by 0.3761 (roughly 45\%) and by 0.3759 (approximate 41\%) to 0.4847 for oversampling with a ratio of 0.5. For the CCS type, the f1 score drops from 0.773 by approximately 0.08 to 0.6948 and 0.6836 for over- and undersampling. The smallest drop is for the CC type from 0.8567 by around 0.06 to 0.79 for over- and undersampling. With undersampling, we reduce the number of examples and with oversampling, we increase the minority class without increasing the variety of samples. Since the performance characteristics for over- and undersampling are comparable, we conclude that more samples with the same variety further decrease the model's performance. The decrease in f1 score is caused by a drop in precision, i.e. an increase of false positives. With less data by undersampling or less variety by oversampling, the classifier tends to be biased towards the positive class.
Second, the larger performance drop for the RN problem type and the smaller decrease for the CC type correlates with the number of training samples. A larger decrease in training data results in a larger drop in performance. 

\begin{table}[]
    \tabcolsep=0.11cm
    \begin{tabularx}{\textwidth}{lXXX|XXX}
    \toprule
                        & \multicolumn{3}{c}{Test F1 Score} & \multicolumn{3}{c}{Dataset}   \\ \midrule
                        & RN        & CCS        & CC     & RN            & CCS         & CC          \\ \midrule
    No resampling     &  0.8306   &  0.773    &  0.8567  &  4,950,329    & 4,950,329   & 4,950,329   \\
    0.5 oversampling  &  0.4847   &  0.6948   &  0.7933  &  7,409,925    & 7,228,141   & 7,064,008   \\
    0.5 undersampling &  0.4545   &  0.6836   &  0.7925  &  31,137       & 394,704     & 722,970     \\ \bottomrule
    \end{tabularx}
    \caption{Dataset sizes for different resampling strategies with f1 performance on the validation set. The f1 performance for the speciifc resampling is the gradient boosting classifier with 200 stages, a learning rate of 0.2 and type encoding.}
    \label{tab:resampling_size_performance_gbc}
    \end{table}

\textit{The gradient boosting classifier is insensitive towards class imbalance.} One supporting point is the aforementioned performance decrease with over- or undersampling. Additionally, the performance of the RN problem type is comparable to the CC problem type despite their difference in class imbalance (0.21\% class frequency for RN vs 4.87\% for CC). 

\textit{The gradient boosting classifier has problems learning the CCS type.} Contradictory to our assumptions, the gradient boosting classifier learns the CC type better than the CCS type. The precision is lower by up to 0.05, whereas the recall is lower by up to 0.11 (without resampling and $\geq 200$ boosting stages). We assume, the gradient boosting classifier has trouble learning the CCS subset and instead learns the more general superset of CC problems. To recap, the CCS problem type only labels a direct comparison in the if-condition as problem wheras the CC type consider nested boolean expressions. With the CCS subset, we restrict the classifier to only detect a direct comparison. 

We analysed the classification of the holdout samples, especially the false-positive ones. If the classifier learn the structure of the CC superset, the classifier would label samples as positive that are only positive for the CC type. We randomly chose 20 false-positive samples and found 11 samples like in \Cref{lst:gbc_false_positive}. Those 11 samples contain a direct comparison nested inside boolean operators. Such nesting is only labeled as a CC problem, not as CCS problem. We think our restriction in problem types collide with the classifier's ability to detect the more gneral pattern of a comparison somewhere inside the if-condition.


\textit{Subsampling the train data for the individual base learner during training has no impact.} The performance for all subsampling ratios are identical, so the individual base learners are trained with the same data as before.


\begin{lstlisting}[float=h, language=Python, label=lst:gbc_false_positive, caption={False-positive samples for the best performing random forest classifier with 300 boosting stages, a learning rate of 0.2, type encoding and without resampling for the CCS type. We removed spaces between tokens if necessary, but leave the intendation as in the sample.}]
    if self . _supported_features & support_color : 
    self . _hs_color = ( 
<UNKNOWN> ( self .
DEDENT,NAME,NAME,OP,NAME,OP,NAME,OP,NEWLINE,INDENT,NAME,OP,NAME,OP,OP,NL,NAME,OP,NAME,OP
########################

if self . state is not none and self . _sensor_type [ 0 ] == "battery" : 
   
NEWLINE,NAME,NAME,OP,NAME,NAME,NAME,NAME,NAME,NAME,OP,NAME,OP,NUMBER,OP,OP,STRING,OP,NEWLINE,INDENT
########################
if isinstance ( <UNKNOWN> , datetimearray ) and <UNKNOWN> . tz is not none : 
    pytest .
INDENT,NAME,NAME,OP,NAME,OP,NAME,OP,NAME,NAME,OP,NAME,NAME,NAME,NAME,OP,NEWLINE,INDENT,NAME,OP
\end{lstlisting}

\textit{The gradient boosting classifier is robust to overfitting the training data.} We observe a comparable train and test performance for all non-resampled configurations. Only with under- or oversampling, the classifier seems to overfit the train data, since the test performance is worse than the train performance. 
Another indicator for the robustness is the performance increase when using more boosting stages. Increasing the boosting stages increases the recall and precision for all problem types. Our best model with 300 boosting stages and a 0.2 learning rate can probably be more fine-tuned by increasing the boosting stages and adapting the learning rate. Again, being restraint by the increase in training time, we did not fine-tune the classifier further.

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
        In conclusion, the gradient boosting classifier is robust to overfitting and somewhat improves its performance with more boosting steps. We can also observe robustness for class imbalance and on the other hand sensitivity towards less training data. Subsampling has no effect and we observe an unexpected high difficulty of learning the CCS type for this classifier.
    \end{minipage}}
\end{center}


\subparagraph{Finding 4: LSTM-Based Neural Network}
\textit{Our implementation of an LSTM-based neural network performs well and fulfils our satisficing condition.} We achieve an f1 score of over 0.97 for all problem types without resampling, 10 LSTM cells, an embedding size of 32 and training with 2 epochs and 256 batch size. 


Since we have a remarkably high performance, we do not vary all parameters. Instead, we use a 0.5 under- and oversampling as for other models, vary the batch size and number of epochs and try reducing the size of the embedding layer since this layer contains the majority of the trainable parameter (see \Cref{lst:lstm}).


\textit{Oversampling does not have a measurable impact on our LSTM-based neural network.} The f1 performance with a 0.5 oversampling is not significantly different from the performance without oversampling. We observe overfitting on the train data with a near-perfect recall and precision. The test performance remains higher than 0.97 f1 score. Although the model overfits to the train data, it still generalises well enough with comparable performance to the models trained on the original class distribution.


\textit{The LSTM-based neural network is sensitive to fewer train data.} We draw this conclusion based on the performance deterioration for undersampled train sets (see \Cref{tab:resampling_size_performance_lstm}). For the RN type, undersampling results in a significant drop in performance from 0.987 by roughly 36\%. For CCS and CC, the performance decrease is smaller at less than 3\%. As described before, the train set of the RN type is significantly smaller after undersampling than for the CCS and CC type, due to the lower class frequency of the minority class of only 0.21\% before undersampling. Therefore, more samples from the majority class are deleted during undersampling and the variety decreases. In combination with many trainable parameters in our model, the model fails to fit the training data correctly. 
Responsible for the performance drop is a decrease in precision, whereas the recall remains high. In other words, the false-positive rate increases by factor 100 to 0.3\%. By removing negative samples and variety in training, we think it becomes more challenging for the model to detect negative samples correctly. Furthermore, the recall remains high, which is a sign that the model tends to classifiy more records into the positive class. In \Cref{fig:conf_diagram_undersampling}, we see this trend towards the positive class but with increased unconfidence. Without undersampling, the model is very confident about the true negatives and false positives but is unconfident with some samples.  



\begin{figure}[ht]
	\begin{tabular}{c}
		\begin{subfigure}{0.5\linewidth}
            \resizebox{1\textwidth}{!}{\input{diagrams/rq2_lstm_not_resampled_confidence_histogram.pgf}}
			\caption{Confidence Histogram after training without resampling.}
			\label{fig:conf_diagram_without_resampling}
		\end{subfigure}%
		\begin{subfigure}{0.5\linewidth}
            \resizebox{1\textwidth}{!}{\input{diagrams/rq2_lstm_undersampled_confidence_histogram.pgf}}
			\caption{Confidence Histogram after training with a 0.5 undersampling-ratio.}
			\label{fig:conf_diagram_undersampling}
		\end{subfigure}%
	\end{tabular}
	\caption[Confidence Histogram for the LSTM model with non-resampled and resampled training set.]{Confidence Histogram for predicting the holdout set for an LSTM trained with an embedding size of 32, three epochs, a batch size of 256 and 10 LSTM-cells}
	\label{fig:eval:distribution}
\end{figure}


\begin{table}[]
    \tabcolsep=0.11cm
    \begin{tabularx}{\textwidth}{lXXX|rrr}
    \toprule
                        & \multicolumn{3}{c}{Test F1 Score} & \multicolumn{3}{c}{Dataset}   \\ \midrule
                        & RN        & CCS        & CC     & RN            & CCS         & CC          \\ \midrule
    No resampling     &  0.987   &  0.9808    &  0.9782  &  4,950,329    & 4,950,329   & 4,950,329   \\
    0.5 oversampling  &  0.9859   &  0.9781   &  0.9745  &  7,409,925    & 7,228,141   & 7,064,008   \\
    0.5 undersampling &  0.6294   &  0.9564   &  0.9561  &  31,137       & 394,704     & 722,970     \\ \bottomrule
    \end{tabularx}
    \caption{Dataset sizes for different resampling strategies with f1 performance on the validation set. The f1 performance for the speciifc resampling is the LSTM with an embedding size of 32, 10 lstm cells, a training on 3 epochs with a batch size of 256.}
    \label{tab:resampling_size_performance_lstm}
\end{table}


\textit{Our model configuration has too many trainable parameters for the problem complexity.} We base this assumption on two observations. First, we halved the size of the embedding layer to 16 and consequently reducing the 3,201,795 trainable parameters of the model to roughly half the size. If the model needed all parameters to fit the training data accurately, we would expect a performance drop. Instead, we do not observe any significant change in performance. Due to this observation, we assume a further reduction of the embedding size may be possible without performance loss. However, since this configuration trains in a reasonable time of under an hour without a GPU and has an acceptable performance on the holdout set, we do not investigate this too powerful model further.
Second, a variation of the epochs and batch size for the training yields no significant difference. We think, the problem detection for our problem types is too simple for the model and it quickly reaches its optimum. Further training does not increase the model's performance further. 

\begin{center}
    \fbox{\begin{minipage}{\dimexpr\textwidth-0.5cm}
        To sum up, we achieve near-perfect performance with our neural network model. We acknowledge, that our model has too many trainable parameters that we do not need for the same performance as shown with the reduction of the embedding layer size. The model does not benefit from resampling. Oversampling has no impact on the test performance and undersampling impairs the training of the model's weight since it reduces the overall number of samples.
    \end{minipage}}
\end{center}

\subparagraph{Finding 6}
We found a weaknes in the way we labeled samples with the rule checkers. We decided to label a sample as rule violation, if all tokens of the problematic line are present. With the sliding window approach, it is possible to have all tokens for the detection in the sample, but missing the first or last newline or intendation tokens. The sample would therefore be labeled as clean code, although the problematic pattern is there. In the analysis of the false-positives for the gradient boosting classifier, we found samples for both cases in the false-positives as seen in \Cref{lst:weaknes_end_start_sliding_window}. In the first sample, not all preceding \texttt{DEDENT} types are within the window, wheras the trailing \texttt{NEWLINE} token is missing in the second sample.

To improve this weaknes, we would strip out all \texttt{DEDENT} and possibly \texttt{NEWLINE} tokens from the data before applying the sliding window. Due to time constrains, we are not able to retrain all models with the improved approach. However, the problematic line will occur in at least another sample, if the line is shorter than 16 tokens. It will most likely not impact the variety of problem patterns, but it will slightly reduce our performance score due to false-positives.


\begin{lstlisting}[float,floatplacement=H, language=Python, label=lst:weaknes_end_start_sliding_window, caption={False-positives of the best gradient boosting classifier with 300 boosting stages, a learning rate of 0.2, type encoding and without resampling for the CCS type. In the first sample, not all preceding \texttt{DEDENT} tokens are in this window. For the second example, the trailing \texttt{NEWLINE} token is missing in the window. Without this encoding issue, both samples would have been detected correctly.}]
if 'add_host' in <UNKNOWN> : 
<UNKNOWN> 
                            <UNKNOWN> = <UNKNOWN> . get ( 'add_host' ,
DEDENT,DEDENT,DEDENT,NAME,STRING,NAME,
NAME,OP,NEWLINE,COMMENT,NL,INDENT,NAME,
OP,NAME,OP,NAME,OP,STRING,OP
########################
if vhost.ssl: 
points += 3 

if points > <UNKNOWN> :
NL,DEDENT,NAME,NAME,OP,NAME,OP,NEWLINE,
INDENT,NAME,OP,NUMBER,NEWLINE,NL,DEDENT,
NAME,NAME,OP,NAME,OP
\end{lstlisting}


\paragraph{Summary}
\input{tables/rq2_best_per_model.tex}
 \Cref{tab:rq2_best_classifier} shows the performance score of the best performing model of each classifier for all problem types.
 The SVM performs poorly and does not meet our mandatory satisficing condition of precision higher than 0.8. Therefore, we will not evaluate it further. On the other hand, the LSTM-based neural network model outperforms all other models on this classification task for all problem types.

 Both random forest and gradient boosting classifier, fulfil the satisficing condition for all problem types. Interestingly, they supplement each other: For the RN problem type, the gradient boosting classifier performs better with an f1 score of 0.9328, whereas the random forest classifier surpasses the gradient boosting classifier for the CCS and CC problem type with 0.9352 and 0.9351 f1. As discussed, we explain the former result with the better insensitivity to a class imbalance of the gradient boosting classifier. In contrast, the latter seems to be harder to learn for the gradient boosting classifier. Especially the difficulties for the CCS type are unexpected and do not affect the random forest classifier or neural network.

\subsection{RQ3: Do Machine Learning-Based Models Cover a Larger Variety of Cases Than Rule-Based Checker? }\label{rq:3}

\paragraph{Motivation}
For the previous research question, we trained models on a dataset. We created the dataset by downloading open-source repositories and applying our analysis plugins from \Cref{sec:analysis_plugins} to label samples as clean or non-clean code. With this automated approach, it is simple to generate a dataset and to scale the dataset to any size needed for training. Such algorithms (checkers) that detect non-clean code are the prerequisite for the dataset and successful training of machine learning models. This research question wants to put the machine learning approach into a scenario, in which this prerequisite is not met and we lack a checker to generate a labelled dataset. 
This scenario transfers to clean code rules with a high level of complexity (as defined in ~\Cref{sec:cc_complexity_levels}). For these rules, we may not be able to analyse the source code or another representation with a checker to generate a representative dataset. We could (1) hand-label samples and train a machine learning model on those samples. Alternatively, we may (2) only find a deterministic checker that detects a subset of the rule violations. We hope that model trained on either of those datasets generalise well to all patterns of the rule violation. Both scenarios boil down to a lack of samples for all or many structural variations of the rule violation and a discrepancy of the dataset and the real world with more variety. We will evaluate both scenarios for our models from the previous experiment in this research question.


\paragraph{Approach}
To answer this research question, we use all models except the support vector classifier from the previous research question for all three problem types: 
We use the models trained on the RN problem type to simulate scenarios (1). First, we manipulate the holdout dataset as described in \Cref{sec:approach_code_manipulation}. The code still contains returns of \texttt{None}, although it is not explicitly written as two consecutive code tokens. We want to explore if a model generalised well enough to detect unseen variations of the problem type. With the manipulation, we ensure that all samples contain the unseen variations and our performance metrics only refer to those samples. 

Second, we simulate scenario (2) of having trained on a subset of all variations with the CCS models. The CCS model was trained on a subset of the CC problem patterns. We manipulate all occurrences of the CC problem type to contain only patterns that are not in the subset for the CCS training. It is important to note that the class frequency of positive samples changes from the trained 2.51\% to the 5.1\% positive samples for the manipulated CC type in the holdout set.

Last, we evaluate the models trained on the CC type with our manipulated code to provide a baseline for the CCS trained models. We expect the CC model to show similar performance than in the previous experiments since it was trained on the problem structure of the manipulated CC dataset.

We expect the best performing CC model from the previous experiments to outperform all CCS models and to show similar performance like in the previous research question. 

\paragraph{Results}
We evaluate the models' performance for both scenarios: The RN problem type simulates scenario (1) whereas the CCS problem type represents scenario (2). We list all evaluation results in the appendix in \Crefrange{tab:rq3_random_forest}{tab:rq3_lstm}.

\subparagraph{Finding 1}\label{finding:return_none_manipulated_bad}
\textit{The detection of the manipulated RN problem type works poorly with all models.} The random forest classifier has an f1 score of less than 0.1, the gradient boosting classifier of 0.24 and the LSTM-based neural network of 0.39. The precision of all models is below 0.26 and would therefore not comply with the satisficing condition from the previous experiment of 0.8. Our simulation for scenario (1) fails.

We think the model did not learn the problem structure during training. Since all models perform significantly worse than in the previous experiment, it is feasible to search for the reason in the problem type. Our reasoning goes as follows:
The problem type appears in the training set only in one variation: A \texttt{return} keyword is followed by a subsequent \texttt{None} value. The surrounding tokens varied, but only the subsequent occurrence of these two tokens had to be learnt to perform to detect the problem type. For the previous experiment, this behaviour resulted in the best performance. To perform well on this experiment, the model should have learned a more sophisticated approach of a \texttt{None} value after the \texttt{return} keyword before the end of the line. Since this was not part of the training data, it is obvious that the model learnt the simple subsequent detection of this pattern, since it yields the best performance in the previous experiment.

\subparagraph{Finding 2}\label{finding:ccs_more_variety}
\textit{The classification of the modified CC samples with the CCS models yields better results than for the RN type.}
For the random forest classifier, the performance on the modified CC dataset is at 0.72 f1 score. The best gradient boosting classifier in this experiment achieves 0.83 and the best LSTM-based neural network reaches 0.6568 f1 score. 
Compared to the performance for the RN types, the performance values are at least double as high, so the model has learnt the pattern from the subset better. We attribute this to the wider variety in problem pattern in the source code: Since the CCS checker looks for comparison in the condition, it will label different comparison operators such \texttt{==}, \texttt{<} or \texttt{>} as rule violations, whereas the only variation for the RN type is \texttt{return None} as subsequent tokens.

\subparagraph{Finding 3}\label{finding:better_vs_worse}
\textit{The more fine-tuning during training, the less the model generalises for the modified problems.}

All models on the modified problem types behave contrary to the unmodified problem types in the previous experiments. The hyperparameter and resampling combinations with a worse performance in the previous experiment score better in this evaluation. By contrast, the best model configurations from the previous experiment are among the worst-performing models for this evaluation. In \Cref{fig:rq3_performance_comparison}, we plotted the f1 performance of the baseline, manipulated RN and CCS. For each classifier, we sorted the models with different resampling and hyperparamter configurations descending on the baseline f1 score. The baseline performance is the performance of the CC model from the previous experiment on the manipualted CC data. It correlates with the performance of the CC model on the original holdout data in the previous experiment. The overall picture shows, that the models with the best baseline performance have a low performance on the manipualted datasets. With decreasing baseline performance, the performance of the manipualted CCS increases for the random forest and gradient boosting classifier. The manipualted RN type follows this pattern for the random forest classifier and the LSTM. 


\begin{figure}[h]
    \begin{center}
        \input{diagrams/rq3_performance_comparison.pgf}
    \end{center}
    \caption[F1 performance of the models, sorted by the baseline f1 score.]{F1 performance of the models, sorted by the baseline f1 score. The baseline performance correlates with the performance of the CC type in the previous experiment. With a worse performance in the previous experiment, the performance for the manipualted types increases.}
    \label{fig:rq3_performance_comparison}
\end{figure}

We observe this behaviour on different hyperparameters and resampling combinations in the previous experiment. It applies to both problem types.
For the random forest classifier, undersampling the training set lead inevitably to worse performance in the previous experiment than without undersampling. In this experiment, the random forest classifier which was trained on the undersampled dataset leads to the overall best scores. For gradient boosting classifier, the models without resampling scored best in the previous experiment and are performing worst in this experiment. Additionally, increases in boosting stages that lead to an improvement in the previous experiment now lead to a deterioration in performance. 
The LSTM-based neural network results draw a similar picture. The undersampling harmed the performance, whereas, in this experiment, a 0.5 undersampling achieves the only f1 score over zero for the RN type and the best for the manipulated CC type.

We would explain this observation separate for the RN and CCS type:
For the RN type, the models probably only learnt the direct token sequence (as described in \hyperref[finding:return_none_manipulated_bad]{Finding 1}). The better it learnt this sequence, the better its performance on the previous experiment. For this experiment, learning the direct token sequence would result in a low performance. Instead, the model would have to learn the structure of the problem. 

We analysed the false-positive samples of the random forest classifier that performed best in this experiment and its false-positives in the previous experiment. It failed in the previous experiment, since it used the \texttt{return} token as indicator to label samples positive in 19 out of 20 samples. The \texttt{None} token was not important, since it was only in 4 out of the 20 samples. This resulted in a low precision of 0.158. We see the same result with the manipulated RN data with 19 out of 20 samples containing a \texttt{return} token and 3 out of 20 samples containing a \texttt{None} token. For the best gradient boosting classifier in this experiment, 16 out of 20 false positive samples contain a \texttt{return} token and 14 out of 20 for the best LSTM in this experiment.
TODO maybe move to the finding for the RN



The CCS problem type, simulating scenario (2), offered more variety to learn from, as discussed. If the model learnt a general problem structure from this variety, fine-tuning the hyperparameters for the previous experiment would restrict this generality to only CCS patterns. We assumed in \Cref{finding:rq2_gbc} that the gradient boosting classifier get restricted by the CCS type and is more likely to learn the CC pattern. In this experiment, the best gradient boosting classifier configuration surpasses all other models. Additionally, it surpasses the baseline performance measure of the model trained on the unmodified CC type that we assumed to perform better. This solidifies our assumptions about the learning of the CCS type. We think this explanation applies to other models as well(TODO false positives from previous experiment anschauen und sehen ob da CC type die indikatoren waren, data available).

The explanations differ for both scenarios, but their common ground is an insufficient amount of variety. While the RN type has only one variety in the problem pattern, the CCS rules restricted the variety to the subset.

\paragraph{Improvements}
As identified in the findings, we see the root cause for the bad performance in the learning of the specific code pattern and not the general structure of a problem type. We identified the lack of variety as the main reason, manifesting as the restriction of the learning on the CCS type. For the RN type, the general structure on a token level would be something like a \texttt{None} value somewhere after a \texttt{return} but before a line break. A slightly more advanced structure could be extracted from the CCS type: A comparison inside a condition (starting with an \texttt{if} statement and closing with the colon) should be marked. With the chosen approach, the model did not train on the general problem structure of the clean code violation but explicit code patterns. 

We see several ways to improve:
First, we did not provide the model with the full variety of problem patterns. Therefore, we suggest hand-label or hand-generated patterns with explicit variations.
In the motivation for this research question, we state that it may not be practically possible to collect many samples with the full variety of problem patterns for an adequate training corpus. Instead, it may be possible to collect samples with enough variety as an incentive to learn the pattern. For the improvement, we need a base collection of samples. This base collection can be obtained by an algorithm that can only detect a subset of the problem patterns (like in our approach with the CCS type) or a similar enough code pattern (like we tried with the RN type). On top of that, we explicitly create samples with more variety by either hand-labelling existing code or writing new code. With the hand-crafted samples, we fine-tune the model to increase the variety of code patterns the model can generalise on. Simultaneously, we loosen the indirect restriction of the dataset covering only a subset of the data. By combining a scaleable, automatic generated dataset with hand-labelled samples, we may be able to improve the models' performance on the samples that we could not label automatically. Additionally, we have to use some hand-labelled samples for more variety in the test corpus to tune the hyperparameters to more possible problem variations in real datasets.

Second, we may compensate for less variety in training samples with a different encoding. Using the encoding to focus on the code structure may require fewer variations to extract the problematic code structure. We use an indexed-based encoding of the source codes token stream. With this approach, we do not exploit the code structure in our encoding. Other machine learning tasks on code profit from other encoding types that exploit more structure from the code. Alon et al. introduced code2vec to represent code as paths in the AST~\cite{alon_code2vec_2018}, capturing the structure of the code. Similiarly, Zhang et al. split the AST into smaller statement trees and apply a recurrent neural network approach to generate a code representation~\cite{zhang_novel_2019}.
Since we identified the lack of learning the problem structure from the samples, an encoding approaches that explicitly take advantage of the structure of code may improve the performance of this experiment. It may be even possible to train the model on a subset of possible problem variations like we tried with the manipulated CCS type without the restricting effect, since the problem structure may be inferred from the subset alone.
Additionally, this may be combined with the fine-tuning on hand-labelled samples to increase the performance to a practical level. The satisficing condition of precision higher than 0.8 we defined in the previous experiment would also apply to this experiment. 
If the precision would be too low, the Cry Wolf Effect would come into effect and the practical use would therefore be small.

\paragraph{Summary}
In summary, the detection of the modified RN type performs poorly with all models and configurations. We see the reason in the low variety of different problem patterns. The experiment with the RN type should simulate the scenario in which we could only obtain a dataset with hand-labelled samples that do not cover all possible patterns. In hindsight, having only one sample variation for the problem pattern simulates the worst possible scenario. Based on the analysis of the results, we admit that a hand-labelled sample set would probably contain a greater variety of samples. With more variety, the result would most likely be more comparable to the performance of the CCS type with more variation. If we define the subset of the CCS type as a hand-labelled set, we can interpret the results for this problem type in the context of scenario (1). 

The performance on the modified CC type is more promising than for the RN type. We explain this by more variety in the training samples, although the models seem to be restricted to this subset (especially the gradient boosting classifier). If we interpret the performance with the CCS type model on the modified CC type data as scenario (1), the variations of our hand-labelled samples will restrict the models' performance. Furthermore, it would lead to a misleading parameter tuning, since our validation set would not contain all real-world variations.

For improvement, we suggest to hand-craft additional samples with a different variation of the problem pattern. This can be used for increasing the variety of the training set with fine-tuning or to better approximate the real world in the validation set to tune the models' parameter towards real-life datasets. Additionally, encoding the data by utilising the source code structure, less variety may be necessary to learn the variations. If this is the case, it would have positive practical implications, since it would require less manual code labelling or creating.



