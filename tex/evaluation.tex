\section{Research Questions}

\begin{description}
    \item[RQ1] Can the Clean Code Analysis Platform be a useful addition to developers workflow besides existing tools? 
    \item[RQ2] What models perform well on detecting non-clean code
\end{description}

\subsection{RQ1: Can the Clean Code Analysis Platform be a useful addition to developers workflow besides existing tools?}
\paragraph{Motivation}
Several tools are established that aim to improve code quality and detect unclean code. Every new tool with similar claims has to prove its usefulness beside the preexisting tools. We compare the CCAP with existing tools based on the design goals expandability, useability and integration. Additionally, we will mention and compare useful features unique to existing tools.
\paragraph{Approach}
To evaluate, if the CCAP is a useful addition, we compare different features to see, if it can supplement or replace existing tools. As described in section TODO, the compared tools are Sonarcube, PMD Source Code Analyser Project, Codacy and PyLint.

\paragraph{Results}
Table TODO shows a summary of the features.

\subparagraph{Finding 1: Expandability}
For expandability of analysis plugins, CCAP is comparable to PyLint, that also offer plugins to analyse the raw string, the token stream or the AST. PMD allows plugins to analyse the AST or define XPath rules. Since XPath rules can be used in the CCAP as well (using a third-party library in the plugin), PMD offers fewer expansion possibilities. Sonarqube, on the other hand, provides the most possibilities for extension. Not only can developers analyse the AST, specify XPath rules, but they have access to an additional semantic model of the code that provides direct access to structures such as methods, parameters, return values. This semantic model simplifies analysis in comparison to the AST analysis.
Additionally, Sonarqube allows plugins to expose an API for other plugins to use. This different type of plugin further increases the possibilities for rule checking plugins. The least expandability offers Codacy, that allow customising or disabling existing rules, but does not offer to add rules to the existing ruleset.

Regarding output plugins, no compared tool offers the output customisation of CCAP. PMD and PyLint have different predefined output formats like JSON, HTML, CSV or text. Although PyLint allows customising the message format using a formatting string as a command-line argument, they do not offer extensions for the output. Sonarqube displays the analysis reports in its WebUI. The scanner tool, SonarScanner, sends the reports to the server component, that renders the results in the browser.  Codacy's local scanner produces text output; the cloud version also has a WebUI.

\subparagraph{Finding 2: Integration shortcomings}
The CCAP has significant shortcomings in its integrations into IDEs, build processes and CI pipelines. All contestant tools provide plugins for build systems like Gradle or Maven. Except for Codacy, all tools have IDE integrations into the most common editors. Sonarqube and Codacy have integrations into source control systems. Same applies to PMD and PyLint since they are included in Codacy and therefore have the same integrations.

\subparagraph{Finding 3: Useability}
For a plugin developer, the useability of the plugin system is simple for CCAP and PyLint. Both tools have the same plugin mechanism and a simple plugin interface to work with. PMD has a similar, straightforward plugin interface, but its Java plugin has to be bundled into a jar file before adding to the classpath. The latter applies to Sonarqube as well; additionally, the powerful and rich plugin API makes it harder to use.

For the user, installing CCAP is like installing every other python package. Running from the command-line is fast and can be automated, e.g. using git pre-commit hooks. The same workflow would be achieved with PyLint. Sonarqube is more complex to install due to its multiple components, but after the setup, the WebUI and integrations into most developer workflow have the best useability. Codacy as a cloud service only requires permission to access the source code repository to start scanning the code.

\subparagraph{Finding 4: Multi-Language scanning}
Sonarqube, PMD and Codacy offer multi-language scanning. This multi-language ability is advantageous for code repositories with multiple languages and for reusing the same tools and infrastructure for multiple projects with different programming languages. Supporting multi-language scanning is not possible with CCAP and would require a redefinition of the architecture. Although multi-language support was not a design goal, it would be a significant advantage for adoption.

\subparagraph{Finding 5: Maturity and Community-Support}
The maturity and community-support of tools impact the integration of the tools into the developer workflow. Some integrations are community-made and shared, so everybody has an advantage. The community support would also be necessary for CCAP to write and share additional analysis plugins and to integrate into different workflows.

\paragraph{Summary}
In summary, CCAP offers a good, but not best in class expandability with analysis plugins. It has a high useability for plugin developer and users. The lack of integrations for different workflows reflects the lack of community support and maturity. 
We see the CCAP as an addition to the powerful Sonarqube. The simple, yet robust expandability of the CCAP could supplement the shortcomings of Sonarqube in its powerful, but complex expandability. For instance, a code reviewer could code problematic code into a simple python plugin, distributes it to the team and would hopefully not reencounter the same problem.
Additionally, the CCAP could be used to teach about Clean Code and to enforce specific coding rules to students, since the local setup is straightforward. Clean Code rules could be turned into analysis plugins by students or teachers without having to understand a complicated interface.

\subsection{RQ2: What models perform well on detecting non-clean code?}
\paragraph{Motivation}
As we have shown with the CCAP and the analysis plugins, it is possible to write an algorithm that can detect certain code patterns like non-clean code patterns. If we are able to write a well tested detection algorithm, we do not need error-prone machine learning models to detect code patterns. Nonetheless, if we can not design an algorithm that detect a desired code pattern reliablely, we may perform better using a machine learning model. Additionally, if we encode a subjective pattern, it may not be possible to design an algorithm to objectively detect such a pattern. As a solution, we could label code parts as our pattern and use machine learning approches to implicitly extract rules to detect such a pattern.

For this research question, we will evaluate different machine learning models on different code patterns and compare the results.

\paragraph{Approach}
For answering the research question, we will use the approach detailed in chapter \ref{chap:clean_code_classification}.  The dataset, consisting of 18 projects from GitHub, is prepared as described in chapter \ref{chap:clean_code_classification_dataset}. The data is split into 70\% train, 10\% train/dev and 20\% test data. We use the train dataset to train Random Forest, Gradient Boosting Classifier, Support Vector Machines and a LSTM-based neural network. Due to the class inbalance, we will additional train with an oversampled dataset (oversampling rate of 0.5) and an undersampled dataset (undersampling rate of 0.5). The train/dev set follows the same (modified) distribution as the train set but is not used during training. We use it at test time to be able to differentiate a generalisation problem from a data missmatch problem. The former is visible in a difference between the metrics for the train set and the train/dev set whereas the latter is seen in the difference between the metrics for the train/dev and test set.

Our evaluation metrics are recall, precision and the combined f1 score. Due to the data inbalance, accuracy is not a meaningful metric. A high recall means a high detection rate of non-clean code. The cost of a false negative prediction are potential costs in unclean code (maintainability, understandability). With a high precision, a detected non-clean code sample is likely to be a non-clean code sample and the system reports less false positives. The immediate cost of false positive predictions (resulting in a lower precision) are the extra developer time needed. Additionally, the cry wolf effect (TODO source) comes into play. In our case, a developer who has seen a false alarm will take subsequent alarms less serious. Additionally, a study has shown that a higher false alarm rate in advisory warning system in cars results in a lower, subjective evaluation of the system (TODO source Cooperative warning systems: The impact of false and unnecessary alarms on driversâ€™ compliance). A low subjective evaluation of developers would lead to a decrease in adoption rate which consequently lead to more unfixed problematic code. Since both, recall and precision are important to the success of our models, we decided to use the equally weighted f1 score as single-value evaluation metric.
Additionally, to compensate the cascading effect of a low precision, we require a precision of 0.8 as a satisficing metric. This means, we accept all models that return two false positive and eight true positive samples. On the other hand, we define the recall as our optimizing metric.

We evaluate all models for the three different problem types RETURN\_NONE, CONDITION\_COMPARISON\_SIMPLE and CONDITION\_COMPARISON.

\paragraph{Results}
Table TODO shows the best performing hyperparameter per model for all three problem types. Figure TODO shows the confusion matrix those model configurations. We list all hyperparameter configurations and the corresponding traing, train/test and test pefromance for all problem types in the appendix (Table \ref{tab:all_results_random_forest}-\ref{tab:all_results_svm}).


\input{tables/rq2_best_per_model.tex}

\subparagraph{Finding 1: poor SVM performance}
SVM performed poorly. Based on the f1 score, the best configuration has a f1 of 0.0066, a recall of 0.0134 and a precision of 0.0044. In other words, only 1.34\% of positive samples are recognized as positive. The model seems to fail to seperate the two classes. A reason may be the class inbalance, since only 0.21\% of all samples of are non-clean code (positive) with the RETURN\_NONE type. In this configuration, we used undersampling with a ratio of 0.5, since oversampling was not feasable due to the increase in train time (see section \ref{sec:svm_quadratic_complexity}). After undersampling, 1/3 of all samples are therefore positive and 2/3 are negative. Since the time to train is still too long, we only took half of the undersampled dataset. 

We additionally used the svm in a configuration with an automatic class weigthing that is inverse-proportional to the class frequency. The recall score increases by 57-fold to 0.7660 wheras the precision decrease by half to 0.0026. We conclude, that the svm is very sensitive to class inbalance, which is an improper characteristic for our inbalanced classification problem. Additionally, a precision of 0.0026 is not useable for any application. We can not boost the precision with more data to help the model to distinguish true positives and false positives, since the train time will increase quadratically. We a baseline that low we do not expect much improvement with further investigation and therefore conclude that there is no valid reason to further invest in the svm model.


\subparagraph{Finding 2: Random Forest}
Generally, the random forest classifier performs with a f1 score over 0.8 over all problem types with an oversampling ratio of 0.5, 100 trees, no additional class weighting and type encoding. For RN, this configuration has a 0.83 f1, 0.72 recall and 0.99 precision. For CCS and CC, the f1 with 0.94 are even better. The recall for those problem types increases to 0.9 while the precision remains high with 0.98 and 0.97 respectively. 

A common pattern is a worse performance on undersampled train sets. We observe a significant drop in performance for the RN problem type, if we use a 0.5 undersampling ratio. Similiarly, the performance for CCS and CC drops, although the decrease is smaller. Furthermore, we observe a moderate performance drop with 0.1 undersampling. From this pattern, we infer that a random forest classifier can profit from more data, even if they contain duplicated samples for the minority class due to oversampling. This can also explain the larger drop for undersampling for the RN problem type, since the non-clean code class contains less samples for RN than for CCS and CC (TODO numders of samples in train dataset). With less samples, the undersampling process reduces the overall amount of samples by deleting samples from the majority class. With a undersampling ratio of 0.1, less examples are deleted are deleted and the effect is less impactful, but observable.

The impact of class weighting in comparison to equivalent configurations seems to be neglegtable. For oversampled datasets, the class weighting seems to slightly lower the performance whereas the opposite applies to undersampling. The former may be explained by an overemphasise effect on the data. With oversampling, we change the the data distribution by duplicating the minority class and additionally emphasize the minority class with balancing. For the original dataset without under- or oversamping, class balancing does not have a significant impact on the performance for type encoding. For random forests without type encoding, we observe a slight 4 percentage points decrease in f1 performance. 

With type encoding enabled, we see an improvement up to two percentage point in recall for all problem types with a comparable precision. Since the improvement is low and we observe a deterioration for undersampling, we can not infer a overall positive impact of type encoding on random forest. This requires additional investigations.

Overall, it seems like a random forest classifier can handle class inbalance fairly well, as long as distribution is not too unbalanced as for RN problem type. The performance deteriotes if the inbalance superceeds a certain threshold (for RN), otherwise it has minor impact.
We base this conclusion on several observations: First, we see a better performance for the CCS and CC problem type overall, both of which have a more blanced distribution as the RN problem type. It is important to note, that the different problem types also have different difficulty levels for the model. But we assume the RN type is easier to detect for a model, since the model only has to learn two subsequent tokens anywhere in the sequence. Therefore, we attribute this effect to the class distribution, but we remark that this may be tested in an additional, isolated experiment. Second, without over- or undersampling, we observe similar performance for CCS and CC, despite their different frequency in the train set (TODO: class frequency values). 
Finally, we expect to have a bigger impact of class balancing, if the random forest classifier would be sensitive to class inbalance.
