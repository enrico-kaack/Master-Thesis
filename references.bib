
@article{alon_structural_2020,
	title = {Structural Language Models of Code},
	url = {http://arxiv.org/abs/1910.00577},
	abstract = {We address the problem of any-code completion - generating a missing piece of source code in a given program without any restriction on the vocabulary or structure. We introduce a new approach to any-code completion that leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling ({SLM}). {SLM} estimates the probability of the program's abstract syntax tree ({AST}) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all {AST} paths leading to a target node. Unlike previous techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary code in any programming language. Our model significantly outperforms both seq2seq and a variety of structured approaches in generating Java and C\# code. We make our code, datasets, and models publicly available.},
	author = {Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
	urldate = {2020-02-28},
	date = {2020-02-07},
	eprinttype = {arxiv},
	eprint = {1910.00577},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning}
}

@inproceedings{asaduzzaman_context-sensitive_2014,
	title = {Context-Sensitive Code Completion Tool for Better {API} Usability},
	doi = {10.1109/ICSME.2014.110},
	abstract = {Developers depend on {APIs} of frameworks and libraries to support the development process. Due to the large number of existing {APIs}, it is difficult to learn, remember, and use them during the development of a software. To mitigate the problem, modern integrated development environments provide code completion facilities that free developers from remembering every detail. In this paper, we introduce {CSCC}, a simple, efficient context-sensitive code completion tool that leverages previous code examples to support method completion. Compared to other existing code completion tools, {CSCC} uses new sources of contextual information together with lightweight source code analysis to better recommend {API} method calls.},
	eventtitle = {2014 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	pages = {621--624},
	booktitle = {2014 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME})},
	author = {Asaduzzaman, M. and Roy, C.K. and Schneider, K.A. and Hou, Daqing},
	date = {2014-09},
	keywords = {\_tablet, Java, Databases, application program interfaces, Context modeling, source code (software), libraries, code completion, Context, {API} methods, {API} usability, context-sensitive code completion tool, {CSCC}, Eclipse plugin, lightweight source code analysis, Proposals, Receivers}
}

@article{bhoopchand_learning_2016,
	title = {Learning Python Code Suggestion with a Sparse Pointer Network},
	url = {http://arxiv.org/abs/1611.08307},
	abstract = {To enhance developer productivity, all modern integrated development environments ({IDEs}) include code suggestion functionality that proposes likely next tokens at the cursor. While current {IDEs} work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern {IDEs} do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from {GitHub}. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an {LSTM} baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.},
	author = {Bhoopchand, Avishkar and Rocktäschel, Tim and Barr, Earl and Riedel, Sebastian},
	urldate = {2019-12-03},
	date = {2016-11-24},
	eprinttype = {arxiv},
	eprint = {1611.08307},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Software Engineering}
}

@inproceedings{bielik_phog_2016,
	title = {{PHOG}: Probabilistic Model for Code},
	url = {http://dl.acm.org/citation.cfm?id=3045390.3045699},
	series = {{ICML}'16},
	shorttitle = {{PHOG}},
	abstract = {We introduce a new generative model for code called probabilistic higher order grammar ({PHOG}). {PHOG} generalizes probabilistic context free grammars ({PCFGs}) by allowing conditioning of a production rule beyond the parent non-terminal, thus capturing rich contexts relevant to programs. Even though {PHOG} is more powerful than a {PCFG}, it can be learned from data just as efficiently. We trained a {PHOG} model on a large {JavaScript} code corpus and show that it is more precise than existing models, while similarly fast. As a result, {PHOG} can immediately benefit existing programming tools based on probabilistic models of code.},
	pages = {2933--2942},
	booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	publisher = {{JMLR}.org},
	author = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin},
	urldate = {2019-11-09},
	date = {2016}
}

@article{bobadilla_collaborative_2012,
	title = {A Collaborative Filtering Approach to Mitigate the New User Cold Start Problem},
	volume = {26},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705111001882},
	doi = {10.1016/j.knosys.2011.07.021},
	abstract = {The new user cold start issue represents a serious problem in recommender systems as it can lead to the loss of new users who decide to stop using the system due to the lack of accuracy in the recommendations received in that first stage in which they have not yet cast a significant number of votes with which to feed the recommender system’s collaborative filtering core. For this reason it is particularly important to design new similarity metrics which provide greater precision in the results offered to users who have cast few votes. This paper presents a new similarity measure perfected using optimization based on neural learning, which exceeds the best results obtained with current metrics. The metric has been tested on the Netflix and Movielens databases, obtaining important improvements in the measures of accuracy, precision and recall when applied to new user cold start situations. The paper includes the mathematical formalization describing how to obtain the main quality measures of a recommender system using leave-one-out cross validation.},
	pages = {225--238},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Bobadilla, Jesús and Ortega, Fernando and Hernando, Antonio and Bernal, Jesús},
	urldate = {2016-02-08},
	date = {2012-02},
	keywords = {collaborative filtering, recommender systems, Cold start, Leave-one-out-cross validation, Neural learning, Similarity measures}
}

@article{brockschmidt_generative_2018,
	title = {Generative Code Modeling with Graphs},
	url = {http://arxiv.org/abs/1805.08490},
	abstract = {Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. The generative procedure interleaves grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.},
	author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L. and Polozov, Oleksandr},
	urldate = {2018-07-12},
	date = {2018-05-22},
	eprinttype = {arxiv},
	eprint = {1805.08490},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning}
}

@inproceedings{bruch_learning_2009,
	title = {Learning from Examples to Improve Code Completion Systems},
	isbn = {978-1-60558-001-2},
	url = {http://doi.acm.org/10.1145/1595696.1595728},
	doi = {10.1145/1595696.1595728},
	series = {{ESEC}/{FSE} '09},
	abstract = {The suggestions made by current {IDE}'s code completion features are based exclusively on static type system of the programming language. As a result, often proposals are made which are irrelevant for a particular working context. Also, these suggestions are ordered alphabetically rather than by their relevance in a particular context. In this paper, we present intelligent code completion systems that learn from existing code repositories. We have implemented three such systems, each using the information contained in repositories in a different way. We perform a large-scale quantitative evaluation of these systems, integrate the best performing one into Eclipse, and evaluate the latter also by a user study. Our experiments give evidence that intelligent code completion systems which learn from examples significantly outperform mainstream code completion systems in terms of the relevance of their suggestions and thus have the potential to enhance developers' productivity.},
	pages = {213--222},
	booktitle = {Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the {ACM} {SIGSOFT} Symposium on The Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Bruch, Marcel and Monperrus, Martin and Mezini, Mira},
	urldate = {2015-12-02},
	date = {2009},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, code completion, code recommender, content assist, integrated development environment}
}

@article{eyal_salman_identification_2017,
	title = {Identification Multi-Level Frequent Usage Patterns from {APIs}},
	volume = {130},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121217300869},
	doi = {10.1016/j.jss.2017.05.039},
	abstract = {Software developers increasingly rely on application programming interfaces ({APIs}) of frameworks to increase productivity. An {API} method is generally used within code snippets along with other methods of the {API} of interest. When developers invoke {API} methods in a framework, they often encounter difficulty to determine which methods to call due to the huge number of included methods in that {API}. Developers usually exploit a source code search tool searching for code snippets that use the {API} methods of interest. However, the number of returned code snippets is very large which hinders the developer to locate useful ones. Moreover, co-usage relationships between {API} methods are often not documented. This article presents an approach to identify multi-level frequent usage patterns ({IML}-{FUP}) to help developers understand {API} usage and facilitate the development tasks when they use new {APIs}. An identified pattern represents a set of {API} methods that are frequently called together across interfering usage scenarios. In order to investigate the efficiency of the proposed approach, an experimental evaluation is conducted using four {APIs} and 89 client programs. For all studied {APIs}, the experimental results show that the proposed approach identifies usage patterns that are always strongly cohesive and highly consistent.},
	pages = {42--56},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Eyal Salman, Hamzeh},
	urldate = {2019-03-24},
	date = {2017-08-01},
	keywords = {{API} usage, {API} documentation, Formal concept analysis, Identification, Usage patterns}
}

@article{hidasi_session-based_2015,
	title = {Session-Based Recommendations with Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1511.06939},
	abstract = {We apply recurrent neural networks ({RNN}) on a new domain, namely recommendation system. Real-life recommender systems often face the problem of having to base recommendations only on short session-based data (e.g. a small sportsware website) instead of long user histories (as in the case of Netflix). In this situation the frequently praised matrix factorization approaches are not accurate. This problem is usually overcome in practice by resorting to item-to-item recommendations, i.e. recommending similar items. We argue that by modeling the whole session, more accurate recommendations can be provided. We therefore propose an {RNN}-based approach for session-based recommendations. Our approach also considers practical aspects of the task and introduces several modifications to classic {RNNs} such as a ranking loss function that make it more viable for this specific problem. Experimental results on two data-sets show marked improvements over widely used approaches.},
	author = {Hidasi, Balázs and Karatzoglou, Alexandros and Baltrunas, Linas and Tikk, Domonkos},
	urldate = {2016-02-10},
	date = {2015-11-21},
	eprinttype = {arxiv},
	eprint = {1511.06939},
	keywords = {\_tablet, Computer Science - Neural and Evolutionary Computing, Computer Science - Learning, Computer Science - Information Retrieval}
}

@inproceedings{hindle_naturalness_2012,
	title = {On the Naturalness of Software},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337322},
	series = {{ICSE} '12},
	abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations—and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's completion capability. We conclude the paper by laying out a vision for future research in this area.},
	pages = {837--847},
	booktitle = {Proceedings of the 34th International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
	urldate = {2015-12-17},
	date = {2012},
	note = {event-place: Piscataway, {NJ}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{holmes_using_2005,
	title = {Using Structural Context to Recommend Source Code Examples},
	isbn = {1-58113-963-2},
	url = {http://doi.acm.org/10.1145/1062455.1062491},
	doi = {10.1145/1062455.1062491},
	series = {{ICSE} '05},
	abstract = {When coding to a framework, developers often become stuck, unsure of which class to subclass, which objects to instantiate and which methods to call. Example code that demonstrates the use of the framework can help developers make progress on their task. In this paper, we describe an approach for locating relevant code in an example repository that is based on heuristically matching the structure of the code under development to the example code. Our tool improves on existing approaches in two ways. First, the structural context needed to query the repository is extracted automatically from the code, freeing the developer from learning a query language or from writing their code in a particular style. Second, the repository can be generated easily from existing applications. We demonstrate the utility of this approach by reporting on a case study involving two subjects completing four programming tasks within the Eclipse integrated development environment framework.},
	pages = {117--125},
	booktitle = {Proceedings of the 27th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Holmes, Reid and Murphy, Gail C.},
	urldate = {2015-12-07},
	date = {2005},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, development environment framework, examples, recommender, software structure}
}

@software{iedmrc_galois-autocompletergalois-autocompleter_2019,
	title = {Galois-Autocompleter/Galois-Autocompleter},
	url = {https://github.com/galois-autocompleter/galois-autocompleter},
	abstract = {Galois is an auto code completer for code editors (or any text editor) based on {OpenAI} {GPT}-2.},
	publisher = {Galois Autocompleter},
	author = {{iedmrc}},
	urldate = {2020-07-30},
	date = {2019-08},
	keywords = {nlp, code-completion, deep-learning, gpt-2}
}

@inproceedings{jacobellis_cookbook_2014,
	title = {Cookbook: In Situ Code Completion Using Edit Recipes Learned from Examples},
	url = {http://dl.acm.org/citation.cfm?id=2591076},
	shorttitle = {Cookbook},
	pages = {584--587},
	booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Jacobellis, John and Meng, Na and Kim, Miryung},
	urldate = {2016-06-24},
	date = {2014},
	keywords = {\_tablet}
}

@article{karampatsis_big_2020,
	title = {Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code},
	url = {http://arxiv.org/abs/2003.07914},
	doi = {10.1145/3377811.3380342},
	shorttitle = {Big Code != Big Vocabulary},
	abstract = {Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and {API} migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models ({NLMs}) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code {NLM} that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest {NLMs} for code that have been reported. All datasets, code, and trained models used in this work are publicly available.},
	author = {Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
	urldate = {2020-06-11},
	date = {2020-03-17},
	eprinttype = {arxiv},
	eprint = {2003.07914},
	keywords = {Computer Science - Software Engineering}
}

@article{kim_code_2020,
	title = {Code Prediction by Feeding Trees to Transformers},
	url = {http://arxiv.org/abs/2003.13848},
	abstract = {We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous state-of-the-art systems. With this, it outperforms the accuracy of an {RNN}-based system by 37.0{\textbackslash}textbackslash\%, the decision-tree based Deep3 system by 29.7{\textbackslash}textbackslash\%, and an adaptation of Code2Seq for code prediction by 30.0{\textbackslash}textbackslash\%. These are significant margins. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. The most effective of these is when we enrich the self-attention mechanism of the Transformer. We enable the mechanism to learn weights—that is, how much to focus on each preceding token in the input—not only on the basis of a token's value, but also on the basis of the spatial relationships, as in their positions in the abstract syntax tree, between each pair of tokens. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a company internal Python corpus. Our code and data preparation pipeline are available in open source.},
	author = {Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
	urldate = {2020-06-11},
	date = {2020-05-21},
	eprinttype = {arxiv},
	eprint = {2003.13848},
	keywords = {Computer Science - Software Engineering}
}

@inproceedings{lee_temporal_2013,
	title = {Temporal Code Completion and Navigation},
	isbn = {978-1-4673-3076-3},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486956},
	series = {{ICSE} '13},
	abstract = {Modern {IDEs} make many software engineering tasks easier by automating functionality such as code completion and navigation. However, this functionality operates on one version of the code at a time. We envision a new approach that makes code completion and navigation aware of code evolution and enables them to operate on multiple versions at a time, without having to manually switch across these versions. We illustrate our approach on several example scenarios. We also describe a prototype Eclipse plugin that embodies our approach for code completion and navigation for Java code. We believe our approach opens a new line of research that adds a novel, temporal dimension for treating code in {IDEs} in the context of tasks that previously required manual switching across different code versions.},
	pages = {1181--1184},
	booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Lee, Yun Young and Harwell, Sam and Khurshid, Sarfraz and Marinov, Darko},
	urldate = {2015-12-07},
	date = {2013},
	note = {event-place: Piscataway, {NJ}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{li_software_2017,
	title = {Software Defect Prediction via Convolutional Neural Network},
	doi = {10.1109/QRS.2017.42},
	abstract = {To improve software reliability, software defect prediction is utilized to assist developers in finding potential bugs and allocating their testing efforts. Traditional defect prediction studies mainly focus on designing hand-crafted features, which are input into machine learning classifiers to identify defective code. However, these hand-crafted features often fail to capture the semantic and structural information of programs. Such information is important in modeling program functionality and can lead to more accurate defect prediction. In this paper, we propose a framework called Defect Prediction via Convolutional Neural Network ({DP}-{CNN}), which leverages deep learning for effective feature generation. Specifically, based on the programs' Abstract Syntax Trees ({ASTs}), we first extract token vectors, which are then encoded as numerical vectors via mapping and word embedding. We feed the numerical vectors into Convolutional Neural Network to automatically learn semantic and structural features of programs. After that, we combine the learned features with traditional hand-crafted features, for accurate software defect prediction. We evaluate our method on seven open source projects in terms of F-measure in defect prediction. The experimental results show that in average, {DP}-{CNN} improves the state-of-the-art method by 12\%.},
	eventtitle = {2017 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	pages = {318--328},
	booktitle = {2017 {IEEE} International Conference on Software Quality, Reliability and Security ({QRS})},
	author = {Li, Jian and He, Pinjia and Zhu, Jieming and Lyu, Michael R.},
	date = {2017-07},
	note = {{ISSN}: null},
	keywords = {abstract syntax trees, deep learning, Semantics, Software, neural nets, {AST}, {CNN}, Convolutional codes, defect prediction via convolutional neural network, {DP}-{CNN}, Feature extraction, learning (artificial intelligence), Machine learning, machine learning classifiers, Neural networks, open source projects, software defect prediction, software reliability, Software reliability}
}

@inproceedings{li_code_2018,
	title = {Code Completion with Neural Attention and Pointer Networks},
	url = {https://doi.org/10.24963/ijcai.2018/578},
	doi = {10.24963/ijcai.2018/578},
	pages = {4159--4165},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI}-18},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Li, Jian and Wang, Yue and Lyu, Michael R. and King, Irwin},
	date = {2018-07}
}

@article{lika_facing_2014,
	title = {Facing the Cold Start Problem in Recommender Systems},
	volume = {41},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417413007240},
	doi = {10.1016/j.eswa.2013.09.005},
	abstract = {A recommender system ({RS}) aims to provide personalized recommendations to users for specific items (e.g., music, books). Popular techniques involve content-based ({CB}) models and collaborative filtering ({CF}) approaches. In this paper, we deal with a very important problem in {RSs}: The cold start problem. This problem is related to recommendations for novel users or new items. In case of new users, the system does not have information about their preferences in order to make recommendations. We propose a model where widely known classification algorithms in combination with similarity techniques and prediction mechanisms provide the necessary means for retrieving recommendations. The proposed approach incorporates classification methods in a pure {CF} system while the use of demographic data help for the identification of other users with similar behavior. Our experiments show the performance of the proposed system through a large number of experiments. We adopt the widely known dataset provided by the {GroupLens} research group. We reveal the advantages of the proposed solution by providing satisfactory numerical results in different experimental scenarios.},
	pages = {2065--2073},
	number = {4},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Lika, Blerina and Kolomvatsos, Kostas and Hadjiefthymiades, Stathes},
	urldate = {2016-02-08},
	date = {2014-03},
	keywords = {recommender systems, Cold start problem}
}

@inproceedings{liu_nomen_2016,
	title = {Nomen Est Omen: Exploring and Exploiting Similarities Between Argument and Parameter Names},
	isbn = {978-1-4503-3900-1},
	url = {http://doi.acm.org/10.1145/2884781.2884841},
	doi = {10.1145/2884781.2884841},
	series = {{ICSE} '16},
	shorttitle = {Nomen Est Omen},
	abstract = {Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We find that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to the corresponding parameter than any alternative argument available in the call site's scope. As applications of our findings, we present an anomaly detection technique that identifies 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83\%.},
	pages = {1063--1073},
	booktitle = {Proceedings of the 38th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Liu, Hui and Liu, Qiurong and Staicu, Cristian-Alexandru and Pradel, Michael and Luo, Yue},
	urldate = {2016-06-19},
	date = {2016},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, empirical study, identifier names, method arguments, name-based program analysis, static analysis}
}

@article{liu_neural_2017,
	title = {Neural Code Completion},
	abstract = {Code completion, an essential part of modern software development, yet can be challenging for dynamically typed programming languages. In this paper we explore the use of neural network techniques to automatically learn code completion from a large corpus of dynamically typed {JavaScript} code. We show different neural networks that leverage not only token level information but also structural information, and evaluate their performance on different prediction tasks. We demonstrate that our models can outperform the state-of-the-art approach, which is based on decision tree techniques, on both next non-terminal and next terminal prediction tasks by 3.8 points and 0.5 points respectively. We believe that neural network techniques can play a transformative role in helping software developers manage the growing complexity of software systems, and we see this work as a first step in that direction.},
	pages = {14},
	author = {Liu, Chang and Wang, Xin and Shin, Richard and Gonzalez, Joseph E and Song, Dawn},
	date = {2017}
}

@article{liu_self-attentional_2019,
	title = {A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning},
	volume = {abs/1909.06983},
	url = {https://arxiv.org/abs/1909.06983v1},
	abstract = {Code completion, one of the most useful features in the integrated development environments, can accelerate software development by suggesting the libraries, {APIs}, method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program's representation; b) In programs, the semantic relationships can be very long, existing {LSTM} based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. In this paper, we present a novel method that introduces the hierarchical structural information into the representation of programs by considering the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we apply Transformer-{XL} network as the base language model. Besides, we creatively propose a Multi-Task Learning ({MTL}) framework to learn two related tasks in code completion jointly, where knowledge acquired from one task could be beneficial to another task. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.},
	journaltitle = {{ArXiv}},
	author = {Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Li, Ming and Fu, Zhiyi and Jin, Zhi},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1909.06983},
	keywords = {Transformer, Emoticon, Experiment, Integrated development environment, Language model, Library (computing), Long short-term memory, Multi-task learning, Real-time clock, Real-time computing, Software development, Software repository, Tree (data structure)}
}

@article{liu_self-attentional_2020,
	title = {A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning},
	url = {http://arxiv.org/abs/1909.06983},
	abstract = {Code completion, one of the most useful features in the Integrated Development Environments ({IDEs}), can accelerate software development by suggesting the libraries, {APIs}, and method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program's representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. To address these challenges, in this paper, we propose a self-attentional neural architecture for code completion with multi-task learning. To utilize the hierarchical structural information of the programs, we present a novel method that considers the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable the knowledge sharing between related tasks, we creatively propose a Multi-Task Learning ({MTL}) framework to learn two related tasks in code completion jointly. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.},
	author = {Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Fu, Zhiyi and Jin, Zhi},
	urldate = {2020-07-01},
	date = {2020-06-26},
	eprinttype = {arxiv},
	eprint = {1909.06983},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering}
}

@inproceedings{lozano_mendel_2011,
	title = {Mendel: Source Code Recommendation Based on a Genetic Metaphor},
	isbn = {978-1-4577-1638-6},
	url = {http://dx.doi.org/10.1109/ASE.2011.6100078},
	doi = {10.1109/ASE.2011.6100078},
	series = {{ASE} '11},
	shorttitle = {Mendel},
	abstract = {When evolving software systems, developers spend a considerable amount of time understanding existing source code. To successfully implement new or alter existing behavior, developers need to answer questions such as: "Which types and methods can I use to solve this task?" or "Should my implementation follow particular naming or structural conventions?". In this paper we present Mendel, a source code recommendation tool that aids developers in answering such questions. Based on the entity the developer currently browses, the tool employs a genetics-inspired metaphor to analyze source-code entities related to the current working context and provides its user with a number of recommended properties (naming conventions, used types, invoked messages, etc.) that the source code entity currently being worked on should exhibit. An initial validation of Mendel seems to confirm the potential of our approach.},
	pages = {384--387},
	booktitle = {Proceedings of the 2011 26th {IEEE}/{ACM} International Conference on Automated Software Engineering},
	publisher = {{IEEE} Computer Society},
	author = {Lozano, Angela and Kellens, Andy and Mens, Kim},
	urldate = {2015-12-07},
	date = {2011},
	note = {event-place: Washington, {DC}, {USA}},
	keywords = {\_tablet}
}

@article{luan_aroma_2019,
	title = {Aroma: Code Recommendation via Structural Code Search},
	volume = {3},
	issn = {24751421},
	url = {http://arxiv.org/abs/1812.01158},
	doi = {10.1145/3360578},
	shorttitle = {Aroma},
	abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an {IDE} plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.},
	pages = {1--28},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
	urldate = {2019-11-09},
	date = {2019-10-10},
	eprinttype = {arxiv},
	eprint = {1812.01158},
	keywords = {Computer Science - Software Engineering}
}

@inproceedings{lv_apisynth_2014,
	title = {{APISynth}: A New Graph-Based {API} Recommender System},
	isbn = {978-1-4503-2768-8},
	url = {http://doi.acm.org/10.1145/2591062.2591133},
	doi = {10.1145/2591062.2591133},
	series = {{ICSE} Companion 2014},
	shorttitle = {{APISynth}},
	abstract = {Current {API} recommendation tools yield either good recall ratio or good precision, but not both. A tool named {APISynth} is proposed in this paper by utilizing a new graph based approach. Preliminary evaluation demonstrates that {APISynth} wins over the state of the art with respect to both the two criteria.},
	pages = {596--597},
	booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Lv, Chen and Jiang, Wei and Liu, Yue and Hu, Songlin},
	urldate = {2015-12-07},
	date = {2014},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, {API} Recommender, Code Assistant, code reuse}
}

@inproceedings{malheiros_source_2012,
	title = {A Source Code Recommender System to Support Newcomers},
	doi = {10.1109/COMPSAC.2012.11},
	abstract = {Newcomers in a software development project often need assistance to complete their first tasks. Then a mentor, an experienced member of the team, usually teaches the newcomers what they need to complete their tasks. But, to allocate an experienced member of a team to teach a newcomer during a long time is neither always possible nor desirable, because the mentor could be more helpful doing more important tasks. During the development the team interacts with a version control system, bug tracking and mailing lists, and all these tools record data creating the project memory. Recommender systems can use the project memory to help newcomers in some tasks answering their questions, thus in some cases the developers do not need a mentor. In this paper we present Mentor, a recommender system to help newcomers to solve change requests. Mentor uses the Prediction by Partial Matching ({PPM}) algorithm and some heuristics to analyze the change requests, and the version control data, and recommend potentially relevant source code that will help the developer in the change request solution. We did three experiments to compare the {PPM} algorithm with the Latent Semantic Indexing ({LSI}). Using {PPM} we achieved results for recall rate between 37\% and 66.8\%, and using {LSI} the results were between 20.3\% and 51.6\%.},
	eventtitle = {Computer Software and Applications Conference ({COMPSAC}), 2012 {IEEE} 36th Annual},
	pages = {19--24},
	booktitle = {Computer Software and Applications Conference ({COMPSAC}), 2012 {IEEE} 36th Annual},
	author = {Malheiros, Y. and Moraes, A. and Trindade, C. and Meira, S.},
	date = {2012-07},
	keywords = {\_tablet, Databases, recommender systems, Pattern matching, Software maintenance, Software, software engineering, bug tracking, configuration management, Context, Entropy, experienced team member, Indexing, information theory, Large scale integration, latent semantic indexing, {LSI}, mailing lists, Measurement, Mentor, newcomers, {PPM} algorithm, prediction by partial matching algorithm, program debugging, project memory, recommender system, software development project, source code recommender system, version control data, version control system}
}

@inproceedings{mkaouer_recommendation_2014,
	title = {Recommendation System for Software Refactoring Using Innovization and Interactive Dynamic Optimization},
	isbn = {978-1-4503-3013-8},
	url = {http://doi.acm.org/10.1145/2642937.2642965},
	doi = {10.1145/2642937.2642965},
	series = {{ASE} '14},
	abstract = {We propose a novel recommendation tool for software refactoring that dynamically adapts and suggests refactorings to developers interactively based on their feedback and introduced code changes. Our approach starts by finding upfront a set of non-dominated refactoring solutions using {NSGA}-{II} to improve software quality, reduce the number of refactorings and increase semantic coherence. The generated non-dominated refactoring solutions are analyzed using our innovization component to extract some interesting common features between them. Based on this analysis, the suggested refactorings are ranked and suggested to the developer one by one. The developer can approve, modify or reject each suggested refactoring, and this feedback is used to update the ranking of the suggested refactorings. After a number of introduced code changes, a local search is performed to update and adapt the set of refactoring solutions suggested by {NSGA}-{II}. We evaluated this tool on four large open source systems and one industrial project provided by our partner. Statistical analysis of our experiments over 31 runs shows that the dynamic refactoring approach performed significantly better than three other search-based refactoring techniques, manual refactorings, and one refactoring tool not based on heuristic search.},
	pages = {331--336},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy and Ó Cinnéide, Mel},
	urldate = {2015-12-05},
	date = {2014},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, refactoring, software quality, search based software engineering}
}

@inproceedings{nguyen_graph-based_2012,
	title = {Graph-Based Pattern-Oriented, Context-Sensitive Source Code Completion},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337232},
	series = {{ICSE} '12},
	abstract = {Code completion helps improve programming productivity. However, current support for code completion is limited to context-free code templates or a single method call of the variable on focus. Using libraries for development, developers often repeat {API} usages for certain tasks. Therefore, in this paper, we introduce {GraPacc}, a graph-based pattern-oriented, context-sensitive code completion approach that is based on a database of {API} usage patterns. {GraPacc} manages and represents the {API} usage patterns of multiple variables, methods, and control structures via graph-based models. It extracts the context-sensitive features from the code, e.g. the {API} elements on focus or under modification, and their relations to other elements. The features are used to search and rank the patterns that are most fitted with the current code. When a pattern is selected, the current code will be completed via our novel graph-based code completion algorithm. Empirical evaluation on several real-world systems and human subjects shows that {GraPacc} has a high level of accuracy and a better level of usefulness than existing tools.},
	pages = {69--79},
	booktitle = {Proceedings of the 34th International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Hoan Anh and Tamrawi, Ahmed and Nguyen, Hung Viet and Al-Kofahi, Jafar and Nguyen, Tien N.},
	urldate = {2015-12-04},
	date = {2012},
	note = {event-place: Piscataway, {NJ}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{nguyen_statistical_2013,
	title = {A Statistical Semantic Language Model for Source Code},
	isbn = {978-1-4503-2237-9},
	url = {http://doi.acm.org/10.1145/2491411.2491458},
	doi = {10.1145/2491411.2491458},
	series = {{ESEC}/{FSE} 2013},
	abstract = {Recent research has successfully applied the statistical n-gram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce {SLAMC}, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/functionality into an n-gram topic model, together with pairwise associations of program elements. Based on {SLAMC}, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18-68\% higher accuracy than the state-of-the-art approach.},
	pages = {532--542},
	booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
	urldate = {2015-12-08},
	date = {2013},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, code completion, Statistical Semantic Language Model}
}

@inproceedings{nguyen_recommending_2015,
	title = {Recommending {API} Usages for Mobile Apps with Hidden Markov Model},
	doi = {10.1109/ASE.2015.109},
	abstract = {Mobile apps often rely heavily on standard {API} frameworks and libraries. However, learning to use those {APIs} is often challenging due to the fast-changing nature of {API} frameworks and the insufficiency of documentation and code examples. This paper introduces {DroidAssist}, a recommendation tool for {API} usages of Android mobile apps. The core of {DroidAssist} is {HAPI}, a statistical, generative model of {API} usages based on Hidden Markov Model. With {HAPIs} trained from existing mobile apps, {DroidAssist} could perform code completion for method calls. It can also check existing call sequences to detect and repair suspicious (i.e. unpopular) {API} usages.},
	eventtitle = {2015 30th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {795--800},
	booktitle = {2015 30th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Nguyen, T. T. and Pham, H. V. and Vu, P. M. and Nguyen, T. T.},
	date = {2015-11},
	keywords = {recommender systems, Data mining, application program interfaces, code completion, Maintenance engineering, Android mobile applications, Androids, {API} usage, {API} usage recommendation, call sequences, Documentation, {DroidAssist} recommendation tool, {HAPI}, hidden Markov model, hidden Markov models, Hidden Markov models, Humanoid robots, method calls, Mobile communication, mobile computing, statistical analysis, Statistical code completion, statistical generative model, suspicious {API} usage detection, suspicious {API} usage repair}
}

@inproceedings{nguyen_learning_2016,
	title = {Learning {API} Usages from Bytecode: A Statistical Approach},
	isbn = {978-1-4503-3900-1},
	url = {http://doi.acm.org/10.1145/2884781.2884873},
	doi = {10.1145/2884781.2884873},
	series = {{ICSE} '16},
	shorttitle = {Learning {API} Usages from Bytecode},
	abstract = {Mobile app developers rely heavily on standard {API} frameworks and libraries. However, learning {API} usages is often challenging due to the fast-changing nature of {API} frameworks for mobile systems and the insufficiency of {API} documentation and source code examples. In this paper, we propose a novel approach to learn {API} usages from bytecode of Android mobile apps. Our core contributions include {HAPI}, a statistical model of {API} usages and three algorithms to extract method call sequences from apps' bytecode, to train {HAPI} based on those sequences, and to recommend method calls in code completion using the trained {HAPIs}. Our empirical evaluation shows that our prototype tool can effectively learn {API} usages from 200 thousand apps containing 350 million method sequences. It recommends next method calls with top-3 accuracy of 90\% and outperforms baseline approaches on average 10–20\%.},
	pages = {416--427},
	booktitle = {Proceedings of the 38th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Nguyen, Tam The and Pham, Hung Viet and Vu, Phong Minh and Nguyen, Tung Thanh},
	urldate = {2016-06-19},
	date = {2016},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, api usage, mobile apps, statistical model}
}

@inproceedings{omar_active_2012,
	title = {Active Code Completion},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337324},
	series = {{ICSE} '12},
	abstract = {Code completion menus have replaced standalone {API} browsers for most developers because they are more tightly integrated into the development workflow. Refinements to the code completion menu that incorporate additional sources of information have similarly been shown to be valuable, even relative to standalone counterparts offering similar functionality. In this paper, we describe active code completion, an architecture that allows library developers to introduce interactive and highly-specialized code generation interfaces, called palettes, directly into the editor. Using several empirical methods, we examine the contexts in which such a system could be useful, describe the design constraints governing the system architecture as well as particular code completion interfaces, and design one such system, named Graphite, for the Eclipse Java development environment. Using Graphite, we implement a palette for writing regular expressions as our primary example and conduct a small pilot study. In addition to showing the feasibility of this approach, it provides further evidence in support of the claim that integrating specialized code completion interfaces directly into the editor is valuable to professional developers.},
	pages = {859--869},
	booktitle = {Proceedings of the 34th International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Omar, Cyrus and Yoon, {YoungSeok} and {LaToza}, Thomas D. and Myers, Brad A.},
	urldate = {2015-12-04},
	date = {2012},
	note = {event-place: Piscataway, {NJ}, {USA}},
	keywords = {\_tablet}
}

@article{pradel_typewriter_2020,
	title = {{TypeWriter}: Neural Type Prediction with Search-Based Validation},
	url = {http://arxiv.org/abs/1912.03768},
	shorttitle = {{TypeWriter}},
	abstract = {Maintaining large code bases written in dynamically typed languages, such as {JavaScript} or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, {IDE} support is limited, and {APIs} are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents {TypeWriter}, the first combination of probabilistic type prediction with search-based refinement of predicted types. {TypeWriter}'s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, {TypeWriter} invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the {TypeWriter} approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that {TypeWriter}'s type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, {TypeWriter} can fully annotate between 14\% to 44\% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that {TypeWriter} adds many more non-trivial types. {TypeWriter} currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.},
	author = {Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish},
	urldate = {2020-06-11},
	date = {2020-03-06},
	eprinttype = {arxiv},
	eprint = {1912.03768},
	keywords = {Computer Science - Software Engineering}
}

@inproceedings{prisca_language_2015,
	title = {A Language Independent User Adaptable Approach for Word Auto-Completion},
	doi = {10.1109/ICCP.2015.7312604},
	abstract = {In this paper, we address the problem of word auto-completion for free text (e.g. messages, emails, articles, poems, etc.) written in different languages. We focus on improving the user experience by developing a user-oriented model that is able to learn different writing styles, while still providing initial predictions without any user written documents. We show that by learning from the user, the performance of an auto-completion system can be improved by up to 18\% compared to a generic, not user-adaptable approach. In order to keep query processing times low, we deploy a binary search technique that retrieves groups of words from an inverted index based on their first letters. This retrieval method reduces the query processing time by up to 80\%.},
	eventtitle = {2015 {IEEE} International Conference on Intelligent Computer Communication and Processing ({ICCP})},
	pages = {43--49},
	booktitle = {2015 {IEEE} International Conference on Intelligent Computer Communication and Processing ({ICCP})},
	author = {Prisca, S. and Potolea, R. and Dinsoreanu, M.},
	date = {2015-09},
	keywords = {\_tablet, natural language processing, Indexes, auto-completion system, binary search technique, Data models, Electronic mail, Encoding, free text, inverted index, language independent user adaptable approach, query processing, query processing time, retrieval method, Runtime, Search problems, text analysis, user written document, user-oriented model, word auto-completion, Writing, writing style}
}

@inproceedings{raghothaman_swim_2016,
	title = {{SWIM}: Synthesizing What I Mean: Code Search and Idiomatic Snippet Synthesis},
	isbn = {978-1-4503-3900-1},
	url = {http://doi.acm.org/10.1145/2884781.2884808},
	doi = {10.1145/2884781.2884808},
	series = {{ICSE} '16},
	shorttitle = {{SWIM}},
	abstract = {Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing {XML} files and sending email. Programmers often use search engines such as Google and Bing to learn about existing {APIs}. In this paper, we describe {SWIM}, a tool which suggests code snippets given {API}-related natural language queries such as "generate md5 hash code". The query does not need to contain framework-specific trivia such as the type names or methods of interest. We translate user queries into the {APIs} of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these {APIs}. We introduce structured call sequences to capture {API}-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated {API} usage patterns, and are simple to extract and amenable to synthesis. We evaluated swim with 30 common C\# {API}-related queries received by Bing. For 70\% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.},
	pages = {357--367},
	booktitle = {Proceedings of the 38th International Conference on Software Engineering},
	publisher = {{ACM}},
	author = {Raghothaman, Mukund and Wei, Yi and Hamadi, Youssef},
	urldate = {2016-06-19},
	date = {2016},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{raychev_code_2014,
	title = {Code Completion with Statistical Language Models},
	isbn = {978-1-4503-2784-8},
	url = {http://doi.acm.org/10.1145/2594291.2594321},
	doi = {10.1145/2594291.2594321},
	series = {{PLDI} '14},
	abstract = {We address the problem of synthesizing code completions for programs using {APIs}. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls. Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments. Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90\% of the cases.},
	pages = {419--428},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {{ACM}},
	author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
	urldate = {2015-12-04},
	date = {2014},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{raychev_probabilistic_2016,
	title = {Probabilistic Model for Code with Decision Trees},
	isbn = {978-1-4503-4444-9},
	url = {http://doi.acm.org/10.1145/2983990.2984041},
	doi = {10.1145/2983990.2984041},
	series = {{OOPSLA} 2016},
	abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., {GitHub}) to make predictions about new programs (e.g., code completion, repair, etc). The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called {TGen}). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as {ID}3, but also to obtain new variants we refer to as {ID}3+ and E13, not previously explored and ones that outperform {ID}3 in prediction accuracy. Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of {JavaScript} and Python. Our experimental results indicate that Deep3 predicts elements of {JavaScript} and Python code with precision above 82\% and 69\%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
	pages = {731--747},
	booktitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
	publisher = {{ACM}},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
	urldate = {2019-11-09},
	date = {2016},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {Code Completion, Decision Trees, Probabilistic Models of Code}
}

@inproceedings{robbes_how_2008,
	title = {How Program History Can Improve Code Completion},
	doi = {10.1109/ASE.2008.42},
	abstract = {Code completion is a widely used productivity tool. It takes away the burden of remembering and typing the exact names of methods or classes: As a developer starts typing a name, it provides a progressively refined list of candidates matching the name. However, the candidate list always comes in alphabetic order, i.e., the environment is only second-guessing the name based on pattern matching. Finding the correct candidate can be cumbersome or slower than typing the full name. We present an approach to improve code completion with program history. We define a benchmark measuring the accuracy and usefulness of a code completion engine. Further, we use the change history data to also improve the results offered by code completion tools. Finally, we propose an alternative interface for completion tools.},
	eventtitle = {23rd {IEEE}/{ACM} International Conference on Automated Software Engineering, 2008. {ASE} 2008},
	pages = {317--326},
	booktitle = {23rd {IEEE}/{ACM} International Conference on Automated Software Engineering, 2008. {ASE} 2008},
	author = {Robbes, R. and Lanza, M.},
	date = {2008-09},
	keywords = {\_tablet, Performance evaluation, Pattern matching, Computer languages, code completion, Benchmark testing, codes, Engines, History, Information resources, Measurement standards, Productivity, productivity tool, Switches}
}

@article{robillard_recommendation_2010,
	title = {Recommendation Systems for Software Engineering},
	volume = {27},
	issn = {0740-7459},
	doi = {10.1109/MS.2009.161},
	abstract = {Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering ({RSSEs}) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.},
	pages = {80--86},
	number = {4},
	journaltitle = {{IEEE} Software},
	author = {Robillard, M.P. and Walker, R.J. and Zimmermann, T.},
	date = {2010-07},
	keywords = {\_tablet, Software Development, recommender systems, software engineering, programming environments, bug reports, coding tools and techniques, design tools and techniques, development tools, information space, recommendation system, software construction tools, software tool, Software tools, time seeking information, value-producing task}
}

@book{robillard_recommendation_2014,
	title = {Recommendation Systems in Software Engineering},
	isbn = {978-3-642-45135-5},
	url = {http://www.springer.com/computer/swe/book/978-3-642-45134-8},
	abstract = {With the growth of public and private data stores and the emergence of off-the-shelf data-mining technology, recommendation systems have emerged that specifically address the unique challenges of navigating and interpreting software engineering data. This book collects, structures, and formalizes knowledge on recommendation systems in software engineering. It adopts a pragmatic approach with an explicit focus on system design, implementation, and evaluation. The book is divided into three parts: Part I Techniques introduces basics for building recommenders in software engineering, including techniques for collecting and processing software engineering data, but also for presenting recommendations to users as part of their workflow. Part {II} Evaluation summarizes methods and experimental designs for evaluating recommendations in software engineering. Part {III} Applications describes needs, issues, and solution concepts involved in entire recommendation systems for specific software engineering tasks, focusing on the engineering insights required to make effective recommendations. The book is complemented by the webpage rsse.org/book, which includes free supplemental materials for readers of this book and anyone interested in recommendation systems in software engineering, including lecture slides, data sets, source code, and an overview of people, groups, papers, and tools with regard to recommendation systems in software engineering. The book is particularly well-suited for graduate students and researchers building new recommendation systems for software engineering applications or in other high-tech fields. It may also serve as the basis for graduate courses on recommendation systems, applied data mining, or software engineering. Software engineering practitioners developing recommendation systems or similar applications with predictive functionality will also benefit from the broad spectrum of topics covered.},
	pagetotal = {560},
	publisher = {Springer Science \& Business},
	author = {Robillard, Martin P.},
	date = {2014-01-01},
	keywords = {\_tablet, Computers / Programming / Open Source, Computers / Software Development \& Engineering / General, Computers / Software Development \& Engineering / Tools}
}

@inproceedings{schein_methods_2002,
	title = {Methods and Metrics for Cold-Start Recommendations},
	isbn = {1-58113-561-0},
	url = {http://doi.acm.org/10.1145/564376.564421},
	doi = {10.1145/564376.564421},
	series = {{SIGIR} '02},
	abstract = {We have developed a method for recommending items that combines content and collaborative data under a single probabilistic framework. We benchmark our algorithm against a naïve Bayes classifier on the cold-start problem, where we wish to recommend items that no one in the community has yet rated. We systematically explore three testing methodologies using a publicly available data set, and explain how these methods apply to specific real-world applications. We advocate heuristic recommenders when benchmarking to give competent baseline performance. We introduce a new performance metric, the {CROC} curve, and demonstrate empirically that the various components of our testing strategy combine to obtain deeper understanding of the performance characteristics of recommender systems. Though the emphasis of our testing is on cold-start recommending, our methods for recommending and evaluation are general.},
	pages = {253--260},
	booktitle = {Proceedings of the 25th Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Schein, Andrew I. and Popescul, Alexandrin and Ungar, Lyle H. and Pennock, David M.},
	urldate = {2016-02-06},
	date = {2002},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {collaborative filtering, content-based filtering, graphical models, information retrieval, Performance evaluation, recommender systems}
}

@article{sun_treegen_2019,
	title = {{TreeGen}: A Tree-Based Transformer Architecture for Code Generation},
	url = {http://arxiv.org/abs/1911.09983},
	shorttitle = {{TreeGen}},
	abstract = {A code generation system generates programming language code based on an input natural language description. State-of-the-art approaches rely on neural networks for code generation. However, these code generators suffer from two problems. One is the long dependency problem, where a code element often depends on another far-away code element. A variable reference, for example, depends on its definition, which may appear quite a few lines before. The other problem is structure modeling, as programs contain rich structural information. In this paper, we propose a novel tree-based neural architecture, {TreeGen}, for code generation. {TreeGen} uses the attention mechanism of Transformers to alleviate the long-dependency problem, and introduces a novel {AST} reader (encoder) to incorporate grammar rules and {AST} structures into the network. We evaluated {TreeGen} on a Python benchmark, {HearthStone}, and two semantic parsing benchmarks, {ATIS} and {GEO}. {TreeGen} outperformed the previous state-of-the-art approach by 4.5 percentage points on {HearthStone}, and achieved the best accuracy among neural network-based approaches on {ATIS} (89.1\%) and {GEO} (89.6\%). We also conducted an ablation test to better understand each component of our model.},
	author = {Sun, Zeyu and Zhu, Qihao and Xiong, Yingfei and Sun, Yican and Mou, Lili and Zhang, Lu},
	urldate = {2020-01-01},
	date = {2019-11-28},
	eprinttype = {arxiv},
	eprint = {1911.09983},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering}
}

@inproceedings{svyatkovskiy_pythia_2019,
	title = {Pythia: {AI}-Assisted Code Completion System},
	isbn = {978-1-4503-6201-6},
	url = {https://doi.org/10.1145/3292500.3330699},
	doi = {10.1145/3292500.3330699},
	series = {{KDD} '19},
	shorttitle = {Pythia},
	abstract = {In this paper, we propose a novel end-to-end approach for {AI}-assisted code completion called Pythia. It generates ranked lists of method and {API} recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code {IDE}. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 ms. We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices. The offline evaluation results obtained on 2700 Python open source software {GitHub} repositories show a top-5 accuracy of 92\%, surpassing the baseline models by 20\% averaged over classes, for both intra and cross-project settings.},
	pages = {2727--2735},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
	urldate = {2020-06-08},
	date = {2019-07-25},
	note = {event-place: Anchorage, {AK}, {USA}},
	keywords = {neural networks, code completion, naturalness of software}
}

@article{svyatkovskiy_fast_2020,
	title = {Fast and Memory-Efficient Neural Code Completion},
	url = {http://arxiv.org/abs/2004.13651},
	abstract = {Code completion is one of the most widely used features of modern integrated development environments ({IDEs}). Deep learning has recently made significant progress in the statistical prediction of source code. However, state-of-the-art neural network models consume prohibitively large amounts of memory, causing computational burden to the development environment, especially when deployed in lightweight client devices. In this work, we reframe neural code completion from a generation task to a task of learning to rank the valid completion suggestions computed from static analyses. By doing so, we are able to design and test a variety of deep neural network model configurations. One of our best models consumes 6 {MB} of {RAM}, computes a single suggestion in 8 ms, and achieves 90\% recall in its top five suggestions. Our models outperform standard language modeling code completion techniques in terms of predictive performance, computational speed, and memory efficiency. Furthermore, they learn about code semantics from the natural language aspects of the code (e.g. identifier names) and can generalize better to previously unseen code.},
	author = {Svyatkovskiy, Alexey and Lee, Sebastian and Hadjitofi, Anna and Riechert, Maik and Franco, Juliana and Allamanis, Miltiadis},
	urldate = {2020-06-10},
	date = {2020-04-29},
	eprinttype = {arxiv},
	eprint = {2004.13651},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering}
}

@article{svyatkovskiy_intellicode_2020,
	title = {{IntelliCode} Compose: Code Generation Using Transformer},
	url = {http://arxiv.org/abs/2005.08025},
	shorttitle = {{IntelliCode} Compose},
	abstract = {In software development through integrated development environments ({IDEs}), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and {APIs}, or arguments. In this paper, we introduce {IntelliCode} Compose \$-\$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, \$C{\textbackslash}textbackslash\#\$, {JavaScript} and {TypeScript} programming languages. {IntelliCode} Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code {IDE} and Azure Notebook. Our best model yields an average edit similarity of \$86.7{\textbackslash}textbackslash\%\$ and a perplexity of 1.82 for Python programming language.},
	author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
	urldate = {2020-06-11},
	date = {2020-05-16},
	eprinttype = {arxiv},
	eprint = {2005.08025},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Encoding, gpt-2},
	file = {Volltext:/home/enrico/Zotero/storage/N59K8XW8/Svyatkovskiy et al. - 2020 - IntelliCode Compose Code Generation Using Transfo.pdf:application/pdf}
}

@online{tabnine_autocompletion_2019,
	title = {Autocompletion with Deep Learning},
	url = {https://tabnine.com/blog/deep/},
	abstract = {{TabNine} is the all-language autocompleter. We use deep learning to help you write code faster.},
	author = {{TabNine}},
	urldate = {2020-02-20},
	date = {2019-07-15}
}

@unpublished{vechev_pldi_2015,
	title = {{PLDI} 2015 Tutorial: Machine Learning for Code Analysis},
	url = {https://www.sri.inf.ethz.ch/ml4code-tutorial},
	author = {Vechev, Martin and Raychev, Veselin},
	date = {2015-06-14}
}

@inproceedings{wallach_topic_2006,
	title = {Topic Modeling: Beyond Bag-of-Words},
	isbn = {1-59593-383-2},
	url = {http://doi.acm.org/10.1145/1143844.1143967},
	doi = {10.1145/1143844.1143967},
	series = {{ICML} '06},
	shorttitle = {Topic Modeling},
	abstract = {Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the "bag-of-words" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs {EM} algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.},
	pages = {977--984},
	booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Wallach, Hanna M.},
	urldate = {2015-12-21},
	date = {2006},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{wang_active_2014,
	title = {Active Code Search: Incorporating User Feedback to Improve Code Search Relevance},
	isbn = {978-1-4503-3013-8},
	url = {http://doi.acm.org/10.1145/2642937.2642947},
	doi = {10.1145/2642937.2642947},
	series = {{ASE} '14},
	shorttitle = {Active Code Search},
	abstract = {Code search techniques return relevant code fragments given a user query. They typically work in a passive mode: given a user query, a static list of code fragments sorted by the relevance scores decided by a code search technique is returned to the user. A user will go through the sorted list of returned code fragments from top to bottom. As the user checks each code fragment one by one, he or she will naturally form an opinion about the true relevance of the code fragment. In an active model, those opinions will be taken as feedbacks to the search engine for refining result lists. In this work, we incorporate users' opinion on the results from a code search engine to refine result lists: as a user forms an opinion about one result, our technique takes this opinion as feedback and leverages it to re-order the results to make truly relevant results appear earlier in the list. The refinement results can also be cached to potentially improve future code search tasks. We have built our active refinement technique on top of a state-of-the-art code search engine—Portfolio. Our technique improves Portfolio in terms of Normalized Discounted Cumulative Gain ({NDCG}) by more than 11.3\%, from 0.738 to 0.821.},
	pages = {677--682},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} International Conference on Automated Software Engineering},
	publisher = {{ACM}},
	author = {Wang, Shaowei and Lo, David and Jiang, Lingxiao},
	urldate = {2015-12-05},
	date = {2014},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, active learning, code search, user feedback}
}

@article{xu_incorporating_2020,
	title = {Incorporating External Knowledge through Pre-Training for Natural Language to Code Generation},
	url = {http://arxiv.org/abs/2004.09015},
	abstract = {Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language ({NL}) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into {NL}-to-code generation: automatically mined {NL}-code pairs from the online programming {QA} forum {StackOverflow} and programming language {API} documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2\% absolute {BLEU} score on the code generation testbed {CoNaLa}. The code and resources are available at https://github.com/neulab/external-knowledge-codegen.},
	author = {Xu, Frank F. and Jiang, Zhengbao and Yin, Pengcheng and Vasilescu, Bogdan and Neubig, Graham},
	urldate = {2020-06-23},
	date = {2020-04-19},
	eprinttype = {arxiv},
	eprint = {2004.09015},
	keywords = {Computer Science - Computation and Language}
}

@inproceedings{yuan_libraryguru_2018,
	title = {{LibraryGuru}: {API} Recommendation for Android Developers},
	isbn = {978-1-4503-5663-3},
	url = {http://doi.acm.org/10.1145/3183440.3195011},
	doi = {10.1145/3183440.3195011},
	series = {{ICSE} '18},
	shorttitle = {{LibraryGuru}},
	abstract = {Developing modern mobile applications often require the uses of many libraries specific for the mobile platform, which can be overwhelmingly too many for application developers to find what are needed for a functionality and where and how to use them properly. This paper presents a tool, named {LibraryGuru}, to recommend suitable Android {APIs} for given functionality descriptions. It not only recommends functional {APIs} that can be invoked for implementing the functionality, but also recommends event callback {APIs} that are inherent in the Android framework and need to be overridden in the application. {LibraryGuru} internally builds correlation databases among various functionality descriptions and Android {APIs}. These correlations are extracted from Android development tutorials and {SDK} documents with domain-specific code parsing and natural language processing techniques adapted for functional {APIs} and event callback {APIs} separately, and are matched against functionality queries to recommend relevant {APIs} for developers. {LibraryGuru} is publicly accessible at http://libraryguru.info, and a demo video is available at https://youtu.be/f7MtjliUM-4.},
	pages = {364--365},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
	publisher = {{ACM}},
	author = {Yuan, Weizhao and Nguyen, Hoang H. and Jiang, Lingxiao and Chen, Yuting},
	urldate = {2019-03-24},
	date = {2018},
	note = {event-place: New York, {NY}, {USA}}
}

@article{yuan_api_2019,
	title = {{API} Recommendation for Event-Driven Android Application Development},
	volume = {107},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584918302222},
	doi = {10.1016/j.infsof.2018.10.010},
	abstract = {Context Software development is increasingly dependent on existing libraries. Developers need help to find suitable library {APIs}. Although many studies have been proposed to recommend relevant functional {APIs} that can be invoked for implementing a functionality, few studies have paid attention to an orthogonal need associated with event-driven programming frameworks, such as the Android framework. In addition to invoking functional {APIs}, Android developers need to know where to place functional code according to various events that may be triggered within the framework. Objective This paper aims to develop an {API} recommendation engine for Android application development that can recommend both (1) functional {APIs} for implementing a functionality and (2) the event callback {APIs} that are to be overridden to contain the functional code. Method We carry out an empirical study on actual Android programming questions from {StackOverflow} to confirm the need of recommending callbacks. Then we build Android-specific {API} databases to contain the correlations among various functionalities and {APIs}, based on customized parsing of code snippets and natural language processing of texts in Android tutorials and {SDK} documents, and then textual and code similarity metrics are adapted for recommending relevant {APIs}. Results We have evaluated our prototype recommendation engine, named {LibraryGuru}, with about 1500 questions on Android programming from {StackOverflow}, and demonstrated that our top-5 results on recommending callbacks and functional {APIs} can on estimate achieve up to 43.5\% and 50.9\% respectively in precision, 24.6\% and 32.5\% respectively in mean average precision ({MAP}) scores, and 51.1\% and 44.0\% respectively in recall. Conclusion We conclude that it is important and possible to recommend both functional {APIs} and callbacks for Android application development, and future work is needed to take more data sources into consideration to make more relevant recommendations for developers’ needs.},
	pages = {30--47},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Yuan, Weizhao and Nguyen, Hoang H. and Jiang, Lingxiao and Chen, Yuting and Zhao, Jianjun and Yu, Haibo},
	urldate = {2019-03-24},
	date = {2019-03-01},
	keywords = {Android programming, {API} recommendation, Code search, Event callbacks, Information retrieval}
}

@inproceedings{zhang_automatic_2012,
	title = {Automatic Parameter Recommendation for Practical {API} Usage},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337321},
	series = {{ICSE} '12},
	abstract = {Programmers extensively use application programming interfaces ({APIs}) to leverage existing libraries and frameworks. However, correctly and efficiently choosing and using {APIs} from unfamiliar libraries and frameworks is still a non-trivial task. Programmers often need to ruminate on {API} documentations (that are often incomplete) or inspect code examples (that are often absent) to learn {API} usage patterns. Recently, various techniques have been proposed to alleviate this problem by creating {API} summarizations, mining code examples, or showing common {API} call sequences. However, few techniques focus on recommending {API} parameters. In this paper, we propose an automated technique, called Precise, to address this problem. Differing from common code completion systems, Precise mines existing code bases, uses an abstract usage instance representation for each {API} usage example, and then builds a parameter usage database. Upon a request, Precise queries the database for abstract usage instances in similar contexts and generates parameter candidates by concretizing the instances adaptively. The experimental results show that our technique is more general and applicable than existing code completion systems, specially, 64\% of the parameter recommendations are useful and 53\% of the recommendations are exactly the same as the actual parameters needed. We have also performed a user study to show our technique is useful in practice.},
	pages = {826--836},
	booktitle = {Proceedings of the 34th International Conference on Software Engineering},
	publisher = {{IEEE} Press},
	author = {Zhang, Cheng and Yang, Juyuan and Zhang, Yi and Fan, Jing and Zhang, Xin and Zhao, Jianjun and Ou, Peizhao},
	urldate = {2015-02-19},
	date = {2012},
	note = {event-place: Piscataway, {NJ}, {USA}},
	keywords = {\_tablet}
}

@inproceedings{zhang_bing_2016,
	title = {Bing Developer Assistant: Improving Developer Productivity by Recommending Sample Code},
	isbn = {978-1-4503-4218-6},
	url = {http://doi.acm.org/10.1145/2950290.2983955},
	doi = {10.1145/2950290.2983955},
	series = {{FSE} 2016},
	shorttitle = {Bing Developer Assistant},
	abstract = {In programming practice, developers often need sample code in order to learn how to solve a programming-related problem. For example, how to reuse an Application Programming Interface ({API}) of a large-scale software library and how to implement a certain functionality. We believe that previously written code can help developers understand how others addressed the similar problems and can help them write new programs. We develop a tool called Bing Developer Assistant ({BDA}), which improves developer productivity by recommending sample code mined from public software repositories (such as {GitHub}) and web pages (such as Stack Overflow). {BDA} can automatically mine code snippets that implement an {API} or answer a code search query. It has been implemented as a free-downloadable extension of Microsoft Visual Studio and has received more than 670K downloads since its initial release in December 2014. {BDA} is publicly available at: http://aka.ms/devassistant.},
	pages = {956--961},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
	publisher = {{ACM}},
	author = {Zhang, Hongyu and Jain, Anuj and Khandelwal, Gaurav and Kaushik, Chandrashekhar and Ge, Scott and Hu, Wenxiang},
	date = {2016},
	note = {event-place: New York, {NY}, {USA}},
	keywords = {\_tablet, code search, {API}, {API} Usage Extraction, github, software reuse}
}

@article{kim_code_2020-1,
	title = {Code Prediction by Feeding Trees to Transformers},
	url = {http://arxiv.org/abs/2003.13848},
	abstract = {We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an {RNN}-based system (similar to Hellendoorn et al. 2018) by 18.3{\textbackslash}\%, the Deep3 system (Raychev et al 2016) by 14.1{\textbackslash}\%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4{\textbackslash}\%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.},
	journaltitle = {{arXiv}:2003.13848 [cs]},
	author = {Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
	urldate = {2020-08-02},
	date = {2020-07-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.13848},
	keywords = {Computer Science - Software Engineering},
	file = {Kim et al. - 2020 - Code Prediction by Feeding Trees to Transformers.pdf:/home/enrico/Zotero/storage/I3CPGWHP/Kim et al. - 2020 - Code Prediction by Feeding Trees to Transformers.pdf:application/pdf}
}

@article{dam_deep_2016,
	title = {A deep language model for software code},
	url = {http://arxiv.org/abs/1608.02715},
	abstract = {Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the eﬀectiveness of our language model. This work contributes to realizing our vision for {DeepSoft}, an end-to-end, generic deep learning-based framework for modeling software and its development process.},
	journaltitle = {{arXiv}:1608.02715 [cs, stat]},
	author = {Dam, Hoa Khanh and Tran, Truyen and Pham, Trang},
	urldate = {2020-08-02},
	date = {2016-08-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.02715},
	keywords = {Statistics - Machine Learning, Computer Science - Software Engineering},
	file = {Dam et al. - 2016 - A deep language model for software code.pdf:/home/enrico/Zotero/storage/BC8FUFQM/Dam et al. - 2016 - A deep language model for software code.pdf:application/pdf}
}

@article{allamanis_learning_2018,
	title = {Learning to Represent Programs with Graphs},
	url = {http://arxiv.org/abs/1711.00740},
	abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code’s known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: {VARNAMING}, in which a network attempts to predict the name of a variable given its usage, and {VARMISUSE}, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the {VARMISUSE} task in many cases. Additionally, our testing showed that {VARMISUSE} identiﬁes a number of bugs in mature open-source projects.},
	journaltitle = {{arXiv}:1711.00740 [cs]},
	author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
	urldate = {2020-08-02},
	date = {2018-05-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.00740},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
	file = {Allamanis et al. - 2018 - Learning to Represent Programs with Graphs.pdf:/home/enrico/Zotero/storage/6WWJP2AN/Allamanis et al. - 2018 - Learning to Represent Programs with Graphs.pdf:application/pdf}
}

@article{alon_general_2018,
	title = {A General Path-Based Representation for Predicting Program Properties},
	url = {http://arxiv.org/abs/1803.09544},
	abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree ({AST}). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens.},
	journaltitle = {{arXiv}:1803.09544 [cs]},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2020-08-02},
	date = {2018-04-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.09544},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {Alon et al. - 2018 - A General Path-Based Representation for Predicting.pdf:/home/enrico/Zotero/storage/9AURHKYP/Alon et al. - 2018 - A General Path-Based Representation for Predicting.pdf:application/pdf}
}

@article{dai_transformer-xl_2019,
	title = {Transformer-{XL}: Attentive Language Models Beyond a Fixed-Length Context},
	url = {http://arxiv.org/abs/1901.02860},
	shorttitle = {Transformer-{XL}},
	abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a ﬁxed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-{XL} that enables learning dependency beyond a ﬁxed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, {TransformerXL} learns dependency that is 80\% longer than {RNNs} and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on {WikiText}-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without ﬁnetuning). When trained only on {WikiText}-103, Transformer-{XL} manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorﬂow and {PyTorch}1.},
	journaltitle = {{arXiv}:1901.02860 [cs, stat]},
	author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
	urldate = {2020-08-02},
	date = {2019-06-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1901.02860},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf:/home/enrico/Zotero/storage/SJYYNQ6I/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf:application/pdf}
}

@article{liu_self-attentional_2020-1,
	title = {A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning},
	url = {http://arxiv.org/abs/1909.06983},
	abstract = {Code completion, one of the most useful features in the Integrated Development Environments ({IDEs}), can accelerate software development by suggesting the libraries, {APIs}, and method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program’s representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. To address these challenges, in this paper, we propose a selfattentional neural architecture for code completion with multi-task learning. To utilize the hierarchical structural information of the programs, we present a novel method that considers the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable the knowledge sharing between related tasks, we creatively propose a Multi-Task Learning ({MTL}) framework to learn two related tasks in code completion jointly. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.},
	journaltitle = {{arXiv}:1909.06983 [cs]},
	author = {Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Fu, Zhiyi and Jin, Zhi},
	urldate = {2020-08-02},
	date = {2020-06-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.06983},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {Liu et al. - 2020 - A Self-Attentional Neural Architecture for Code Co.pdf:/home/enrico/Zotero/storage/5RXXFIKM/Liu et al. - 2020 - A Self-Attentional Neural Architecture for Code Co.pdf:application/pdf}
}

@inproceedings{zhang_novel_2019,
	location = {Montreal, {QC}, Canada},
	title = {A Novel Neural Source Code Representation Based on Abstract Syntax Tree},
	isbn = {978-1-72810-869-8},
	url = {https://ieeexplore.ieee.org/document/8812062/},
	doi = {10.1109/ICSE.2019.00086},
	abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree ({AST}) based neural models can better represent source code. However, the sizes of {ASTs} are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel {AST}-based Neural Network ({ASTNN}) for source code representation. Unlike existing models that work on entire {ASTs}, {ASTNN} splits each large {AST} into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional {RNN} model is used to leverage the naturalness of statements and ﬁnally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classiﬁcation and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
	eventtitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	pages = {783--794},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	publisher = {{IEEE}},
	author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
	urldate = {2020-08-02},
	date = {2019-05},
	langid = {english},
	file = {Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf:/home/enrico/Zotero/storage/NQM9D52E/Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf:application/pdf}
}

@inproceedings{buch_learning-based_2019,
	location = {Hangzhou, China},
	title = {Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection},
	isbn = {978-1-72810-591-8},
	url = {https://ieeexplore.ieee.org/document/8668039/},
	doi = {10.1109/SANER.2019.8668039},
	abstract = {Code clone detection remains a crucial challenge in maintaining software projects. Many classic approaches rely on handcrafted aggregation schemes, while recent work uses supervised or unsupervised learning. In this work, we study several aspects of aggregation schemes for code clone detection based on supervised learning. To this aim, we implement an {AST}-based Recursive Neural Network. Firstly, our ablation study shows the inﬂuence of model choices and hyperparameters. We introduce error scaling as a way to effectively and efﬁciently address the class imbalance problem arising in code clone detection. Secondly, we study the inﬂuence of pretrained embeddings representing nodes in {ASTs}. We show that simply averaging all node vectors of a given {AST} yields strong baseline aggregation scheme. Further, learned {AST} aggregation schemes greatly beneﬁt from pretrained node embeddings. Finally, we show the importance of carefully separating training and test data by clone clusters, to reliably measure generalization of models learned with supervision.},
	eventtitle = {2019 {IEEE} 26th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {95--104},
	booktitle = {2019 {IEEE} 26th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	publisher = {{IEEE}},
	author = {Buch, Lutz and Andrzejak, Artur},
	urldate = {2020-08-02},
	date = {2019-02},
	langid = {english},
	file = {Buch und Andrzejak - 2019 - Learning-Based Recursive Aggregation of Abstract S.pdf:/home/enrico/Zotero/storage/MQQNFFDW/Buch und Andrzejak - 2019 - Learning-Based Recursive Aggregation of Abstract S.pdf:application/pdf}
}

@article{li_code_2018-1,
	title = {Code Completion with Neural Attention and Pointer Networks},
	url = {http://arxiv.org/abs/1711.09573},
	doi = {10.24963/ijcai.2018/578},
	abstract = {Intelligent code completion has become an essential research task to accelerate modern software development. To facilitate effective code completion for dynamically-typed programming languages, we apply neural language models by learning from large codebases, and develop a tailored attention mechanism for code completion. However, standard neural language models even with attention mechanism cannot correctly predict the outof-vocabulary ({OoV}) words that restrict the code completion performance. In this paper, inspired by the prevalence of locally repeated terms in program source code, and the recently proposed pointer copy mechanism, we propose a pointer mixture network for better predicting {OoV} words in code completion. Based on the context, the pointer mixture network learns to either generate a withinvocabulary word through an {RNN} component, or regenerate an {OoV} word from local context through a pointer component. Experiments on two benchmarked datasets demonstrate the effectiveness of our attention mechanism and pointer mixture network on the code completion task.},
	pages = {4159--4165},
	journaltitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
	author = {Li, Jian and Wang, Yue and Lyu, Michael R. and King, Irwin},
	urldate = {2020-08-02},
	date = {2018-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.09573},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Li et al. - 2018 - Code Completion with Neural Attention and Pointer .pdf:/home/enrico/Zotero/storage/WLLNLQEF/Li et al. - 2018 - Code Completion with Neural Attention and Pointer .pdf:application/pdf}
}

@article{luan_aroma_2019-1,
	title = {Aroma: Code Recommendation via Structural Code Search},
	volume = {3},
	issn = {2475-1421, 2475-1421},
	url = {http://arxiv.org/abs/1812.01158},
	doi = {10.1145/3360578},
	shorttitle = {Aroma},
	abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an {IDE} plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently. {CCS} Concepts: • Information systems → Near-duplicate and plagiarism detection; • Software and its engineering → Development frameworks and environments; Software post-development issues.},
	pages = {1--28},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
	urldate = {2020-08-02},
	date = {2019-10-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1812.01158},
	keywords = {Computer Science - Software Engineering},
	file = {Luan et al. - 2019 - Aroma Code Recommendation via Structural Code Sea.pdf:/home/enrico/Zotero/storage/LYSLGYDG/Luan et al. - 2019 - Aroma Code Recommendation via Structural Code Sea.pdf:application/pdf}
}

@article{alon_code2seq_2019,
	title = {code2seq: Generating Sequences from Structured Representations of Code},
	url = {http://arxiv.org/abs/1808.01400},
	shorttitle = {code2seq},
	abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation ({NMT}), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present {CODE}2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree ({AST}) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model signiﬁcantly outperforms previous models that were speciﬁcally designed for programming languages, as well as state-of-the-art {NMT} models. An online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
	journaltitle = {{arXiv}:1808.01400 [cs, stat]},
	author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
	urldate = {2020-08-02},
	date = {2019-02-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.01400},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
	file = {Alon et al. - 2019 - code2seq Generating Sequences from Structured Rep.pdf:/home/enrico/Zotero/storage/JGI2N2UI/Alon et al. - 2019 - code2seq Generating Sequences from Structured Rep.pdf:application/pdf}
}

@article{alon_code2vec_2018,
	title = {code2vec: Learning Distributed Representations of Code},
	url = {http://arxiv.org/abs/1803.09473},
	shorttitle = {code2vec},
	abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (“code embeddings”). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path simultaneously with learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method’s name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
	journaltitle = {{arXiv}:1803.09473 [cs, stat]},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2020-08-02},
	date = {2018-10-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.09473},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Statistics - Machine Learning},
	file = {Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf:/home/enrico/Zotero/storage/YRQYVA3F/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf:application/pdf}
}

@article{feng_codebert_2020,
	title = {{CodeBERT}: A Pre-Trained Model for Programming and Natural Languages},
	url = {http://arxiv.org/abs/2002.08155},
	shorttitle = {{CodeBERT}},
	abstract = {We present {CodeBERT}, a bimodal pre-trained model for programming language ({PL}) and natural language ({NL}). {CodeBERT} learns generalpurpose representations that support downstream {NL}-{PL} applications such as natural language code search, code documentation generation, etc. We develop {CodeBERT} with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both “bimodal” data of {NLPL} pairs and “unimodal” data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate {CodeBERT} on two {NL}-{PL} applications by ﬁne-tuning model parameters. Results show that {CodeBERT} achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in {CodeBERT}, we construct a dataset for {NL}-{PL} probing, and evaluate in a zero-shot setting where parameters of pre-trained models are ﬁxed. Results show that {CodeBERT} performs better than previous pre-trained models on {NL}-{PL} probing.},
	journaltitle = {{arXiv}:2002.08155 [cs]},
	author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
	urldate = {2020-08-02},
	date = {2020-04-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.08155},
	keywords = {Computer Science - Programming Languages, Computer Science - Computation and Language},
	file = {Feng et al. - 2020 - CodeBERT A Pre-Trained Model for Programming and .pdf:/home/enrico/Zotero/storage/DWHALAFF/Feng et al. - 2020 - CodeBERT A Pre-Trained Model for Programming and .pdf:application/pdf}
}

@inproceedings{hellendoorn_are_2017,
	location = {Paderborn, Germany},
	title = {Are deep neural networks the best choice for modeling source code?},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106290},
	doi = {10.1145/3106237.3106290},
	abstract = {Current statistical language modeling techniques, including deeplearning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add \& remove text, and mix \& swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N -gram, as well as {RNN}, and {LSTM} deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N -gram models for source code can yield performance that surpasses even {RNN} and {LSTM} based deep-learning models.},
	eventtitle = {the 2017 11th Joint Meeting},
	pages = {763--773},
	booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering  - {ESEC}/{FSE} 2017},
	publisher = {{ACM} Press},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
	urldate = {2020-08-02},
	date = {2017},
	langid = {english},
	file = {Hellendoorn und Devanbu - 2017 - Are deep neural networks the best choice for model.pdf:/home/enrico/Zotero/storage/LY75RLM3/Hellendoorn und Devanbu - 2017 - Are deep neural networks the best choice for model.pdf:application/pdf}
}

@article{brockschmidt_generative_2019,
	title = {{GENERATIVE} {CODE} {MODELING} {WITH} {GRAPHS}},
	abstract = {Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.},
	pages = {24},
	author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander and Polozov, Oleksandr},
	date = {2019},
	langid = {english},
	file = {Brockschmidt et al. - 2019 - GENERATIVE CODE MODELING WITH GRAPHS.pdf:/home/enrico/Zotero/storage/AKKUQ459/Brockschmidt et al. - 2019 - GENERATIVE CODE MODELING WITH GRAPHS.pdf:application/pdf}
}

@article{kim_code_2020-2,
	title = {Code Prediction by Feeding Trees to Transformers},
	url = {http://arxiv.org/abs/2003.13848},
	abstract = {We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an {RNN}-based system (similar to Hellendoorn et al. 2018) by 18.3{\textbackslash}\%, the Deep3 system (Raychev et al 2016) by 14.1{\textbackslash}\%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4{\textbackslash}\%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.},
	journaltitle = {{arXiv}:2003.13848 [cs]},
	author = {Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
	urldate = {2020-08-02},
	date = {2020-07-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.13848},
	keywords = {Computer Science - Software Engineering, Encoding, gpt-2},
	file = {Kim et al. - 2020 - Code Prediction by Feeding Trees to Transformers.pdf:/home/enrico/Zotero/storage/Y8P66MJV/Kim et al. - 2020 - Code Prediction by Feeding Trees to Transformers.pdf:application/pdf}
}

@article{radford_language_nodate,
	title = {Language Models are Unsupervised Multitask Learners},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	pages = {24},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	langid = {english},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/home/enrico/Zotero/storage/TD9UPAG8/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf}
}

@article{babii_modeling_2019,
	title = {Modeling Vocabulary for Big Code Machine Learning},
	url = {http://arxiv.org/abs/1904.01873},
	abstract = {When building machine learning models that operate on source code, several decisions have to be made to model source-code vocabulary. These decisions can have a large impact: some can lead to not being able to train models at all, others significantly affect performance, particularly for Neural Language Models. Yet, these decisions are not often fully described. This paper lists important modeling choices for source code vocabulary, and explores their impact on the resulting vocabulary on a large-scale corpus of 14,436 projects. We show that a subset of decisions have decisive characteristics, allowing to train accurate Neural Language Models quickly on a large corpus of 10,106 projects.},
	journaltitle = {{arXiv}:1904.01873 [cs]},
	author = {Babii, Hlib and Janes, Andrea and Robbes, Romain},
	urldate = {2020-08-02},
	date = {2019-04-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.01873},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	file = {Babii et al. - 2019 - Modeling Vocabulary for Big Code Machine Learning.pdf:/home/enrico/Zotero/storage/GMNMUM5Q/Babii et al. - 2019 - Modeling Vocabulary for Big Code Machine Learning.pdf:application/pdf}
}

@article{raychev_probabilistic_nodate,
	title = {Probabilistic Model for Code with Decision Trees},
	abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly beneﬁts an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., {GitHub}) to make predictions about new programs (e.g., code completion, repair, etc).},
	pages = {17},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
	langid = {english},
	file = {Raychev et al. - Probabilistic Model for Code with Decision Trees.pdf:/home/enrico/Zotero/storage/CG4K8ECH/Raychev et al. - Probabilistic Model for Code with Decision Trees.pdf:application/pdf}
}

@article{svyatkovskiy_intellicode_2020-1,
	title = {{IntelliCode} Compose: Code Generation Using Transformer},
	url = {http://arxiv.org/abs/2005.08025},
	shorttitle = {{IntelliCode} Compose},
	abstract = {In software development through integrated development environments ({IDEs}), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and {APIs}, or arguments. In this paper, we introduce {IntelliCode} Compose \$-\$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, \$C{\textbackslash}textbackslash\#\$, {JavaScript} and {TypeScript} programming languages. {IntelliCode} Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code {IDE} and Azure Notebook. Our best model yields an average edit similarity of \$86.7{\textbackslash}textbackslash\%\$ and a perplexity of 1.82 for Python programming language.},
	author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
	urldate = {2020-06-11},
	date = {2020-05-16},
	eprinttype = {arxiv},
	eprint = {2005.08025},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering}
}

@article{noauthor_standardized_nodate,
	title = {Standardized code quality benchmarking for improving software maintainability}
}

@article{baggen_standardized_2012,
	title = {Standardized code quality benchmarking for improving software maintainability},
	volume = {20},
	pages = {287--307},
	number = {2},
	journaltitle = {Software Quality Journal},
	author = {Baggen, Robert and Correia, José Pedro and Schill, Katrin and Visser, Joost},
	date = {2012},
	note = {Publisher: Springer},
	file = {Baggen et al. - 2012 - Standardized code quality benchmarking for improvi.pdf:/home/enrico/Zotero/storage/LH4HVAVP/Baggen et al. - 2012 - Standardized code quality benchmarking for improvi.pdf:application/pdf}
}

@article{ismail_responses_1978,
	title = {Responses of Anopheles minimus to {DDT} residual spraying in a cleared forested foothill area in central Thailand},
	volume = {35},
	issn = {0001-706X},
	abstract = {Anopheles balabacensis balabacensis and Anopheles minimus are the main malaria vectors in Thailand. In a cleared forested foothill area in the central part of the country A. minimus was the most prevalent anopheline species found, only 6 specimens of A. b. balabacensis being collected over a 3-year period. Cattle were scarce in the area, tractors being largely used for working in the fields. This situation contributed to high man-vector contact. A minimus occurred throughout the year, with a major peak of density in the dry cool season and a smaller peak in the wet season. The contact of A. minimus with man was much higher outdoors than indoors, and studies showed the species to be an early biter, especially in the dry season, thus increasing the chance of man-vector contact. {DDT} spraying appeared to reduce considerably the estimated vectorial capacities, however, this effect was not maintained and malaria transmission was not interrupted. Trials with supplementary or alternative attack measures are therefore indicated in this particular ecological situation.},
	pages = {69--82},
	number = {1},
	journaltitle = {Acta Tropica},
	shortjournal = {Acta Trop.},
	author = {Ismail, I. A. and Phinichpongse, S. and Boonrasri, P.},
	date = {1978-03},
	pmid = {25000},
	keywords = {Animals, Anopheles, {DDT}, Disease Vectors, Malaria, Thailand}
}

@report{iso_central_secretary_systems_2014,
	location = {Geneva, {CH}},
	title = {Systems and software engineering — Systems and software Quality Requirements and Evaluation ({SQuaRE}) — Guide to {SQuaRE}},
	url = {https://www.iso.org/standard/64764.html},
	shorttitle = {{ISO}/{IEC} 25000:2014},
	number = {{ISO}/{IEC} 25000:2014},
	institution = {International Organization for Standardization},
	type = {Standard},
	author = {{ISO} Central Secretary},
	date = {2014},
	langid = {english}
}

@book{martin_clean_2009,
	title = {Clean code: a handbook of agile software craftsmanship},
	publisher = {Pearson Education},
	author = {Martin, Robert C},
	date = {2009}
}