
@article{allamanis_learning_2018,
  title = {Learning to {{Represent Programs}} with {{Graphs}}},
  author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  date = {2018-05-04},
  url = {http://arxiv.org/abs/1711.00740},
  urldate = {2020-08-02},
  abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code’s known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VARNAMING, in which a network attempts to predict the name of a variable given its usage, and VARMISUSE, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VARMISUSE task in many cases. Additionally, our testing showed that VARMISUSE identifies a number of bugs in mature open-source projects.},
  archivePrefix = {arXiv},
  eprint = {1711.00740},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/6WWJP2AN/Allamanis et al. - 2018 - Learning to Represent Programs with Graphs.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  langid = {english},
  primaryClass = {cs}
}

@article{alon_code2seq_2019,
  title = {Code2seq: {{Generating Sequences}} from {{Structured Representations}} of {{Code}}},
  shorttitle = {Code2seq},
  author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
  date = {2019-02-21},
  url = {http://arxiv.org/abs/1808.01400},
  urldate = {2020-08-02},
  abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models. An online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
  archivePrefix = {arXiv},
  eprint = {1808.01400},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/JGI2N2UI/Alon et al. - 2019 - code2seq Generating Sequences from Structured Rep.pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{alon_code2vec_2018,
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  date = {2018-10-30},
  url = {http://arxiv.org/abs/1803.09473},
  urldate = {2020-08-02},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (“code embeddings”). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. This is performed by decomposing code to a collection of paths in its abstract syntax tree, and learning the atomic representation of each path simultaneously with learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method’s name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
  archivePrefix = {arXiv},
  eprint = {1803.09473},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/YRQYVA3F/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{alon_general_2018,
  title = {A {{General Path}}-{{Based Representation}} for {{Predicting Program Properties}}},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  date = {2018-04-22},
  url = {http://arxiv.org/abs/1803.09544},
  urldate = {2020-08-02},
  abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens.},
  archivePrefix = {arXiv},
  eprint = {1803.09544},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/9AURHKYP/Alon et al. - 2018 - A General Path-Based Representation for Predicting.pdf},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  langid = {english},
  primaryClass = {cs}
}

@article{babii_modeling_2019,
  title = {Modeling {{Vocabulary}} for {{Big Code Machine Learning}}},
  author = {Babii, Hlib and Janes, Andrea and Robbes, Romain},
  date = {2019-04-03},
  url = {http://arxiv.org/abs/1904.01873},
  urldate = {2020-08-02},
  abstract = {When building machine learning models that operate on source code, several decisions have to be made to model source-code vocabulary. These decisions can have a large impact: some can lead to not being able to train models at all, others significantly affect performance, particularly for Neural Language Models. Yet, these decisions are not often fully described. This paper lists important modeling choices for source code vocabulary, and explores their impact on the resulting vocabulary on a large-scale corpus of 14,436 projects. We show that a subset of decisions have decisive characteristics, allowing to train accurate Neural Language Models quickly on a large corpus of 10,106 projects.},
  archivePrefix = {arXiv},
  eprint = {1904.01873},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/GMNMUM5Q/Babii et al. - 2019 - Modeling Vocabulary for Big Code Machine Learning.pdf},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  langid = {english},
  primaryClass = {cs}
}

@article{baggen_standardized_2012,
  title = {Standardized Code Quality Benchmarking for Improving Software Maintainability},
  author = {Baggen, Robert and Correia, José Pedro and Schill, Katrin and Visser, Joost},
  date = {2012},
  journaltitle = {Software Quality Journal},
  volume = {20},
  pages = {287--307},
  publisher = {{Springer}},
  file = {/home/enrico/Zotero/storage/LH4HVAVP/Baggen et al. - 2012 - Standardized code quality benchmarking for improvi.pdf},
  number = {2}
}

@article{brockschmidt_generative_2019,
  title = {{{GENERATIVE CODE MODELING WITH GRAPHS}}},
  author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander and Polozov, Oleksandr},
  date = {2019},
  pages = {24},
  abstract = {Generative models for source code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.},
  file = {/home/enrico/Zotero/storage/AKKUQ459/Brockschmidt et al. - 2019 - GENERATIVE CODE MODELING WITH GRAPHS.pdf},
  langid = {english}
}

@inproceedings{buch_learning-based_2019,
  title = {Learning-{{Based Recursive Aggregation}} of {{Abstract Syntax Trees}} for {{Code Clone Detection}}},
  booktitle = {2019 {{IEEE}} 26th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Buch, Lutz and Andrzejak, Artur},
  date = {2019-02},
  pages = {95--104},
  publisher = {{IEEE}},
  location = {{Hangzhou, China}},
  doi = {10.1109/SANER.2019.8668039},
  url = {https://ieeexplore.ieee.org/document/8668039/},
  urldate = {2020-08-02},
  abstract = {Code clone detection remains a crucial challenge in maintaining software projects. Many classic approaches rely on handcrafted aggregation schemes, while recent work uses supervised or unsupervised learning. In this work, we study several aspects of aggregation schemes for code clone detection based on supervised learning. To this aim, we implement an AST-based Recursive Neural Network. Firstly, our ablation study shows the influence of model choices and hyperparameters. We introduce error scaling as a way to effectively and efficiently address the class imbalance problem arising in code clone detection. Secondly, we study the influence of pretrained embeddings representing nodes in ASTs. We show that simply averaging all node vectors of a given AST yields strong baseline aggregation scheme. Further, learned AST aggregation schemes greatly benefit from pretrained node embeddings. Finally, we show the importance of carefully separating training and test data by clone clusters, to reliably measure generalization of models learned with supervision.},
  eventtitle = {2019 {{IEEE}} 26th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  file = {/home/enrico/Zotero/storage/MQQNFFDW/Buch und Andrzejak - 2019 - Learning-Based Recursive Aggregation of Abstract S.pdf},
  isbn = {978-1-72810-591-8},
  langid = {english}
}

@article{dai_transformer-xl_2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed}}-{{Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  date = {2019-06-02},
  url = {http://arxiv.org/abs/1901.02860},
  urldate = {2020-08-02},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, TransformerXL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1.},
  archivePrefix = {arXiv},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/SJYYNQ6I/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{dam_deep_2016,
  title = {A Deep Language Model for Software Code},
  author = {Dam, Hoa Khanh and Tran, Truyen and Pham, Trang},
  date = {2016-08-09},
  url = {http://arxiv.org/abs/1608.02715},
  urldate = {2020-08-02},
  abstract = {Existing language models such as n-grams for software code often fail to capture a long context where dependent code elements scatter far apart. In this paper, we propose a novel approach to build a language model for software code to address this particular issue. Our language model, partly inspired by human memory, is built upon the powerful deep learning-based Long Short Term Memory architecture that is capable of learning long-term dependencies which occur frequently in software code. Results from our intrinsic evaluation on a corpus of Java projects have demonstrated the effectiveness of our language model. This work contributes to realizing our vision for DeepSoft, an end-to-end, generic deep learning-based framework for modeling software and its development process.},
  archivePrefix = {arXiv},
  eprint = {1608.02715},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/BC8FUFQM/Dam et al. - 2016 - A deep language model for software code.pdf},
  keywords = {Computer Science - Software Engineering,Statistics - Machine Learning},
  langid = {english},
  primaryClass = {cs, stat}
}

@article{feng_codebert_2020,
  title = {{{CodeBERT}}: {{A Pre}}-{{Trained Model}} for {{Programming}} and {{Natural Languages}}},
  shorttitle = {{{CodeBERT}}},
  author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
  date = {2020-04-27},
  url = {http://arxiv.org/abs/2002.08155},
  urldate = {2020-08-02},
  abstract = {We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns generalpurpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both “bimodal” data of NLPL pairs and “unimodal” data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.},
  archivePrefix = {arXiv},
  eprint = {2002.08155},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/DWHALAFF/Feng et al. - 2020 - CodeBERT A Pre-Trained Model for Programming and .pdf},
  keywords = {Computer Science - Computation and Language,Computer Science - Programming Languages},
  langid = {english},
  primaryClass = {cs}
}

@online{gerrand_error_2011,
  title = {Error Handling and {{Go}}},
  author = {Gerrand, Andrew},
  date = {2011-07-12},
  url = {https://blog.golang.org/error-handling-and-go},
  type = {The Go Blog}
}

@inproceedings{hellendoorn_are_2017,
  title = {Are Deep Neural Networks the Best Choice for Modeling Source Code?},
  booktitle = {Proceedings of the 2017 11th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}}  - {{ESEC}}/{{FSE}} 2017},
  author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
  date = {2017},
  pages = {763--773},
  publisher = {{ACM Press}},
  location = {{Paderborn, Germany}},
  doi = {10.1145/3106237.3106290},
  url = {http://dl.acm.org/citation.cfm?doid=3106237.3106290},
  urldate = {2020-08-02},
  abstract = {Current statistical language modeling techniques, including deeplearning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add \& remove text, and mix \& swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N -gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N -gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models.},
  eventtitle = {The 2017 11th {{Joint Meeting}}},
  file = {/home/enrico/Zotero/storage/LY75RLM3/Hellendoorn und Devanbu - 2017 - Are deep neural networks the best choice for model.pdf},
  isbn = {978-1-4503-5105-8},
  langid = {english}
}

@unpublished{hoare_null_2009,
  title = {Null {{References}}: {{The Billion Dollar Mistake}}},
  author = {Hoare, Tony},
  date = {2009-03-13},
  url = {https://qconlondon.com/london-2009/qconlondon.com/london-2009/speaker/Tony+Hoare.html},
  eventtitle = {{{QCon}}},
  type = {Presentation},
  venue = {{London, UK}}
}

@report{iso_central_secretary_systems_2014,
  title = {Systems and Software Engineering — {{Systems}} and Software {{Quality Requirements}} and {{Evaluation}} ({{SQuaRE}}) — {{Guide}} to {{SQuaRE}}},
  shorttitle = {{{ISO}}/{{IEC}} 25000:2014},
  author = {ISO Central Secretary},
  date = {2014},
  institution = {{International Organization for Standardization}},
  location = {{Geneva, CH}},
  url = {https://www.iso.org/standard/64764.html},
  langid = {english},
  number = {ISO/IEC 25000:2014},
  type = {Standard}
}

@article{kim_code_2020-2,
  title = {Code {{Prediction}} by {{Feeding Trees}} to {{Transformers}}},
  author = {Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
  date = {2020-07-02},
  url = {http://arxiv.org/abs/2003.13848},
  urldate = {2020-08-02},
  abstract = {We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2018) by 18.3\textbackslash\%, the Deep3 system (Raychev et al 2016) by 14.1\textbackslash\%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4\textbackslash\%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.},
  archivePrefix = {arXiv},
  eprint = {2003.13848},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/Y8P66MJV/Kim et al. - 2020 - Code Prediction by Feeding Trees to Transformers.pdf},
  keywords = {Computer Science - Software Engineering,Encoding,gpt-2},
  langid = {english},
  primaryClass = {cs}
}

@article{li_code_2018-1,
  title = {Code {{Completion}} with {{Neural Attention}} and {{Pointer Networks}}},
  author = {Li, Jian and Wang, Yue and Lyu, Michael R. and King, Irwin},
  date = {2018-07},
  journaltitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
  pages = {4159--4165},
  doi = {10.24963/ijcai.2018/578},
  url = {http://arxiv.org/abs/1711.09573},
  urldate = {2020-08-02},
  abstract = {Intelligent code completion has become an essential research task to accelerate modern software development. To facilitate effective code completion for dynamically-typed programming languages, we apply neural language models by learning from large codebases, and develop a tailored attention mechanism for code completion. However, standard neural language models even with attention mechanism cannot correctly predict the outof-vocabulary (OoV) words that restrict the code completion performance. In this paper, inspired by the prevalence of locally repeated terms in program source code, and the recently proposed pointer copy mechanism, we propose a pointer mixture network for better predicting OoV words in code completion. Based on the context, the pointer mixture network learns to either generate a withinvocabulary word through an RNN component, or regenerate an OoV word from local context through a pointer component. Experiments on two benchmarked datasets demonstrate the effectiveness of our attention mechanism and pointer mixture network on the code completion task.},
  archivePrefix = {arXiv},
  eprint = {1711.09573},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/WLLNLQEF/Li et al. - 2018 - Code Completion with Neural Attention and Pointer .pdf},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  langid = {english}
}

@article{liu_self-attentional_2020-1,
  title = {A {{Self}}-{{Attentional Neural Architecture}} for {{Code Completion}} with {{Multi}}-{{Task Learning}}},
  author = {Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Fu, Zhiyi and Jin, Zhi},
  date = {2020-06-26},
  url = {http://arxiv.org/abs/1909.06983},
  urldate = {2020-08-02},
  abstract = {Code completion, one of the most useful features in the Integrated Development Environments (IDEs), can accelerate software development by suggesting the libraries, APIs, and method names in real-time. Recent studies have shown that statistical language models can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilized in the program’s representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networks based language models are not sufficient to model the long-term dependency. c) Existing approaches perform a specific task in one model, which leads to the underuse of the information from related tasks. To address these challenges, in this paper, we propose a selfattentional neural architecture for code completion with multi-task learning. To utilize the hierarchical structural information of the programs, we present a novel method that considers the path from the predicting node to the root node. To capture the long-term dependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable the knowledge sharing between related tasks, we creatively propose a Multi-Task Learning (MTL) framework to learn two related tasks in code completion jointly. Experiments on three real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {1909.06983},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/5RXXFIKM/Liu et al. - 2020 - A Self-Attentional Neural Architecture for Code Co.pdf},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  langid = {english},
  primaryClass = {cs}
}

@article{luan_aroma_2019-1,
  title = {Aroma: {{Code Recommendation}} via {{Structural Code Search}}},
  shorttitle = {Aroma},
  author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
  date = {2019-10-10},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {3},
  pages = {1--28},
  issn = {2475-1421, 2475-1421},
  doi = {10.1145/3360578},
  url = {http://arxiv.org/abs/1812.01158},
  urldate = {2020-08-02},
  abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently. CCS Concepts: • Information systems → Near-duplicate and plagiarism detection; • Software and its engineering → Development frameworks and environments; Software post-development issues.},
  archivePrefix = {arXiv},
  eprint = {1812.01158},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/LYSLGYDG/Luan et al. - 2019 - Aroma Code Recommendation via Structural Code Sea.pdf},
  issue = {OOPSLA},
  keywords = {Computer Science - Software Engineering},
  langid = {english}
}

@book{martin_clean_2009,
  title = {Clean Code: A Handbook of Agile Software Craftsmanship},
  author = {Martin, Robert C},
  date = {2009},
  publisher = {{Pearson Education}}
}

@online{noauthor_go_nodate,
  title = {Go at {{Google}}: {{Language Design}} in the {{Service}} of {{Software Engineering}}},
  url = {https://talks.golang.org/2012/splash.article},
  urldate = {2020-08-08},
  file = {/home/enrico/Zotero/storage/JCV5KWP7/splash.html}
}

@online{noauthor_null_nodate,
  title = {Null {{Safety}} - {{Kotlin Programming Language}}},
  journaltitle = {Kotlin},
  url = {https://kotlinlang.org/docs/reference/null-safety.html},
  urldate = {2020-08-08},
  file = {/home/enrico/Zotero/storage/TJCBEQAP/null-safety.html},
  langid = {english}
}

@article{radford_language_nodate,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/home/enrico/Zotero/storage/TD9UPAG8/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf},
  langid = {english}
}

@article{raychev_probabilistic_nodate,
  title = {Probabilistic {{Model}} for {{Code}} with {{Decision Trees}}},
  author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
  pages = {17},
  abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).},
  file = {/home/enrico/Zotero/storage/CG4K8ECH/Raychev et al. - Probabilistic Model for Code with Decision Trees.pdf},
  langid = {english}
}

@inproceedings{schmedding_clean_2015,
  title = {Clean {{Code}}-Ein Neues {{Ziel}} Im {{Software}}-{{Praktikum}}.},
  booktitle = {{{SEUH}}},
  author = {Schmedding, Doris and Vasileva, Anna and Remmers, Julian},
  date = {2015},
  pages = {81--91},
  file = {/home/enrico/Zotero/storage/8IEJRBXF/Schmedding et al. - 2015 - Clean Code-ein neues Ziel im Software-Praktikum..pdf}
}

@article{svyatkovskiy_intellicode_2020,
  title = {{{IntelliCode Compose}}: {{Code Generation Using Transformer}}},
  shorttitle = {{{IntelliCode Compose}}},
  author = {Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  date = {2020-05-16},
  url = {http://arxiv.org/abs/2005.08025},
  urldate = {2020-06-11},
  abstract = {In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose \$-\$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, \$C\textbackslash textbackslash\#\$, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of \$86.7\textbackslash textbackslash\%\$ and a perplexity of 1.82 for Python programming language.},
  archivePrefix = {arXiv},
  eprint = {2005.08025},
  eprinttype = {arxiv},
  file = {/home/enrico/Zotero/storage/N59K8XW8/Svyatkovskiy et al. - 2020 - IntelliCode Compose Code Generation Using Transfo.pdf},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering,Encoding,gpt-2}
}

@inproceedings{zhang_novel_2019,
  title = {A {{Novel Neural Source Code Representation Based}} on {{Abstract Syntax Tree}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong},
  date = {2019-05},
  pages = {783--794},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICSE.2019.00086},
  url = {https://ieeexplore.ieee.org/document/8812062/},
  urldate = {2020-08-02},
  abstract = {Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.},
  eventtitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  file = {/home/enrico/Zotero/storage/NQM9D52E/Zhang et al. - 2019 - A Novel Neural Source Code Representation Based on.pdf},
  isbn = {978-1-72810-869-8},
  langid = {english}
}


